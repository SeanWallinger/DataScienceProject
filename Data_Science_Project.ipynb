{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanWallinger/DataScienceProject/blob/main/Data_Science_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0riwwMKCn9_q"
      },
      "source": [
        "#Mental Health Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmiUqMl-oEdZ"
      },
      "source": [
        "\n",
        "\n",
        "*   Sean Wallinger\n",
        "*   Math 5364 Data Science 1\n",
        "*   Tarleton State University\n",
        "*   Spring 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qOsJ3ZbYvJ"
      },
      "source": [
        "DISCLAMER: This is NOT a diagnosis! These findings are based on common symptoms of mental issues. If you are concerned about your mental health seek professional medical help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONfsEzjQxddX"
      },
      "source": [
        "##Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2nA6MY3xgwg"
      },
      "source": [
        "The task of this project To create a model that can aid in the early dectetion of serious mental issues using data from the data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_VYpW2oL6j"
      },
      "source": [
        "##Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVMq3sMAV_P"
      },
      "source": [
        "For many years, Mental issues and Anxiety has effected people and caused many problems. The field of pycology has worked very hard to try to understand, identify sympotoms, and try to get patients the help they need before it is too late. But, identifing these big mental issues with simple symptoms is a very challenging task. Now the question can be asked, can past cases help future ones? Can past patterns and symptoms possibly help identify current patients who may not even know if they have an issue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4khyxgT0P3p4"
      },
      "source": [
        "###Depression Anxiety Stress Scales (DASS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOM3UnsQhl4"
      },
      "source": [
        "The Psychology Foundation of Australia created the Depression Anxiety Stress Scales (DASS) and hosted the questionnaire on a public website in an effort to help educate the public on Depression, Anxiety, and Stress.\n",
        "\n",
        "The DASS is comprised of 42 questions which are divided into three sets of 14 questions which are related to the topics of Depression, Anxiety, and Stress. These questions have four possible responses which relate to a possible score as seen below:\n",
        "\n",
        "    Did not apply to me at all ---> 0\n",
        "    Applied to me to some degree, or some of the time ---> 1\n",
        "    Applied to me to a considerable degree, or a good part of time ---> 2\n",
        "    Applied to me very much, or most of the time ---- 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8-xWnQULif"
      },
      "source": [
        "####Question Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXfI9Z0cUYu7"
      },
      "source": [
        "![demo1.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pp71514hoxZ"
      },
      "source": [
        "###The Ten-Item Personality Test (TIPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRTVOEDphzN0"
      },
      "source": [
        "To help get an idea of the personality of each entry, a Ten-Item Personality Test was administered and resposes where measured. The TIPT asks ten questions about a persons behavor and the User responds on a 7 point scale as show below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAbASZYjOxe"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOcAAAC6CAYAAABV5FoJAAAbXklEQVR4nO2dPY67sNaHf/fqbanpmIY1uMpUKdNSIE3FCiKxAcQGkGYFqSKlcJsy1aTyGtKEjjoLeG/hfNjGfIbkD+E80hQzk9jg5IB9sB//53K5/D8Ighgd//3XB0AQhB0KToIYKRScBDFSKDgJYqRQcBLESKHgJHogkDkZxL8+jA+nRXAW4KGDkBcvOYCCh3AcR/ux1yWQOSFedBgEMToaglMgc3xwrF57FOkBl8vl+nNCwH04GV2XJQV4xkHXpPlRHZwFR+gsgcMFv8Ebjwgugt0JG7GEHp8M8WWHwH3nsYyBM3K6Ts2S6uB0A+wuF8TsjUfzqBxBnCKhOwZQ5DS2mynjTQh9eVjtc5zvf7AkIQqOUB2vhkowi0wbx9p6ySJzSuPdW3e64KF8z60ctQCt3orEiFp/2HyRMcfeIS9kPX6E/T6Crx5DwRHKg0NWOgaZI6gcvxccYchR3N/rwLGO5fVyqvMB9lxAwUMamjzJcMFpBIP+0yOR434jWAnkle8TyHyO4HS5j1dPv9+49XpF7uF0G8eeNhBL/RgKHmKJw32cu1kB6eGCi9JVEMcMYXYt5/53vd7TRmBpBl/BES6Bw7X+A4vg131RRQafB4/jvZzw++3K3stpg9VqYxwDAHFEFmbwThdcLjHkfwrw0AcPTtr4vRxQObbhEYtbfQeGaK2fg8jUcg5IscLmdMGuNK5gWKR75Gf9r+d8j3TxT7pdH8NwwcliJalj/vQdK5Y/9DviiCSNtXJd9/ELC4J7oMINEGtfoAJ/HNj83L481270UQ+gfSIQ/CrlACh4BrH5vdfrBr/YIMJWeavYRmCHW8AALD4gTbLKC5Q4JkhjtR4XblN77ROI4Fdv1+IPHBv83v/oIvjdANFWv7vvEyB+HB/YAume4+/R7cAxSRHfy2H42QD8z34CX94KQruKFsjFCt5XwzkQtYy3W/tSzsj3DJ76xf7ysBK5fgdcBfjWgqTAH9+DaW908R2oX06BY2J+Me13l+dYIdAPDsUfx5552sXE3gNJod/UvuCpCfkih1h5UE/B9Rj2FSfgfgcA/3u0XfEHDrPtiK5MNzjZAmliZnTrEdV95Io6jC86AKDNHcEI/KZXL1Iky64P9ct1nPM9VqWDc+GxoS8MZhUemJofOOfliwTRmekGJxji0wZi6VQkHvSExjJR//cFz7ybWL5Q5S/6Gfl+j8jXx9R+tH/cVYocAgmWxrh7mdRcHFgsx64ViSsrxp2tCa3upveawQagyIWlPW4wLNIEt1FBkQsabw7AeBNCOCPfN9ylro97DlgaGcvr5Il7QuOCQ6q9Ed8BEN0HigV4JpQxaB0yMVIaV2vPnNJ7Mkj9KSdT1FPZ4XI5AJUXm3cigy3jj676NmLKGLTMY9wpx/M03nye/xusJBbjcokHK07egRgWLfpGLL7g5IXwM4FLzFDwDEl6wKUhGA6ZA8eRv682J+wa6zLGZtaCPTDkzQdthSG+nOCFPjLR/RnzV83BsS79bAAsPiEIfTiR/D09XFB3OO53AGzlvTZHgB/q0z7NeLu15xz7dFH7hVBxgxhpcoRAyzR+wZGJzf3xRd1dTamlxfjN0mXuhD1z3JZy0qZn5lRsEbHHtMrGC4XrgSVHiCKHoPHmIIw0OAvwLOk2bjFm0ujjO4FjYrz8jwPBd+cv0Ze3aggcGcBVjx3aUPScr+d+B+WMc/EHbmamWyCOHdv/elE6bjkYjTcHYYTBKR+kR+xQf7UWXBvHim10v9OyRYq9ktoXWQakepfPDXaIc7/FahgY74vLWeJCQChvZT8bIFrr42whKrOxgqsTAAS2kXLndz0w7Rlk7cEhZhHW94oL8HUEbH5a90Du5xCf4BkzqOqHwnIcLwSj8eZAjCM4k6XyJbgmcpr6UV8AV7KmS7HB6fYeFuMUcDnlzXGQeb/48fS3i8zBcaEmbKpm05gwxLfEze2Y1zm+tEefAXanQDs+54jKAPnC41gdZwmxOSkXJjkBIPLbTQNk8XVVj9KW7brsKgV4uAZ+1faR51wXoK7HsN+j812asPOfWaoxRQbnuChfAEQGJ/Nw2gWzHjMVPMQav6WgLngIP4+rL5xV7Ur0Yhx3zjdTOZ768l69cnUCyEch5uwjQN4Za99Z+yyU6Mosg7M8F1Rinf42O6oz0uKY1ARfdVAT/ZhlcLoew76UsMngR2g5EeGz+fJWpemEBQ+x1CbDG4gtIppPOyjDTUKYEizG5ZDB8R1Et7+tNjhd4pnfNSVusMMJIfzbDA3gqpKxXLhua06xwuZE7Tck80wIEcQEmGW3liCmAAUnQYwUCs6ZUvDwZS7iYSBxdYMaU52+9ZqGIqk0QdipCM4CfG3IszYCy1ddyUgq/eGQGLsPFcHpItjpUi43+MVm9Vjt/jpIKv15kBi7Dx3GnC481sPD0wuSSn8UJMbuxXgTQrOSSt/OTRU9X8ttUZd2Hi18SpWJoIa6RCbH/Lc8QahoTDJLG97LtImxW57bo/z5ias7B2el7oKk0mrh3aTSt3IV0fNpI7DMMmR+jvjuQUqwNMoRmaOcx9WnVHqNr1sNch/b3Jzt0+K8sEe+zeTKFNUeIfKr3PralmL5CNxKMXab+m7MU1zdITilaLiyLUgq/Ti0jlLp27kieCyKdr8DrJIEUMv52RimA/mZHJQLSrku+2uQ6GqINucFyLcdzCVhLFA+B9mWVY7brvXdmKO4un1wiiOSDk6fcTNGqbQhiXY9MNORayorxRFJSXNp1FX5mq7ndWWQ70CH+m7/naG4umVwdlFHvokPkkr3LafIhfUY1btM3Wt02t2FWq/XNC905ZK63fVmKK5uFZwFXyNi8cgeY3yQVPp6PkPqPZq6lTotzut2lBUHqU0mWSbW1/SpT6l5duLq5uAsONYRK48zTEgqrbymu1T639PmvOyIzNF3SdMbe7D65iaubljPKe1trEEoDICk0veCn5FKd8P1GKqq6qYLaXFeVRQcWZLicOniXepX39zE1bV3TtmdbVBUvorZSqU7YhnbnfNHssWVM0esr3nwxGZHHT+np+qbmbi6duJ7q+7sS5i3VLo1bIHU2HColC2ufI1eVPN51WAEvzALt9CvvnmJq2smvuvP6t7H/KTS/WH42QhtYoLIlsbzX5lIMV+DVB8Xtjkv+yEYG++KDBlS3WJoEWP3q29e4mp7cBZ/4HsgWVqSOy3Exp2ZuVT6Gdxgd02IXdsBh1LbsVjO2rlPZfROiBc9zssKQ6yea+bh12xsqxi7X31zElfP0yFEUunpMiNx9Xgnvr8QkkpPlzmJq2cZnCSVnirzElfPMjhJKj1RZiaunmVwSqk006ePZR5OZFoYJ7d1n0uBze988gHzTAgRxASY552TICYABSdBjBQKzokylBRaZE3byRP/iprgNKRNJJUeNZ8uu5ojFdP3OEJnCRyU6W0HkFR6UlhEzgVHRle3yWBfz+kG2F0C/W/sB5uVj6OIwV76KFBKpRH6yMRFmfwupdKEHTfY4aL9xSJyPpM/dkp0kkrXCZiGhaTST2MRORekXZ8UnRJC6iLelzMrqXS7461DSxBZRM4FD6WjJ/Kby28teyZeSUdvbY3Th6TSauGdpdJNx9sJi8jZDXY4bVZYbaRXqXpRRxfZM/FKWganQOYsNQlwCZJKPw6th1S6/njfR1fZM/E6GhxC4b1r453Gbo/rwhil0mOgu+yZeB21wekGu+ud7xdYV/lh/xEzkEr/m4D47C0OpkTLbm3Vnpn/kk+TStcd77voI3smXkXnRymVxjSSSiuv6SqVbjred9JfLk0MS4NUWqdOYkxS6VvB3aXSbY73PTwhlyYGp9Nzzrc+xJ6RVHo8+0w+IZcmBqdDcMpM3nvkSvOTSjcd77t4Si5NDErlxHdzgnTB14iwwe/Lu17zk0q3Od7O2ETOHtPqsb6tr1yaGBx7cLrf8Lj+xfV58Dqf69yl0i2OtzsWkTP7wQZySl/1efaVSxNDM0+HEEmliQnQKVv7KUip9KX8D5JKEyNilpoSkkoTU2CWwUlSaWIKzHPMCcjxpTpHbrWhsSYxKuYbnAQxcmbZrSWIKUDBSRAjZZ7BWXCEpN4YPzP/nFoHp8hes+W8tC1ULCnr8+EUHCFJqT6TmX227YKz4MjE6oUP6LtPFG+PRa5MEBOgVXCKbQQWBM0v7MkqTYFoO8wV0Q2wu8TKPFaLXJmYJqXP9rNpEZwCR7HBz/cLj8L7QZwmeMlKJYtcmSCmQGNwFjyD6LHusSts0d7wrsqgS6sr1HGqRa5slKRs1mQf9zbWJc3T13Kqx0P6hk22unSHkFmnyOR7buVox9IosNbLbvJA3QXVLcTYmpjb1r4hR3Gvv15Xo7WRrT5LDsLcCEttlym1uY2G4BTYRgxxmzWcTziE9vn5upxJX39oQ5dBH8Aiv/rELXLlBzm24RGL+0ZNDNG6/ME31iWOyEKpDr1UdrkK/EHx+h4YIl8PZJGpDqED0qvL52Fo2CPfZvDzWDc3tBBYa2W3FVbnW4THxcNpxCKsjTeJzFHa5+pyMhton2Ob+cjjBn+xyOBH7OFeinP427y+CysyuZRRWfb3e3eZTrDNDRq8tRnE5qddH/9pqbQp3bIhLxaHe5AxxIeee6rsEyBWgoktkGqLk1vWtU8gghrZNgDARRAorch+sNFUJgLHJFUugnItppkkSxIox3N9Z5PA+qpj+X1YovG7aWrnW/M86jIXhN+OWT0euzw7QYKGhfO4rhQ6qJ9HjAMS1AkhxDFBGqtTLl08vOLTa3OT2v05W981B0L1ANkP6Yhk5UH3NS+QanuqtCWFbkEx5Fat61oh+H6yjYocwqjL9VhZR1lyKjULrG0rbdzvoCzQNjHrMpfT2dqnQp7drJuxb/XBFi9UEI6xzQ2qg1MckXQQbA0DwyJN6veQfOeSrlZ19RVIdxdp2f1NzfW/wvtU5MLaPuXleC0k1VfTYuk8GtbXskWKZNnluee02rwiOLt4XIeFyb6F9QpT5MJQmjhwnCUSdLPdAQBKV/2edTWU80BNPvmI9sq/XA/MuCMXuSh9wKUd3loIrM/5Y2ex+48fYd/Q23jmy6XffYax31thsdxoqTLhMq02N7EHp9giYnG3zYeGkkq73wjqNs7RdsF+dqOkBoaqq7RTuNzV7IHZY+gypGgWWN92FtN/PuN5odwy5Oo8Km3TOO02t2pKxDEBEsCxjMb3TiQFzOZJDCaVljt+OUeB2Lh410qtB2bIusQ2Ajan2qQIi08IQh9OJH9PD5fmD7KFwPrrRZbouvZ5jz5VhSG+nOApu6F/Qptb75wstmRclUcSL99tjC1k9svsA9h2AnsVg9VVIBctkkZii4g97tTtdj9oFlhbkxxDYWmfXhssux6YbWhyzrG3vsFaiLKN42e0+UhXpVzT2kfjCmVxsb6MQesyEhHFH7jxrZPSsa4dzRYC6y8Pq7oMeF+smeuGDZarC7NmeUVHs7a+I8H023ykwSlTz0gS48p5dbEakwWEqGmG3kHWoy77ARgbQBXg6xzMeErA4hO8TB+rt5lV0iiwdgPEaYKlbomGeNoSzfCzEVq5IluWNjRuXZqZeRUZlkhR9zBFcK49d91Gt20tPqPNRxuc8gRtf95pAmbHcXCsHSlY5MqtD6FrXdXlyJ3QZNYwj2MstFcU4OEa+FWHEjLJ0fhlaSGwZvFFqd+B46yRD2CJ1s/rOluo725kSuZVTgX0cIoXtW/5gvrZLCGUMeYntDk5hEZAwUOs8Vsayxc8lFPHaPu9wZlCm4/3zjkbCsg9lcpXVdf791+Qz2QabU7B+c+p3nZPHJN/8FhiDkyjzSk4R8CXtypNQyt4iKU2MZsYkim0+Sz3ShkbbrDDCSH82zbbwHV20ni6WJ/GFNqcEkIEMVKoW0sQI4WCkyBGCgXnyxHIBnGtDlUOMRVqFlvbloC97stR8PAl0uoxIrK6bd8JQlIZnEUuLGvSXrcG8JzvgV66kQ9AZKUpYyKju+Tcqe3Wdl760xspW0pf5a4dGSzWl90VJet1QSJsojo4z3n7lXRPc5UtLSq2g/90ym19xjubnxgnNXfOPuvy+lHIiY5g34HdH1RwhJXqk3pxca0UWhbeUf5riIh7jJNF9qhHZNI/kyxvxy6QOUskN09NQ/nN50dMleGytU84hO6r5y3SJUAg8zmC000OnMot4lWXT4W4uI0UurP8t/gDFFOETSjcBRZfcEilIkMeO0N8OSC9eWp2QaUBsJNgm5gclfa9XOzlGsi2mdreUmk53pQL0qV0SRt3iqO+gNdqhreJi1tIofvIf90Auqt48z51isaAgm1ilFQEp4tgpweYXAj7ggyiIfc1vadlXaF9RUFJN9FCCj2U/PefMKhgmxgjrbu1brDDoUn43IdzrgWI6zFD+9+GivFxCyn0IMuD/lVAvFOwTbydTmPO8n4ZzyM1nIrOYZloX/ayyUya1Uwdftn92yyF7i3/VcfXftTBEDccgwq2iVHyjxNCMtA2J7ULfdI3nLlpMu/u3w7C6xZS6K7y34KHcJQdpi6nzQt3/G7gnYJt4u0MF5x9EkLFH/jevOvJ3cYe6kGGWJUpLcu7Ptloo5voLv8VkK7if29LH5NOg3gNnYKzavOa3pxz7C2bJZldWbnlWscphC2k0J3lv0UOsQrw7KZig/BOwTbxT+gQnAX++L6HhLemRMvGMQCuXdmblFd91NKBNr7aPvJfc/ObP/5PxpxvFWwT/4SK4BTIDCEuD31ErHkT1PbIYLfP31WV9wzxyVN2i2q7OqaFFLqr/Nf9RrBSnsMWHOuc1YqP2/DlqQJkQJ5/0zzjoaTXxFipCE6GhSbElYE5rMvzjHxfdUdUlPcFh3T/KmPYA1o9c20jhe4m/3UR7K47WjkOHD9H3CA+boPcNHipTB+87vuxrL8QDSW9JsbJyB1CNyu3mVCSd/I8brv5DEFMj3GbEIo/cNgSMC4oWUl8OuMOTutEeKD/blYEMR3GHZzXxIiZsOFh/92sCGIqjFwq7SLYnYDQh+7+vWBE7l+CeAkjTwgRxHwZebeWIOYLBSdBjBQKztEzAZl0wRHOxDn8TpqDU1sKRlJpgngX9cEpMn3tIkmlPx+L4Jr4N9QEZwGeiTetXZyXVHrMlAXXxL+iZq+ULSLr1LkXMHOp9Jh4q0ycqKV+r5Tg+y0CqSlJpQseIuSFHCMb5TbWpZ1Hhbql7RhffZ05Vi84wkzItjHKUY/b1la64Fr/Xxt5dcFDOKVGFMga3MVEmdrtGDrtlTIXqTQA8DXW+FXKXSPLQmTeoxxEa72cgiNUz+MUgPth+TVL8XAqnTxk4RElSVHBESq5AKvYWhyRhRm8k5orKPAHRSdzYIj8R+CWBdft2/GGVS06JoPEhKiRSq/gfXW4q8xFKg1gv2eIHweERbpHIgKtnDjV3brFHwc2v4/zsNRlfU0gkBg9TalteeQCWHzQJWgAsE8ggl+j3V0EuhFbl6lZ6Sivtl1gDf0p0Y7abC1fr4HfS7e7SlemKJU2vEdf3qrkVtLPw2590OuqeY1+YpYVOfICobfJCsEQt6rO8uryBVYck0H1NnOhJjj3QFC+0vOhpTWfIJVuLOeMfG85Ru0uU/OaUnllT2+ZNq8BgPKFrlxUt7ueeWEqe4aJNlQEp/yimFfefoFTzxSl0kMFtKSjBLrIIW47kCk/ywR6ptu82yn1PXxMPqKG5GwfebWrJfbOyN+V9f8wKoLzC1al65dXLVCeiVR6HKTKxJDHz66pUQqOUM4qUdq6TXUd5dVqj0AckdB4sxcklX4LFRc7AO27n1es3dx2CGnE7uRd6ievZlik8gJb5ILGmz2p3GXsbr9TGTrrNjWp9FNYxnZaMuwL3qriNdofVG1oF2QvpXOSqKe8+suTF9hzDhpv9qTyzskWaemLWymB7skkpdK9cPEdlGc/6dnimtcYZVkvnK0wgr/4A2+aENRTXi0vfFscBY03+1LdrWU/2Ijl49lmwbGO1Od7zzJBqfQTuEEMpk5MKDjWEbD5eVx13O9An7xQcKw5Q2p0idmPZZKDEA1tIoP/Ia8uwNc5mGHELguue8qr2QKpEBA03uxNzZjTFChzBINOgp+iVPoZjLGzzxGcjLG4G2B3YI8dxf0c8e4HXvnEsFPLchw4RzR+Nm6wU87VRx7HMJXYZcF1X3n1FzzsqU/7BCN3CJFUerpUfXZEW8ZtQiCp9IQ5Iy9l4okujDs4SSo9XcQRiSUTT7Rn3MFJUumJUoBnNJ/2WUgqTQzKbU0o0gN9Rk8y8oQQQcyXkXdrCWK+UHASxEih4Px0SPg8WezBWbv86zViaZJKE4SOPTirln+dNlhVLuJ9jo+QShccGSnmiIHo1K29KSyHf7z4IVLps7m8iyD60yE4C8jYfMGT/w+RSpMtnRiS9sFZOc/1eaYkldYdPI8xeMFD+NHDSXQrR2TyeG4y51A/uNqxvMhkOZoI2naA1hxBuTwSPk+L1sHZ2KWdhVRaTh18OHguOJ1+wHBdVrVZ3Z1Ej9Uye+TbDH4e654fc5OoqmVwx0wRWJ/0NbbyJDXB9Gmzujp/ysv7SPg8LVoGZ4su7Ryk0sUfODZQ1kfDdZu/1UlS9h6JY4JUEUODxTikSSmhlAjvcXylBdM3EfXPvRy5HrPC7kDC50nRLjitIq6BmKJUuiul1Rn2VTU2NYy5X40p3Cpvm1HnGCLh85RoFZzFH7eKuAZhSlJp9xsBIqw7DtBKdRQ5hM2611Om1QUSPk+HFsEpXT+vurpOSyot1S0s8jtNmOi0IZRRd9N7zZ5G0yJnEj5Ph+bgLP7AK10/CrORSjPEl8vVqfPvs5xSCrZVtvjLIJQxaPkNJHyeCo3B2bpLOzOptBvs5BZ66x5TDuvE0F1nYLkBpIdNto/Pld3OrJDweSo0BOdru7STl0pbM8dtsSRtemVOC/BM2dNzFzS+n4TP06A+ONt2aXsyfan0Gf12abdt2SfH352l3T0mh5DweRrUBudLs7RTlEoXHFyov2ZIlAf4XTLN7GcDsVTOQ2RYJml3abcbYBfnmlO2MVlFwudJUBuc5/yFXdopSqVdD/nyUZYfMRzUbiT7wQYR/MqpgtrBaWNFOVuoh7RbZHCOC22Mfwo4/NoAJeHzFBi5Q4ik0vUIZM4Ri1JQC2ROBs80yt8h4fMUGLcJgaTS9VS6Yeu2HARI+DwNxh2cJJWupyoj3TTdkoTPk2DcwUlS6XpcD2xvTicUyPxImwyvQ8LnqUBS6UnDEF8OyBwfTnT7m5xxFVsuXCR8nhYjTwgRxHwZebeWIOYLBSdBjBQKToIYKRScBDFSKDgJYqRQcBLESKHgJIiR8j+lN+805SUr1AAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuaegifQjW9q"
      },
      "source": [
        "Two of the Ten questions relate to one of the big Five personality traits, the average of these finds a person's mesure in each of these personalities. The five big personalities are Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmv53LFEljNK"
      },
      "source": [
        "<img src = https://saylordotorg.github.io/text_principles-of-management-v1.1/section_06/2ca4a3e8a5ffc2a3ff954f79a1475071.jpg width = \"575\" height = \"383\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9mgS95IU_pY"
      },
      "source": [
        "###Scoring Scales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NLXNl-gVEKP"
      },
      "source": [
        "The Score for the DASS 42 questionair is derived by summing up all the questions that relate to the relative topic. The total score is an indicator of the severity of the self reported symptoms that relate to mental issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rbeXnOIfp8T"
      },
      "source": [
        "####Scoring Scale of the DASS 42 Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwjFAv1MfwNo"
      },
      "source": [
        "![Screenshot 2023-03-01 124519.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjAAAADLCAYAAABwO6mcAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAF/VSURBVHhe7d0JXBTlGwfw3+6yu9ynCoIHCXJ43/d9JWoqYGpmZmmZpXml5pFnaZmmaaaZot3et2WaB17gkSAKaB6ggjeHCsLC7j7/d3YXBFwUS/+1+nw/ny125p13rmfeeeadmVVGAhhjjDHGLIjc9H/GGGOMMYvBCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYvDCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYvDv8TLLIbu4j6s2nMeGkPEyiCTy6G0dUXFmk3R2M8FCkMpy6dLDMfKvRegK98c/dr6moY+LXqkn9iKzafd0K9X0xJvY316DLZtjodr2x5o6vWsbPnHk336d6yOvAK9ygete7VAxSe+GXRIDF+JvRd0KN+8D9r6WpmGl4BOB51C8Q+OicJxUdDD1pvj4knQIy32V6xavw+xVzIgc/RElWbd0atzNbg80OUg9rNO7Of/4KbWp5/A1s2n4ZYfC49YL306YrZtRrxrW/Ro6lWy2JUSGMYsQdbaPiQCXTq1FvrIFM5Uo38YnbxnKmjhMlf1IkcZSN0ljPR608CnJW07Da6sJvdXVhu+Zq199SHbeBmdyjIUI9IcofHVVeTWeQldyDUNe67cop97liLR7hIUL9C7OzPoye+qTFrVy4lkUFOXsPSS1a9Joj3z3qSmL82i01rTsL9DxMU7vioRF6tMA/KI9X7ZjcTJxbjef2QWXq7nPi7+KQ3FfRNCFVSywsegTEUVghfTqWxTMVEuac88erPpSzTrH+3opyWNtg+uTCr3V2h1ivT9Yeu1iGI1xjJHJtQglVtnWpJQsnXiBIZZjLwERlGuAw2fOo2mThpPI95oSz524qCQqanqB+Giybd8ubHr6LPp02jGymjTkKcll07NaES2Vt40WJyIJHkJTPHbeJ9pG2vp7OfNyVpMO+i3O4YhzxNd0mIKcpSbGmI5ub+6hlKfeAaTS7HrPqPp02bQyuj8M9dDZe8ZSj5WIGXDGRT/t89rxriwkeJCJGYFSevd0SHvJCSt99oi6/18x8U/dmct9S2jIJmyPHWcEEabdmyndV8NpDpSrMldKHjFNdJJ5bL30Ps+VgRlQ5rx93f0U5N7agY1trUi78F/GNuLR63Xd9cN66U9+zk1txbTDdpOd0pwPHECwyxGXgKjrDuVTuVf3eno2tq+VE4BkrsE0/c3DIe3GHyLIha9R53qB5CvX01q9cokWns6r/tAXNvu+ph6BofSiO/DaePkntSkih8FNg6hCevOUt6pIu23yfRycA8avWwVTe9emwJqdKDJu2+brzv+fvePJmELTXutDdXxr0Qv+FajRl0G01cHbxgbHokmgbZM60tt6vhTpRd8qVqjLjT4qwOUt+iag7Pp1eBg6vnxrvyrW+21A7R4RAg1r+lHvv51qd3r02hjgfXJ3vcZ9Rbr8+7ScNox63VqXcOP/Gq2oTfnhufX+4DscBrma0WKSkNot2ml8xIYZd1pdDLHOCxvG5cXJ8aC21h74QtqqZaTc/fldL24eTyTtHTm82ZkI5OTW7PWVFNcVcqcutC3yQVPJNm077PeFBz6Li0N30GzXm9NNfz8qGabN2nuPmMsZEUtordDgim4xwj6+ZwU0NkUs+QdChH7vvfETXRFp6GDs/tScHBP+nhXXmquo1sRi+i9TvUpwFfU1+oVmrT2NEnRp0taSx+0qUz2MrGf3KpTh9DRtOrIKhrTQ8yj7+e0Pz8XEVfHU8WyBfehmXsLJygGBeJiV1bBs4hxva0fut7Pc1z8c9oLs6mFStp/PeinW6aBUixNbku1a9amF6cfIo0uidZ+0IYq24tEUu5G1TuE0ug1Fynlb7ZXxvbotSLt0cH77cYj2qsHZVP4MF9SKirREFPD8rD1qmNYrwgydMJoL9AXLdUkd+5Oy689OjHjBIZZDPMJjJC1jQaUFZm8wosG/iodMBqK/rQFOctlpHD2oXp1K5GT+FtZoS+tMjW26WFdSC2uIB3dXMm+dAA1qOtNjqKMTOVHg39LFYmDOGl/3Y5Uooy9owPJxUkBylo06XiGqLu52bpXXxFHdG4UTa1vSzK5A1Ws14paNfAhZ4VMHLh53aK5FDW1nuHk51CxHrVq1YB8nMWViWiIOi9JMCzbA7eQUnfQsKo2JJMpyOmFutSgqoc4icjIqmwXWnzauCEyfwolO1GnfenS5OxRlZrUNy4XFOVowDYzJylBEz6MfK3kVPq19fk9V+YTGEFs44GeivxtbDit5UbTpFpKcRITSY2hm/g5IfaxYb3FtnhjYyx92dpWxIcNNf/8L3GKz5NJP4XYif1qT6VLO5NH1SZUv5KTIY4U5QaQYZdoE2lFcFlSiBgrExxG56JmUlMHEYM2tejDA3cNdRS9haSJ/pSaO8sNt/R86tWlSk7ib2UF6rsqmXLOzKG2rtakEPOQKW3JuUxbmn3yaP6yDthqioO0X6iXm4g5h460OPnBs5AmfLgpLjZQRsH8peB6bzhlWG/ZA+stPK9x8SRoIujDaiqxz2VkW74x9RgyjRatO0jn0wtsYe0ZmtPWlaxFuwKZkmydy1Db2bGU/HfaK0N7VJ9si2mP9CVorx6gMSbA8tKv0Ya8hqUk62WQS9GTapFS5kTB3+VnOsXiBIZZjGITmNxjNKGGUhzM1tR1hWj4726k1z3EQaaqQxMOS88mZFDEhzXFQaGk+tNOikMkL4ERJxOPEPruonQQ3aXDk+obEgP7oCUicchLYMTJwL4eDV8VTru27KcL6Rupn7s4aUh1R0onGWPdKqnu6adIe2sJBVmLxqJcTwqLTxcNeyYd/XYsjZo4izbESdcYt2hJR7Uhsei5LI6k4zfz6Lc0dtREmrUhTowvmsBoKW5mI1KLBsS1/VzjMyi6G7RjaFUxTzm591ljnMaQwIgrHI8e9EOStD636MdQV3FyVFLdqScNZQrT0rlZTUkpxjeeef8EVGwCI7bxxJqq/G1sPK9l0KqeziRXVKDBf5TsFsezIHvfcPKzErFT8R3akamjK992Mp4Uak6kY/lxaUpgxAnFo8cPZMgTbv1IPUTiAGVdmnrSWFCX9Av19hKxqihFFcqL8jI7qj8p0pRQFk1g7tLG1z3EyUlFdSYcprtiJ2REfEg1lWLe9afRKbETH7yFpKX4GVL8KKjcW9sM9aavfoVKi5OfU9cwuvZA/iLFRTNjXHx61nCs5MneNyJ/vX/P0BrW21EcL4XXW/J8xsWTcufwFxTsa2dMQgy36sQFkJ03tRn+C8XldZw8cAvpb7ZXUnsUZC3iT7RHYfFm2qNHt1dFac/NoqZKEYONZ9JZQwwaFb9eP1P8/c5kyljVk5xFvFYYvNM0pHj8GjWzfHottFrpmLCCUiWH9vRRRKXqIHctB7tLO7Bxww4k2XnBWaZF7NEjuCMVNZDBtmkIQipIz7vbo05oJ1QRbUJWXLT4nldIBlWTARjXowXadGmG8mePibr1hrptL+3EhoJ1HxF1O9VH4xq20CetxoAq7nD3b4fph61RvVNfdAkUzQucUL9JDdjqk7B6YFW4u/uj3fTDsK7eCX27BBpnWUgK9u+NRg5s0fK1/qhqLQbJS6P1G6GootTj1qG9xmImVtXboJ2HtD4O8PVxN/xOgkaTbRhXmBaXL12BXqZA6bIej37iv8g2NrJCGXdXyPU3kHTZ3DyeRRn447s1OK9VoEJQF9TMuQObtt3RykmO3NifsWJ/0e1gheqt28FD2mQOvvBxN+wRSLtE2ppyr56YO/9VVJSn4NLle3BoMgFLxjUUe9sM7Wkci0qFXu6KcnaXsHPjBuxIsoOXswza2KM4csdUrhAFKvfujSY2elz9bRP23buNPzbtQiq5oF3PrjAsTiEiLi6biwux3t/nrfdLYr3vmtZbZma9n8e4eHIcGozA+tjz+HPTYkwd0gttq7tDdS8Ru7/sjx6T9uGeqZx5j9leSe1RY1N7NKCKmfbocdsrEUGXL+GKXgZF6bIwNEUmxa/XGwj9aH/+elmVcYerXI8bSZdNQ4r3QPgyZnHuncfZKzrRVpdHpResoM++h2xxdtCnHcTicWMwZswYjPvuLzhV8kFFO0KG3jSdILeyyj8IZNbWkFIMaHMN341ksC9TBo6mQgXr/mb8/bodDXXrkSGvhfEbtmLeey+hjpcSt89GYPPSqXijZQO8vuqqOGlZodb4jdg27z28VMcLyttnEbF5Kaa+0RINXl9lnElB+lxocvTSPQHY2KpNA8VS2VhDymWoSHIis7GDvWFZ5VAorcTSC8X8UoImJ0f8VwErqxI0A0W2sZFoLFViHqL+3FytadgzLnUrvt90VXp5FQmLu8DdxQUuld7GpjSxbbSJWLv8V6SbihrJYGNnb4wxuQJKK8MeKbBL5LArVRoOpoZeJ/anRmd+f4ngwz1j8OHg4nGG2Bsz7jv85VQJPhXtoC8Y2AUoKr6M3i3tQVd/w4Ydv2PzrlsgtxfRq7ObqURhZuNCrPcP+evdGR6uxvXenC7m+cB6P4dx8UTokXbkJ3w6aQw+/PEKqrw0CJMWrMQfMQmIntdBJB0a/LV5E6Kk3VOsx2yv9FJ7tAFbi2mPiB6zvZJocsQFl4ig/Lb14evlIpfWa2P+eslUKljJCJRbsB02rwQtF2P/YaIxj5j3Nf64LVID7/boXFsEfwVvlFPIIXPpgvlRZ3H27Fkc/3kahoyciE9GdhbjTNOKdCLzxDFEa4zf7sXGI1Gch+Re3uKb8UQjUaqUpr/E1UGFimJ6maHuL4//9UDdZW8cxead8aAmE7Hr4i0kx+7Bsv6BUORewfYtEeIMdQ1HN+9EPDXBxF0XcSs5FnuW9UegVS6ubN9imksB8lKo7FsaCv1dREUehzFd0eNm5BH8pRMnCv9qhiH5ZGLZTH8+nAKubi6QUy4y7mSIGh8ibxuLk1XeNjadhnHnTiZIbo9SpewMQ55telxZ+wO2pxAUbn5o1KQJmuR96r4gThriqnHLCqy/VnhrysQ+Kdbt3fjonQU4leOI0qWUuPfn5xg84xAyTaMLsaqAiiJ4ZTIXdJkfhb9E7J09/jOmDRmJiZ+MRGcvEfNysf+l2UmPBxinEjHkgeA+7eCMK9gycRp+vQG4d+qJIGfT+ELMxYW03t+L9daXcL2ft7h4UuSwStyK2TM+x+xJs7D+at72VMHW1nBpdf/4lol9bdzRD1yfPE575QXz7VGAwtQe6a8+XnslKFzdRFIikteMO6aLRWm9tpVsvQS6cweZJId9qVKmIQ9hupXE2H9e3jMwMjtPCqhZk2rWrEZ+ng4kLmpJZlWeev900fB2B2kTaHEnF5LL1OQdNIrmfDmZQv2sSSYvRcHLkwzPe+Q9AyM90+Hb9UOaM/sDCvJWkUxmT80/j6OCD/F6vLmF8m/RSnUHOZuve0Uy5SZ+Q51dpOdvvClo9Be0eNEseqdpaUP5+tNjSK9LpG86SdOrxPSj6YvFi2jWO02ptFxG6vrTDbMo+hBvdsR4qmktI5namzqO+JzmTutP9VzlYp296Y0NN4zTmJ6BUXf73vTgZS4dnVBdLL+Sqo07YihTVPpPIeQks6KA0YeMbwAIec/AFLuNf75k3MYS3VX6ur2a5MrG9GnBm93PKu1Zmt1CephaRQ0/iSv0fAhl76eR/tJzWDbUcs45MS7vGRg1dfvO9PBs7lGaWENFUFajcUdyRYyl0vZ3/Ugpk5Nb0Nd0ct9YqqEW+9m6Bo3eZ+4hXi0lLO5keCBT7R1Eo+Z8SZND/QxvBZUKXm54zib36HiqrgIp3NvR6C/CaFfeQ7ppa6mvu0I61RmeZ3irmAe7JekiloxxEUHZUixJ691cxLhpvQvt6QfWW3je4uJJuruLhgUYH3a19mpA3fq+QX27NaRy4viXfjOl6pgDxrZIxNKE6iKWFO7UTrQzYbsu05W/0V7ppPaoswspimmP9NpHt1cPSP+JQpxkZBUwmiLyGpZHrtdB0zLr6OrX7UktMz6b9yicwDCLkZfAGBphQ/IhJ6VdaarctDdNWn/G8CppHu2lTfRBC09xIBjLypQe1HTUZrpsak+NCYyMHMW0PQPtDT9IJpM7UtXXV1C84bnDYhIYwXzdmyjJmD3RxY0jqWU5tThYTeMVThTYcwEdk85JUomLG2lkC6/86SG9XRTYkxaYCjz4Q3bZdHbVMGrjnfcAnIyUbrXp9W+iyFTl30pgdEkLqb2taFDafWV8yFTIS2AMy2VYtoLb+HSh7UBZW2mAp4KUVceROB8/83KjJ1Ed6dVh2zb0ZUJ+GmeSSzFT65FKeqi11iQ6npPxiARGQ7e2vE0+ShnJXdrTfMPbZHdoz/BAQx2qKsNpd7qZH7LTXqJNH7QgT5HoGPePkjyajqJNeYGdGU4f1rQ3xp5IVAb9nrfHMmjbwPKGH6BTvPAe/XHPUJtZ9+NiIUnPg0vrXVssZ8nWW9T7nMXFk5Z5agW93cRTJKZ5x6GIERsvavrOd3QyP+/MpPAPaxqOeUBB5Qb9Rhf/Vntlao9aliu2PXpUe/UAXRItbG9Lcut2tLDAW27Fr9cKOpX3tpJY8q0DPEmhrErjDhd8i8A8TmDYMyyXUi9EUcTBIxR/rdCpN/81apc+aygz9xadORpBxy+kFb66fKji6zbITaOEExF04OBRiruaeb/XIl8upSWcoIgDB+lo3FXKfLDAg7R36NLJIxRxJJ7MzfKx6a7Qspecyco5lH5KNQ17DMbXsJVUa9KfYm3Y/1Nu6gWKijhIR+KvFU4qJdnXKC7yUJFxd2nbWxVFAmNFPkP3UqGfdynKFBcKERc/pjysoHkcF0+Clu4mxdKRg/vp4JFYSr5rroHIpmtxkXSoRO3BI9qrR7ZHj9Ne6ejKspfIxcqZQh9oWB6xXqZXsKVk+E8pGX4ETmDYc6lgAvMkcgFLlbFrCFVWOVO35VdNQ0oqk3YP8SGl60v0reE1dPbfdIcivptB00a9RJWl25DKGjT+iMbYm/MQUlz4KkVchF0xDSkpjgsmZOyioZVV5NxtOV0tycWZSebuoeSjdKWXll4q0cUkP8TLnktylT1cpTdI7EwPkT2n7FqNw9Re7ji2arVpSAml/Y6fdmSj2diP8ZrhNXT23yRH4tZPMXnOFpwjT7T56Ct8UD/vIeziSXExrVcZ/Ll6jWlICXFcMIldK3w4tRfKHFuF1ed0poGPkobff96B7GZj8fFr5Qu8wl88/teoGWPsGabNuIZLl29DWdYH5Z0f41+0Zuw/jhMYxhhjjFkcvoXEGGOMMYvDCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYvDCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYvDCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYvDCQxjjDHGLA4nMIwxxhizOJzAMMYYY8zicALDGGOMMYsjI8H0d/F0l7B/9W6cy3FA1Y7BaOCel/fk4NwfK7E/SQ2/9j3Q1EthGv5foUNi+ErsvaBD+eZ90NbXyjScsadMl4K48L04cVMF74Zt0NjbzjTiITRXEbXnAM7cc0GVJi1Qw0NlGsGed7qUOITvPYGbKm80bNMYDw2nu1dx7uod6B5o2WVw8PCFpxNftz5fdEiJC8feEzeh8m6INo29UYLWyEh/FZEb4+DapS38imuONCm4mKxFqUruJa/3SZESmEfK2kivl1GIw0FB5fqupWta03BKo6Wd1CSTl6UB27JMw/5LMmlVT0eSQ01dwtJNwxh7ytL20ZQWZUjl4Ek+3q6ktq5I3b8+Sdmm0eZoTq+g1wLsycrOg3x9y5K9ox/1WBhFGabx7PmVtm8KNS+tIgdPH/J2VZN1xe709cnioynz5x5kL4OUvhT5qCno21RTKfZ8SKN9U5pTaZUDefp4k6vamip2/5piSnS6zqTouS9SGZsgKjZsdDdoyyB/Unu8SVv/hRTgMVNxHZJXjsXk31MNRwRjrKgcHJ0zFLMud8APJy/iXMIlRM7wx6HxI7E0QWcqU4T+IpYNG451qn5YF3cZZ88m4uTSJoj5cABmHcs1FWLPpZyjmD3kMyS9+ANiEs8h4VIkZvgdwviRS3FBaypThHXbKfh1zx7syf9sxZxgL1j79sHAzk6mUux5kHN0DoZ8loQXfziJxHMJuBQ5A36HxmPU0gumEsXIiMeqke3RYfQO3NSbhj1Ah4s/Dca7y8+hmFB86h67L5G0F7B89CfYd7e4FEaH6we/wcjQFqjlXxkB9dqj//RNOJNtGo10bJ/SCyEvj0HY6o8RXCcQNV+cgt1/rcOHL4eg59T12L90CILq+sO/Zlu8veRP3DyzFuNDGiHQvxqa9ZyG35LunwgyTv6AD0Kao4ZfJVTyq4bGXYfgm8OpKHabM/Y0aU9g46a/4PfaSIRUlG5Z2qHWu2MQ6nQQ67Ykm0/87+zFjkOE9iOno2sFaRoVvEPG4PXAeKxfH/WvNQ7s36c9sRGb/vLHa6NCUFEpBtjVwrtjQ+F0cB22JJtPiOVlqqJ5y1Zo1cr4qXFvCxbtdMXQZV8ipCzfPnp+aHFi4yb85f8aRoVUhDF83sXYUCccWr/FWMQcfSK+Cm6EQTsqYNTUnvAsJmRyYhdgwLhEBL/V0lD3v+HxotmqInxfsEJu/CKM/PwI8nOSAtJ2foC27Qdj3qYY3HFwhVXyAXw/uQdav7wYZ6SWWK9BwqFN2LB+EUYMnIxNUacRf4PglPMXwjdvwLovBqDb6K24Idrxm7F7sPS9INRs8QZWnBOzv30Wh9ZMxeuj1yJVmln6NozsNgBfbD6Fe64VUc4mDdFbF+K9VyZjbzb3EbF/QZaI5wRr+FWpjPwnrpSBqOKjx9m4M8bO/CL0mnu4p7VFqVK2piGCzBH2duIK58xZaEyD2PMn63Q8Eqz9ULWyFWSmYcrAKvDRn0X8mRL0zmXsw8cf/AB5/7kY39zBNJA9H7JwOj4B1n5VIcLHRInAKj7Qn403fTfHCc0n/I64479gWJ1ieuwyIjD9jdnQDQvDR/Xs82Pz/+3xEhhFTQz7rB8qWmUjau4ozDtZ5ADSxeObyYsRp3FBuzkHEXvsMGJiNuO9QBmu/ToF0zammQoK+kzo/d/HL3t3Yf2C/qhu2sCU5YOhe87gWMQ+fNrBRiSRKchpNB9HoiJx9McBqKDQI+3EcfyVI2Z31xq1+g3HBwu2ISpyD/YdWIxXPBXQJcchNoX7YNj/n/5uOm7rbOHkmN9iiKPMSXyXIeP2bdOAwuQuNVGjQirCt+xF3hGiu7wNO6K1yLmXYfZCgT0P9Libfhs6OycUDifxXZaB27eLuSWZT4dzS6di2a0XMWF8azj/W2cZ9u/Q30W6iBE7J8f7F1PilO8kvssyzbdFBnIX1GzVCJ7FPbSrT8FvYwfie69pWDayBtSmwf+Gx+xPlMEp6GPM7OEJhcjAPhu5FJdEnpB/UZmyH+HRIrOwbYl+/atCpB+Ql26NN0OrQKm/hUN7o4zlJDIVmgwchx4t26BLsxeQ9/6S3KMemgWqIRONvnsZKbNToEqLVvAQBZSenigjlpg0WYarUkX5thjwRmcE3PgJA9vXhXf5UKyQulVJg2zugWH/BoUVFA+cKAjGd/3Ef/Q3EbNzIzZs2GD8bI7EJUVDDPkoBLnLX0bD9v3x3uA+aNV+Hq56OUGuVBVofNjzRmGlePDqVgSTMZz0uBmzExvzYmnDZkReKpDU5BzBt0sOwePVEejJt46eQwpYPdgYSS/uGP8w1xY9KicWSfWlle/hvT8aY/7i/qj0L794/PhRLffAyzOnonMZGdJ3f4r5h+73wuhzNcjRE2RKG9jmp2Uy2NhYi/8TNNkFriVl9ihTxvGBBZBZ28LGtM3lCmmsDNZ2dsaDWK7ILy/tA925b9CjYTsMnLYCB246o/7LvdHMXdqicsge3G+MPXVye1e4qDOQll7gyRV9KlJvE5xc3QDteWyYORZjxowxfsaF4Vi2AhVfXYGDO2ajp68c2VYBeOuHbRjpT3ArUwbS0cOeR3LYu7pAfTcNhcMpFbfJCa5uhPMbZmJsXiyNGYewY9n5F5SaiF+wPsEfvV5vzDH0PJLbw9VFjbtp6QWeo9MjNfU2yNG1mLbIVKw42liEzVmPq2m7MbZlFQQEBKDexD3QpKzH4Fp1MXxrlqng/8ffSssVFfvjswmt4Yw7SL9z/1aNvFRl+JZWQH83ChHHTQeSyPIiD/8FnUwF/2pVDOWMlFCZe/LHTOJhPhfRIWHdd9hxnVDuzTU4Hb0Laxa8Aj/T5aqMMxj2b7Cujuq+OYg7cQo5pkHIOoGYMwr4Vw8E1I0wefcZnD171viJXYIQm2T8sXAWdqp74+NFYVi2YBL6VTuN8KM6VK1X+197QI79+6yrV4dvThyiT+XkJyZZJ2JwRuGPaoF2aDR5N87kxdLZWCwJMV3siVPWyR17kPRCB3Sqwn14zydrVK/ui5y4aIjwMcnCiZgzUPhXA1Rm2qJH/ZCL3A0New/FoD5d0bFjR8OnfQ13KNTlUa9DO9Ty+v/G2t9KYKSuKf935mB0wyIP76hb443Xq8FadxoLXg3GqNnzMH3ASxi1JR2yiq9g2KsVTAVN/lGOIYONvS2s5Hpc370En8+fi3GvDMfKK1IfWBYyM/MOd8b+j8SJJbRHPSQtn4g5h1KgzUnGb1OmY31OO/Tu6m4+5OW2SI9YiBGj5yEiTVwQZCdi84SPsErfDQND3P/uQcqeAQr/ULxc7zKWT5iDQ7e0yEn+DVOmrUdOu97oauhtLoY+BceOnYdd7Qaozr+H+JwS5+nQHqh3eTkmzDmEW9ocJP82BdPW56Btr66mMo9J7omg0XMwb968/M/MnlVgZV8fA2Z9hv61/7+XW3+/bVTVwMi5w1DLumCTrEajj9bgu/dbwePmDswbPQKTvjsBWc1+WLjhS3Qv/SSbYjnK9hmP4Q3doE/YhOnDx2JFSie8260sFLpziP7T8J4SY/9nCgS+vxCftU7CjOZlYe/oja5Ltei9aB76Ffc+IlwQPH0eetyZhxYeTnBy9UPfX73w4U/zEFLmX77JzP5dikC8//UstLo8Ay087eHo3RVLdb2xaF4/PPSHz7UJSLhMKOdT6V99yJL9uxSB7+PrWa1weUZzeNo7wrvrUuh6L8Lcfl6mEpatZP+UwN+gu3sZ8aeTkeXwAqoHuD+9e7C620g8GYfrSm9Ur1oWBV5EZexfpEN6Qgzir8nhVa06KjiUIHk3xHI8big84RdYAc7c88/y6NKREBOPa3IvVKteASUJJ8by6NITEBN/DXKvaqheweGZ6dV9agkMY4wxxtjTwnk8Y4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLo5gimP4uni4Be37ehL1/RiE6JgFaT3942ckMo/Q3jmLD2h2IjIrGibg7cK7qDWdKRPgvG7EnOgUOVSvBteh3s2lTBk5t+wXbIs4hxyMA5eyN9TP2X6a/GokNR+7Bp5IbFKZhBroUxO39DX8cOY8M+3Io76wyjXi0YutENi4f3YHf90XjstYF5cs6wMo0hj3bcm7EYPdvu3Dkr1So3CuglM2j20fN1Sj8sT0cMVf1cCnnDvvCwcSedfqriNxwBPd8KsGtwL7PvpmIC0nXcSslBSmGTypua9VwtS9BG1VMnZK/E6P/GJVE5k8Uai8nqThkdhT0zVXSGkbo6NqSILKXi+FinMyxJ63M0Ivyq6ino4yg7kJh6dL0q6iXk5g+77s52nM0q5mKoKhEQ/dkk6iFsf+2zGia26EM2QR9SykFAzZtH01pUYZUDp7k4+1KauuK1P3rk5RtGv1QUp0vuhvqTDUNMsg9TWF9/MjW2o0qBfhSaRsb8u6+gE5kmcazZ5SOrm5+n2o7K8mxnB/5lbUjZZkWNCU8zTTeHA2dXvEaBdhbkZ2HL/mWtSdHvx701fEM03j27Muk6LkdyN02iL4t1JBk0to+riRyD+P53PCRk0ufNfTopsRYZxmbIFpSqMEzH6OT9xaa8VPxWLeQZDIZZMjCkb37kSmtOjKxb88RZJEYLsblU1VDrwnTMG1yX9SxNg1j7BmSEb8KIzt0wOidN6E3DTPKwdE5QzHrcgf8cPIiziVcQuQMfxwaPxJLE3SmMuYZ6mzfHqN33ChSpx7Xfx6H0b+Wwdjw8zgffxaJR6bDN3wixv94tUhZ9kzJOYw5oxfjdveViL1wBmcSTmBx03P47IOvcCrXVKYI/cVleH/YOqj6rUPspbM4m3gSS5vEYPzAWfhTayrEnl0Z8Vg1sj3aj96JG0WbHO1ZxMRnofrQVdi9Zw/2GD67sWViSzy0/8VQZwdDnTeLNjjFxOjnoxci9inH2+MlMPaV4OshQ/qhvTiiERmM5gj2HEoHylaGT8FbPvp0JMQcR1T0OaQU12brb+Lg/IFoX8cffjVa4/XZ+4svy9h/iD7xKwQ3GoQd5Udhak/PwgeR9gQ2bvoLfq+NREhF6QaPHWq9OwahTgexbkuy4ZLHnPw6K4x+sE5BW74d3vtkOoY2cDJ8tw1ogQblcpB8+TonMM8y7SVcvKpEjdbt4aUU39UvoHP7qqBLF3BBaz6a7uzdgUPUHiOnd0VFaRqVN0LGvI7A0+uxLoozmGeaPhFfhTTGoB0VMHpqT3gWbUgyT+JkQik07NIVrVu1QivDpyWaBpYuPhmQ6gxuhHd2lDdf50Nj1FjkaSl2mc2yqo6mDR1BVw5g7ykttKf24MAVgmPDpqhW8Ga8NglHtmzEhk2RuGj2KkGH+Pm90GXEMuyKTYXC5i72zXwbXxwt5pKCsf8Sp+aY8Hscjv8yDHUcTcPyZJ1GfII1/KpUvv98ijIQVXz0OBt3xthpa05+ne+jjlPRe8dyeLV+F1PfbQUX3T3cOH8Ya6Z/hO+T66JXcBV+DuZZZt0Ends6YO+C8fg5UlzdRvyISUsOo1S7IDRQmXvGQA/NvXvQ2pVCKVvTIEHmaA873UWcOasxDWHPJic0H78dccd/wfuicSp4Y0SSEx+D0zkeoKMT0fel9mjffQCmro7FXdN480SdE35HrKnOBxQTo27tOooYNZV5Sh4vgYE1GjSvA7X2NPbtTUTi3n04o1WhbrP6YsxjyDmMZYv2I13mgeBvjuHUYfHZ+SFqckvMLIDcpSZaNfI02+Wqv5uO2zpbODkWCGa5k/guQ8bt26YBD3pYnQXlHPsEneq1xasfh8Om3Sto780HzTNNXh69P5mOtjeX4LXGAQho0h/fZXTDrI9D4GH2oVw5XGrWQIWUcGzZm2bKl3W4vG0HorU5uJeRbRjCnlFyF9Rs1QiexTQkt0+eQmLWSezcp0Fgm45o4nYa37zWEt3mxCDHVOYBj6izuBj9bHoI3B8zw3hcj1m9HG5NmyJAmYuovZuwaU8Ucq2qomlzt8er6G4sYi/qILNtgpDQCoY3LexqByMoQOp/YsyCKaygeODCmECGM4n4j/4mYnZuxIYNG4yfzZG49Bi3TlUNp+NYWgZST36LZidHo/PAn3GF7yE9s/TX1+LtoGGIrjsTv55KREL0BkysvBdvdxqJnek63IzZiY15sbRhMyJFMKkaDsGkkFwsf7khOvR/D4P7tEL7eVfh6SiHUsUJ7/NLD2WDAfhs4Xrs/nUBJowYhanLdmDN+x44OHsutmfqzcbToxQXo4M6j8IfxV+zPRGPnR8p/FugsacC9w7Ox5cH70FerjFaVjZ7KVAsfW4utIYWXQ553hLIrGGtNv3NmIWS27vCRZ2BtPQCN3/1qUi9TXBydQO057Fh5liMGTPG+BkXhmOPdVFsPGDsq/TBjCFNkL5jI8KzDIPYMyhlywqsu/0ipnw7Eh2rVoR3zW4YHzYRzS5/j2W/38b5DTMxNi+WxoxDmAgmUlTEq98dxI7ZPeErz4ZVwFv4YdtI+MMNZcrwWxXPLzmca4Zi6OAgeOefsu1Qt0UDuKSfx7mbOYZ4+rBgPB19dONSXIw2T/oeS3/PNJV6Oh47gYGqHlo0dIQs/TIupwNODVug7mN2nMidK6KimxyUfRqnYk0dV7dPIOYCP8XLLJx1dVT3zUHciVP3u2SzRGyfUcC/eiCgboTJu8/g7Nmzxk/sEoTYmcoV6x5+H9MItV7/qdAbABpNLmClhtlHIdgzQA+9VgudTAVVgTZWbmMPO4VW7H8rNJq8G2fyYulsLJaIYKLkP7Bw1k6oe3+MRWHLsGBSP1Q7HY5juqqoX5t7uZ9fmdg5uSu6Tf4dd/KfxdMjNSkZGdZl4OFibYin0wXjKdTeVK44D4tRnYjRp9s9/PgJjNwBTZrVhkqaUqZGvRbNxIIaR5WYdUv06lYRVto4fPXWAEyfPxvv9xiF9bf04LaYWTSFP0J71EPS8omYcygF2pxk/DZlOtbntEPvru5/M76tEeDjiktrZ+KjjYnQQIfUo4swcn4kSnd9Ga0LPKzJniVylGrTEY11v2H21G1IkC6G753HholzsEPeEkEtzWe+ctt0HPpqBEbPi0CqOH9kJ27GhI9WQdd1IEKe9kMJ7D/MGpVK3UP43ImY+utFQzuScuRrDP18H0r36I9OxhccH9NDYlTWAp2KidEn5W9EsxxlmzdDoHQr1aoKmrUoXeTXQkvCHm0/WYHJ7TyhOfkTJg2fgHV4HcPEFuQEhlk2BQLfX4jPWidhRvOysHf0RtelWvReNA/9Hnj/sKTkqDhwEZa97YSNvSvD2d4B7o3H4VSdj7F6ble4mkqxZ4/CbwiWfvsa5D8Ew8/VEQ4ufnhlvT0GrViENysW0/K6BOPjL3vgzrwWKOvkBFe/vvjV60P8ODcEpTl/eY4p4DP4G3zzKvBDsC9cHBxQtsl4nKo9Ays/7wJnU6nHVVyMvr38a7xZ4ekGnEz6NTvT3/+CbFyPj8FFfXnUqFr28d5kYuw/TYf0hBjEX5PDq1p1VHB4Mgdy9vV4xJy/A5vyVVG1vP3fuQJhlkhzA2dOnkOKrAz8q/vCrQSvp+puJ+Jk/A0oPP0QWMGZX7dn+bKviXbkwm1Yl6uKahUcnkw78jdi9J/6lxMYxhhjjLHHxxdwjDHGGLM4nMAwxhhjzOJwAsMYY4wxi8MJDGOMMcYsDicwjDHGGLM4nMAwxhhjzOJwAsMYY4wxi8MJDGOMMcYsDicwjDHGGLM4nMAwxhhjzOJwAsMYY4wxi8MJDGOMMcYsDicwjDHGGLM4nMAwxhhjzOJwAsMYY4wxi8MJDGOMMcYsDicwjDHGGLM4nMAwxhhjzOJwAsMYY4wxi8MJDGOMMcYsDicwjDHGGLM4MhJMfxdPl4A9v4QjMVcUlTuhWlB31C9jzH30N45i42+ncFsvKlNVQqveLeGtMIx6fBmnsG3dUdyyq4IXQxrC46mnVzrodAoo/u7ysuee/mokNsa5oktbP6hMwwrTIOViMrSlKsHdzjSoONnJ+DM8AmdSADf/JmhZ1xPWplFGOqTEhWPviZtQeTdEm8beeFSVzILoryJyYxxcu7SFX5Fg0qXEIXzvCdxUeaNhm8bwLvGO1+Nq5EbEuXZB26KV5tGk4GKyFqUquXM8PUuKjScNrkbtwYEz9+BSpQla1PAopu0qKBvJf4Yjwtg4oUnLuvAs2Dhl30TipVRoCmQTMisnePp4wN70/amQEphHyvyJQu3l0qIRZHYU9M1V0hpG6OjakiCyl4vhYpzMsSetzNAbxvwd2nOzqKkSpKg0lHZn/f16Hk1DSXvm0ZvNXqJZ8bmmYYw9psxomtuhDNkEfUspZsNVRze2vE3+1h705tYs0zDzMqMWUDdva1K7VaIqAeXJWWlDPqGLKSZ/sjTaN6UFlVY5kKePN7mqrali94V0Mts0mlm4TIqe24HK2ATRkiLBlLZvCjUvrSIHTx/ydlWTdcXu9HUJd3xm9FzqUMaGgr5NJfMheoO2DPIntceb9IgQZRbFGE/utkEkdv19mtO04rUAcrCyIw9fXypr70h+Pb6i4xmm8eZkRtGCbt5ko3ajSlUCqLyzkmx8QmlxgRjMXNuHXE15QN5H7tKH1j7lmHqsPg6ZTAYZsnBk735kSouITOzbcwRZJIaLcRZDcwifDRiFsMM3IHUqMfa4MuJXYWSHDhi986a4xjVPd/EnDB68HOdyTQOKo0vEstETsO+FqYhIPI/Y+ESc3zsaHn+MxdgVSYYiOUdnY+isy3jxh5O4eC4BlyJnwD9iAkYuTTCMZxYsIx6rRnZA+9E7cbNoMOUcxewhnyHpxR8Qk3gOCZciMcPvEMaPXIoLWlMZszIQv2okOnQYjZ0PVJpHh4s/Dca7y8/hoVUxy2KIp/aGeLqhMw0z0OPisvcxbJ0Kr62NxeWzZ5F4cimanhyPgbOOFRMDOiQu+wDj972AyYcScD42Honn92J02V0YO2aFqYwWZ2PikVVjKFbt2oM9e4yf3VsmosWju3b+kcdLYOwrwddDhvRDe3FE6ivSHMGeQ+lA2crwsS+SwOiu4+A3IxHaohb8KwegXvv+mL7pL2SbRkv0Nw9i/sD2qOPvhxqtX8fs/SkPnAz0KZFYPKQzGgRWhn+t1ugzeR1OZ5lGpm/HlF4heHlMGFZ/HIw6gTXx4pS9uCsO3pM/fICQ5jXgV6kS/Ko1Rtch3+Bwqqhdn4x1E6dj+w3xt+4cVg7vjTFrLxvma25eZwouMGOCPvErBDcahB3lR2FqT0/zB1FOLBYMGIfEkLfRUmkaVhzdLcCjGQaOGIzahv5WOVwb9kO3ahqciTsrvmtxYsMm/OX3GkaFVISVGGJX612MCXXGwXVbpAmYpdIn4qvgRnhnR3mMntoTnkWCSXtiIzb95Y/XRoWgohRHdrXw7thQOB1chy3Jhc5OBeiR+FUwGg3agfKjpqJn0UpNcmIXYMC4RAS/1RKPClFmIaR4CmmMQTsqmImnO9j7+yFQ+5GY1tXYjqi8QzC6XyBOr1+PKLMZjA634IHmA0dgcG0HwxC5a0P061oNmjOxhu9SR8bJkwko1fAldG3dCq1aGT8tmwai9GNlGI/v8aq3qo6mDR1BVw5g7ykttKf24MAVgmPDpqgmbY18adg5qi06DJ6HTTF34OBqheQD32Nyj1Z4+ZszMFyQ6uIxv1cXjFy2C7GpCtjc3YeZb3+BowU3Ys4JfB4chPe+/h1nc53hcDcKq6a/ghffXo2rUi6iScChTRuxftEIDJy8CVGn43GDnKHZNgLdBnyBzafuwbViOdikRWPrwvfwyuRwZOszcTEqGpelJIju4PyR/Th+8R5ImldIpwfm1eGtNbhSXDvBnk9OzTHh9zgc/2UY6jiahhWSgYiP38Rs3TCETaoHe/kjeidV9TD0h22Y9ZKxgZDoLm7HzlgV/Kr4iW9ZOB2fAGu/Kqicf5wpERjoA/3ZONN3Zpmc0HzC74g9/gveNxNMWaJNS7D2Q1Wx4/OiSBlYBT76s4g/U3zXnlPzCfg97jh+GVYH5kM0AtPfmA3dsDB8VM8+v25m6UQ8jd+OOFM8Fboxotfg3j0t7EqVgq1pEMSed7S3E+3NGZzVmAYVokK9oT9i26yX4JhXl+4itv8RC5VfVeP3nHjEnM6BBx3FxNdeQvv23TFg6mrE3jWOfpoeMz+yRoPmdaDWnsa+vYlI3LsPZ7Qq1G1Wv9DDhrr4bzBpcRyyXdphzsE4HDscg5jNQxAou4ZfJ0/DxlRCzuFlWLQ/XVx5BmPJn6dw+Ngp7BxXEwWfp834bS7mHrwNq9ofYnv0ERyL2Y4xNYDLq2ZjWfz9rEKfqYf/+yuxd9d6LOhfBVnWtdFv+AdYsC0KkXv24cDiPvBU6JAcF4sUmR9GbluHtyqKVbeqgwmHkvHHKH9k/fYF5h1MN8zr9xNHC8zrczEv7mBl98ldaqJVI89iHnzTI+W3sRj4vSemLRuJGn+jC1WfsgeT+kxEpM9QTOzrKQbcRfptHWydHA1XTUZyOInvsozbpu/MIsldULNVI3iajRM97qbfhs7OCY4FLhDlTuK7LAO3RUyYJ4dLzVZoZL5SKcDw29iB+N5rGpaNrAG1aTB7BjwsnqRxNSogJXwL9qaZnp3QXca2ndHQ5txDRknuNojY2TOpDz6KqIShH/U1Drt9EqcSs3ByZzg0gW3QsYkbTn/zGlp2m4OTOcYiT8tjJjByuDVtigBlLqL2bsKmPVHItaqKps3dClWUsn8vosWC27Z8Df2rSqmNHKVbv4HQKlbQ3zqE8CgN7sbG4qJOBtsmIQitIKUtdqjdPQgB+QeqFqePRiFVL4drOTtc2rkRG3Ykwc7LGTJtLI4cuWMqJ3JIVRMMHNcDLdt0QbMXVCjfdgDe6ByAGz8NRPu63igfuhxSbytpspFt9pkX07x0pnnt2FBoXkcLzIuxh9FfWokh7/2BxvMWoX8lM6+36W8iRorlDSLGpM/mSFwqcB7KOb8GQ9qHYMG9XlixfiqaGm7NKmClePAamUrwAiGzbAorxYO9I2K/G/Y86XEzZic25sXShs2ILBhMZulxaeV7eO+Pxpi/uD/MhSh7VqnQcOgkhOQuR89GHdD/vcHo06o95l3xhKNcCZXVI+Ip5zzWvNcOwQvuoefydZjaxPjOml7ZAAM+W4j1u3/FggkjMGrqMuxYMxQeB2fji+2ZhjJPy2MmMOKA8m+Bxp4K3Ds4H18evAd5ucZoWbngUaBHriYHepJBaWN7P7uX2cBGymVIg+xsHXJztdJxKJZAnr8QMmvrAlcDemTfyxYHqh5pBxdj/JgxGDNmHL77ywmVfCrCTp8hxhjJ7MugjGNeLTqc+yYUDdsNxLQVB3DTuT5e7t0M7tIiyqWHkM0pPK9xRedF9+fFWPG0iA2bg/VX07D7w1aoEhCAgHoTsSf7FtYProW6w7eBcs9jw8yxIr6kGBOfcWE4Zrjy0SP1wAx0adEPG11HYMuurxFiSOwFuT1cXdTISEsXc8gjyqfeBjm5mr6zZ48c9q4uUN9NQ3qBTmB9aipukxNc3QjnN8zE2LxYEm1WmAimh6a14oIsbM56XE3bjbEtqyBAxGi9iXugSVmPwbXqYvjWrIdPzyyaouKr+O7gDszu6Qt5thUC3voB20b6A27iHGqtNcTThwXj6ajxgVN96gHMeKkl+m1yw6gtu/B1aMX8uyVy55oIHToYQQV+P8Wubks0cEnH+XPFv+TwJDx2AiPdr2/R0BGy9Mu4nA44NWyBuoWeAJOjVGVflFbocTcqEsdN3VL6m5E4/JcOMpU/qlaxhnPFinCTE7JPn0KsqZvp9okYXMhP+KxQwbscFDIZXLrMx/G/zuLs2eP4edoQjJz4CUZ29rq/8ErV/YfQdAlY990OXKdyeHPNaUTvWoMFr/ibut6lt6Wk/8nF/6U/xJWM4Wg1zUskONK8os4WnVe5v7Gh2PNHDreGvTFkUB907dgRHaVP+xoieVajfL0OaFfLE1A3wuTdZ0R8STEmPrFLECIuZDKPzES3rp/ialAYwrdOQstCT79Zo3oNX+TEncCp/C7ZLJw4eUZcUFQ3fWfPIuvq1eGbE4dosePzEoss0U6eUfijWqAdGk3ejTN5sXQ2FktEMJm/SDORu6Fh76EY1KerMT7Fp30NdyjU5VGvQzvU8rr/rA171uiR/MdCzNqpRu+PFyFs2QJM6lcNp/cdg65qPdQW51Epnk4XjKdQe6lxwsxuXfHplY4I27cNH7UsXeh8mLlzCrp2m4zfCzzzok9NQnKGNcp4uDzdc6fxbepHMP0OjPG9bh1dWtCWbKR3vmU21OHrK6TNWkt9XOT3fwcmO4LG17QmmUxN3h1H0Odzp1H/eq4kl1mR9xsb6Lr0IzJ3d9A7L1iRTO5INV6dRl9+PpRaeyrFNLL834HRJiymIGdRr9qbgkbNoS8nh5KftYzkpYJpRbKOdNe+pvZq8b3gbxjokuirtjZiXiqq1O0j+vKLD6m7nx2Jg5KU9abRSelnX3KP0vhqSoLCndqN/oLCdiVTjphXJxeF2XktTzL+6g1jhWXTb295PeR3YIQ7y6mrbdmH/w6M5hhNrmdDar/eNHNZGIWF5X2W08oDSYYi2rjPqLljGWo74yDdytVQ0q8fUH1nN+oWlmwYzyxf9m9vkVfR34HRxtFnzRyoTNsZdOBmLmmSfqUP6jmRW7cwKlGzlP0bveX1kN+BEe4s70o2/DswzxwpnsoV+R2Y1DV9yN25GU05mEI6yqKETcOonlM56rvmuvhujoaOTapL1mo/6j1jWYG2KYyWrzxgKKE9N5/aOjlQvZHbKFHEkPbWYVoQ7E3W3gNpa5qhyFPzNxIYcf6Pnkx1VCKBUdalaadERlA0gRGyz66i4W28yU4uky4cSKZ0o9qvL6Hou4bRgo5SwqdTh/JqQ3IhU3lS63EfUGeRsNz/ITstXdr0AbXwNJYBZKT0aEqjNl02bGyzCYwYk7prPDUuJZIjw3w9qPnw0dS9rILkziLxuS7Vm0nhY2uQvUyqU0HlBu0Qp6Ji5rU5yfSjfYwV9WQSGE3kGKqilGKu6EdOZQdsNZXKophFvchPHIdKtZqsrJyp9qBVlMDB+cwwm8AIWTGLqKefPcmValJbWZFz7UG0uqQ7nhOY55a5BIa0F2jlwBrkYqUie0cbUjpUpq6f7KWb5rMXqXGiMYFWZtomkLzsAFOhXDq38h2qV0pJVjb2ZKNUkINfCH0R+ZSzF6Fk/5TA36bD3cvxOJ2cBYcXqiPAvfAPoxtkX0d8zEXoy9dA1bJmxku0aUg4dQZXc53hUz0A5qopSnc7ESfjrkPpXV3Ue/+lsfs0uB4fjQsZTqhUsM6/MS/G/l906QmIib8GuVc1VK/gwLc2nxe6dCTExOOa3AvVqleAA+949rfpcDvxJOJvKODpF4gKzgVecfsnsq+Jc/kF3LYuh6rV/j8x+pQTGMYYY4yxJ4/zeMYYY4xZHE5gGGOMMWZxOIFhjDHGmMXhBIYxxhhjFocTGMYYY4xZHE5gGGOMMWZxOIFhjDHGmMXhBIYxxhhjFocTGMYYY4xZHE5gGGOMMWZxOIFhjDHGmMXhBIYxxhhjFocTGMYYY4xZHE5gGGOMMWZxOIFhjDHGmMXhBIYxxhhjFocTGMYYY4xZHE5gGGOMMWZxOIFhjDHGmMXhBIYxxhhjFkdGgunvR9AjLfZXrFq/D7FXMiBz9ESVZt3Rq3M1uHAaxJ432cn4MzwCZ1IAN/8maFnXE9amUQXpr0ZiY5wrurT1g8o0rFgPqfPu1XO4ekeHBw5WmQM8fD3hxMeg5dNfReTGOLh2aQu/gsGSfROJl1KhKbDzZVZO8PTxgL3pe/H0uBq5EXGuXdC2UKWA5moU9hw4g3suVdCkRQ14PDJAmWXIRvKf4YgwNiRo0rIuPIs0TtK+33sgHmnWldC4TSNUtDONKFY2Lh/djYhzmXAMaIrWtT2hNo3Jk335KHZHnEOmYwCatq4Nz6IFngYpgXk0DcV9E0IVVDLpELr/kamoQvBiOpVtKsbYcyAzagF187YmtVslqhJQnpyVNuQTuohOFj0OMqNpbocyZBu0hFJNg4pjvs7FFJNlGEs/h9qTyFEKH3/SRx1E3z6qcmYBMil6bgcqYxNES1L0pmFGmWv7kKu88H6Xu/ShNfcKlzMnM3oudShjQ0EiSO6X1tDpFf0owN6K7Dx8ybesPTn69aCFUZmm8cxiZUbRgu4vkLXajSpVCaDyzkqy8QmlxfmNUzbFLX2F/B1sqLRvAL3gqhbje1PYXzmm8WbknqawVyqTnbWoM8CXStvYkHf3BXTC0DZJcul02CtU2daa3CoFkG9pG7Lx7k4Lou+Zxj89JUtg7qylvmUUJFOWp44TwmjTju207quBVMdRTpC7UPCKa6QzFWXsmaZNoPltHcil9Wf05x1pgI5SDk6iZs5OFLQ4yVBEcjduJY1oUpqsZDJSPyqBkeps52io8/hdaYCxzqZOos5Flw3fr5/aR3v37KE9eZ+tcyi4nDX5vrGOrvDBZ9nuxtHKEU2ptJWMZCIhLZzA5FL0pNpkU2sordp1f//vPRBHNx663+9S3MoR1LSMFclk6kIJjC7xa3rR2Y5qvLuJLuaKAZoEWt3fjxzqTKKjOY9Oith/lZYS5rclR5fW9JmxISFdykGa1NSJnIIWk17sWu3pL6iVkye99FWMSJnF+NSDNL6+Pbm/uobSDVMUpaNr3wWTq1MzmhqZZhiSeXI2tXNxos7fXjGc93XXvqNgVydqNvUwGUpknqTZbZ3JqfOSp942lSiB0V6YTS1UIut360E/3TINFJncvsltqXbN2vTi9EMip5fo6FbEInqvU30K8PWjmq1eoUlrT5OUqOkurqbRPYIppP+XdDg/c8ug8M/6UnBwb5q2w7hxdLciaNF7nai+yPT8araiVyatpdN55dN+o8kvB1OP0cto1fTuVDugBnWYvJvumJ1vvGG+jD1RmqM0/9VONHrznftXtNpzNKuZDVUausfwVZewgNo7OVPV3rPok15eZPOoBEaqs6+xznxSnU2tRZ27TQMKSqFt71Qmh+qjaZ+xnWKWSpdAC0Ty6ly1N836pBd5PdADk04/BDtT+UE7KKvEuYWOEha0I0enqtT7s0+ol1fhHpi0Fd3IwakbrUgxDRC0cZ9QA5tqNO5wToGeGmZZNHR0/qvUafRmcU7Mo6Vzs5qSdaWhIoHJoZNT65Jt/Y/plJS4GoiLpeMb6ftNUZRqNtnQUdLuhTRp4Z77bVjuERpf3YZqTYoS6bUokbSbFk5aSHvuF6Aj46qJpHsSReXP5+koWQ+MJoI+rKYiGWRkW74x9RgyjRatO0jn07WmAkaa6E+phYucZApn8qlXlyo5ib+VFajv6iukzT1Bk+soxbjy9PZvGcYJ0tbQq+4Kktm3p4UXxdbTRNOnzZ1JLlOQs089qlvJSfytpAp9V1OymJXu2tfUTkqk7B3JQS7dzlKKjXicMsV8mzubn+9TTgAZI+35r6iDuCLpuDjZ8F2XGk17IpJFc5JNv73lVaJbSEVJdbZ3dqSOi+736uS5Gz6Cqtj50ZCdxqSfWTBdKkXviaBkcQWY/dtbDyYwou0dHWhL9d/+hEa92oXatetGb05ZRacK5LoP0lFq9B6KMFZKbxVKYMQV9dftyabsANpa4ApPl/QltVbZU8iPdzmBeZZoz9NX7Z3JsaPUA5NO33d3Iu93N9HhHydQvy5tqW3XN2na+tOG3phH0mbS9XORtHrSi1TOtRnNjDJ2W9ynpczr5yhy9SR60cuVms08burYeHpK+AwM0Z3DX1Cwr51IKPLuw8pIbudNbYb/QnGGW113aWM/d1LIVFRnwmGSUpSMiHFUUykjZf3pdDJXS+e/aEU2Ijnx7L9RlCa69UMIuSrk5BL8vaE79O7G18ldJCaqOhMo8q44jDJE4lRTJD3K+jTtZG5+AgOZPdUbvorCd22h/RfSDfOVG+YbaZrvh1RLZZzvqcI5FmNPlO7Wbhrf0Jkc6k6gg6a8/L6/l8AUrPOAdBwUpD1Lc1s7UungFYaknj07zCYwN5ZQkI2MrF/oQEM+/oJmT3qTmpRVkVvrOXSiJM8ePpDAiJzo4CjyV/vSoN/yolJLid90Jhe5WiThNzmBeVbobtHu8Y3I2aEuTRCNk157gWY3U1OpSj5UoWYPGjPjU/ro9UZURuVFwWEXRBQ8nCZyAtV1tiOl3Joq91xIx9KKdA9oImlCPReyU8rJunJPWng09al3IJQ4gTHQXKOozYtp6pBe1La6B1lLyYxMTVU+CKfM3KM0obqSoPCgrjPW0Pr162n9munUqbScZLYv0TJxUOquhlFXZznJS/ehNalXaGkXJ5IrPOn1jdLlRC4dnViDlFCQR9cZtFqafv0amt6ptEhObOmlZSn5CYxM3YEWXzdtmoLz/WT1/fmWMc437HEvfRkrIc251fROLSdyqD6I1iaa6ys1k8DobtCJHRuMcSp9NkXQxQIth1Tn4NoieRF1ris4wkRzaDQFqP1oePg9PtE8Y8wlMLq0aFo7/2v6NeF+LGQcGEVVlB7Uf+NtunFiB23Ii6X1myiiaMyYSWBIm0g/vuJNKvvK1O71d+mdV5pR5cA6VM3Fhl4KS+O4ehZoztHqd2qJ5KU6DVp30ZicaM/QzEZKUnj1o3U38+LkLv0xpDJZVxlLhzW6R8ST8Zx7N/YHeiPQltxDf6DkIhmK4evdWPqhfwDZuofSD0ULPGElSGB0lHr4R5r50Wgau6xgl1AWnf6yAznLQVZ+I8WV4n4aUdnKkNC4VfAlX98Cn8DetNSwIdJo7avupJC70ssLZtOL9jJSVBpCuw1dmRraP9KPrCAjtVuFwtP7BlLvpRcpN+8WkltfWp/X/al5xHwvPd0NyJ5HOkrZ/wl18LKmsm2n0d5in6Y0k8BoImhKa7/7MVrlLVpn6Lkx1tneU6pzajF1ZtPeoT6krvkRHeeHLZ85ZntgzMnaTP3LqKn5nNN0cEpr8stv86rQWyKYCk1tLoGRaJJo7+IJ9M4bb9KQqd9R5IUfKdTZiwZszeIExsLpUvbTJ+09ybpsW5q657op7RB012lxBzWpOy2lgiGWubInOdkF048ZGooQ8eRfMJ7WmnvITkdXF7YjtUMo/fxAr7OR7upCamftSKHFFXhCStQDc2dVb3JTQGRuvemX/MeKdXTp2y7GBMZ/FB3MukhftlaTXO5Br280rfSdI/TzvK9pxfojdNU0WeYfg+kFKzk5uZchG5mSqo07bHgQSKrv4pdtSC2Tk8frG+mOYQPfoSM/z6OvV6ynI6KC/GdgPN6kLXkJjM44X5k03w2mG8Nm5svYk5Jx+GNq5uJA1Qb8RGcf2o1f8ltI9+v8mf4q7unz3KM0vpqaAkdHUDafZZ455hKYjB2T6aWuk2h7gWdedMlfU3tbZwr9sQTPQJlJYHRJO+mr6csposBrJxm/vkUVHF6kRXxf0rJlHKaPm7uSQ7UB9PMDjZOGDo7yI3X1CXQk/wJInHfntSFbr4H0q9m2LJO2j25INfr9SNfzQ0NHiV+0JLVLH1qbKUpsH0MNa/SjHwtcdOkSv6CW1i7URyrwFJXsFtLdXTQswPgQr7VXA+rW9w3q260hlbOWGX4LpuqYA5RFWkpYHEQuChmpvYNo1JwvaXKoH1mLhKRU8ApKylt5wxPMUl3SraCG9Ens/a53bcJiCpIexlV7U9CoOfTl5FDyE/OQlwqm5aKCggnM/QfQjPN1lpuf71PuwWLPG80xmlTXmtR+vWnGsjAKC8v7LKeVB4o+cFvCBEbUObmejaHOmQ+pU3dtEXWwcRONQpGrbPZMMJfAaM/Np7ZODlRv5DZKFG2e9tZhWhDsLdq6gbQltQRRYK4HJnUNverhQs2mHDK8eZKVsImG1XWkcn3XFjhJMcujoWOT6pKN2o96z1xWoB0Jo+UrDxhK5B6bRHVtS1Pb6fvoeq6O7sauoFcr25LPuzsMz6U+SCQr0vnVpioNWntBtGhaSjnyFYV4q6nc6xtIepFNlyidt22o6qB1dEGK0ZQj9FVwRVKXe502FHjT7Wko8TMwmadW0NtNPI3PveQ9xGvjRU3f+Y5O5vUSaS/Rpg9akqfa9IN3MiV5NB1Fm5MKZhFaipvZmNQyGdm1W0AFbu0KWrq06QNq4ak2JDjSPJQeTWnU5iQxRmrAzSUwgmG+LcjrofNl7J/TRI6lQKu8Y6DgR05lB2wzlcpTsgRGEzmGqiiLq3OrqZQoFyHmbV2TPjqeywnMM8j8LaRcOrfyHapXSklWNvZko1SQg18IzT1s/lc7HmD2FpKWLqx8i2o4W5HK3lHU6UCVu86g8FvcXlo0TSSNCbQy046Ic2bZAaSXfgiGsih2+RtUy1VJSls7UilsqGLQJ7Qv5SH7XptI64c1Jg+lFVnb2ZCVwoH8Qj6nQ/nvXWspcf0wauwuYtTajmysjDH6+cEUkf48XY/xTwlIdMhIPoP4i6nIVbrCOzAAnvYP/oa5Ni0Bp85cRa6zD6oHuJv9ifWH0qYh4dQZXM11hk/1ALiXsIJ/PF/GGPsvyr6G+JgLuG1dDlWrVYDDE/inI3S3E3Ey/gYUnn4IrOAMK9Nw9hzIvIJTpy4ht5Qfqvq4PvqfORGyr8cj5vwd2JSviqrl7R/8hxSzr4sYPY87NuVRtWp5mEkNnrjHTGAYY4wxxv59/4cciTHGGGPsyeIEhjHGGGMWhxMYxhhjjFkcTmAYY4wxZnE4gWGMMcaYxeEEhjHGGGMWhxMYxhhjjFkcTmAYY4wxZnE4gWGMMcaYxeEEhjHGGGMWhxMYxhhjjFkcTmAYY4wxZnE4gWGMMcaYxeEEhjHGGGMWhxMYxhhjjFkcTmAYY4wxZnE4gWGMMcaYxeEEhjHGGGMWhxMYxhhjjFkcTmAYY4wxZnFkJJj+Lp7uIvat2oPzGvNFFeWbo3c7X6hM3x9Jp4NOoYDC9PW/QpcYjpV7L0An1qdfW1/T0MelR1rsr1i1fh9ir2RA5uiJKs26o1fnanDhdPE5kI3kP8MRcSYFcPNHk5Z14WltGmWQgxsx4dh36hasvOqgZXN/jgtWrOzkPxEecQYpcIN/k5aoWziYRKOVgrjwvThxUwXvhm3Q2NvONIKxB2VfPordEeeQ6RiApq1rw1NtGlGUJgUXk7UoVckd/+mIkhKYR8paS6+6KqTsxexH3elbSjMVfShNEu2Z9yY1e2kWndaahv2HZK7qRY4ysT5dwkivNw18LBqK+yaYKqplhbeRTEUVghdTrMZUjD2bMqNoQTdvslG7UaUqAVTeWUk2PqG0KCbLOF53lTYPq0POSkcq5+dHZe2UVKbFFAov0cHDni+ZFLWgG71goya3SlUooLwzKW18KHRRDGWbSlDaPprSvDSpHDzJx9uV1NYVqfvXJ8kUbYwVkEunw/pQZVtrEU8B5Fvahmy8u9NXJ8xEi+4GbXnbj9Qeb9KWe3/rRPh/83gJjKIcdRg+laZNm1boM+OX6PsH1UNk7xlKlRQgZcNPKP5ZTGDuiO1UWk4yZXnqOCGMNu3YTuu+Gkh1nOQEuQsFf3ed/oOrzZ4ILSXMb0sOLq3p0z/vGIboUg7SpKZO5BS0yPBdc+gDClBXov7rkkRzIo6Hc8so2NOG6k8/RTmGEnnSKXz+SPrs1yukMw1hzxdtwnxq6+BCrT/9k+5KA3QpdHBSM3J2CqLFSVJUaOjIxJpk80JfWpUoRVMGRX3Rnso4t6cF56XvBaSH0/yRn9GvVzianle6a99RiJszNZt6WLQuQuZJmt3WmZw6f0vJhU5KWkr8vgeVUypIUVwC8x+Kp8frvJa7o9nA8fjoo48Kfcb1rgl1djQWDwpFSMjLGPnLBehEcc3JbzE4NAQhr3yEjcfXYOL07bihB3TnVmJYrzFYe1mP9O1T0FNMMyZsNT4OroPAmi9iyt67IH0KIhcPQecGgajsXwut+0zGujPZxuWAHlfWfYiXQ3pi6vr9WDokCHX9/VGz7dtY8udNnFk7HiGNAuFfrRl6TtuOJGlhDPRIiVyMIZ0bILCyP2q17oPJ604jr9aC9MlrMfZlseyvzcaBTNNApOP3aa+IdXwVn4bnD8ynu5WIS+l6yBwb4rURb6Br+xcR8t5XmDe8DWpX98a9SxegNZR8yHLoL2HNmJ4ICX0D849oDKUlmftm4bWQELwyfadYClEsJRKLh3RGg8DK8K/VGn0mr0P+5hEltk8Rdbw8BmGrP0ZwnUDUfHEK9twhs9OdzjJNxv4BHW7BA80HjsDg2g6GIXLXhujXrRpyzsQZvmsvXcRVZQ20bu8FK/Fd/UJntK9KuHRBxIXUV5dHn4mYjd9g9ZFbhuOIPX90twCP5gMxYnBt2EsD5K5o2K8rquWcQezZXBFMJ7Bx01/wf20UQipK0WSHWu+ORajTQazfkixNkU+fGYON36zGkZscTc8tbXm0fe8TTB/aAE7Sd9sAtGhQDprky7iuv9/45MQuwMDxCQh5uyWUpmFFSfG0aYmIp1viZP5vMyUyD5fXA2NVmV79ciWtXr36/mfNRopMklI4kbmtCCZPK5C8TDAtPxdFM5s5kkxmQ7U+PEBpZ+ZQW1drUshAMqUtOZdpS7NP59C1r9uTCnKyd3QguRgHZS2aFJVJ0Z+2IBe5jBTOPlSvbiVyEn8rK/Sl1YasT0vxMxqSUkzn6OxCzhXrUAM/F1G3jKxKl6Uy9mWpesNq5K6SEeSlqffKW4bV0ER/Si1c5CRTOJNPvbpUyUnqLalAfVcbr3QL9cDkRNGkWkpR1osGbM0wTE9pv1AvNwXJHDrS4mQz2acmgsZWFdNARrblG1OPIdNo0bqDdD69cL+LtBzNnYtbjlw6MbkuKWUKKv/2dsowJMBptKZPGbF+9tR+4UXSaaLp0xYuYnspyNmnHtWt5CT+VlKFvqvJsHl01+jrdiqx7vbk6CDqFrtZWWsSHc8Q0zV3NjPdKuN07MnSnqev2ournI7GHhjdpRUUUtaF6g35iSJPn6ZD371NtRzL0aurrxXumdMl04I2dlR3SkyRnhn2/NLS+a86kIuTqe258z0F27tQz1WmtkliiBtrKjfod9MAI13yAmpjV5emnOBoYlrKvH6OIldPohe9XKnZzKj7d0/uHqKJ9b2o1axourX8JbIppgdGiqe29iKeYor09P0L/vkzMDJn6pV3EOmS6JdXyokTrYJKVShPdiKhsKs/iQ5nGkc/eAtJZ0pgRFJjX4+Gr9xLu7bspwvpG6mfh0gUVHVowmGp7gyKGFdLnNSVhu723PwERiQ1dSdRlNgDumvfUJCtdKvGjbqGJYkSWbRzcEVSwIoCxxwSddyljf3cxbKpqM6Ew6JGUWvEOKolkhxl/el0SixP4VtI0jwakVqsS7m3tpG0CumrX6HSChk5dQ2ja8Wc8O8c/oJCKtsZkzHDNpKR3M6b2gz/heINtxuNyyE3LEekaTk+LLQc2vNfUCsbkbx59qeNd0QA3fqBQkTiJXcJpu9v6Ojuxn7kIZZDVWcCGTdPBI2rpRJJUH2aLlWQl8CIedvXG06rwnfRlv0XKF1M5y4SQWm6yLuiXjHdhzVFwpU3HXtydLdo9/iG5OxQlyYcMNwEELIp7tseVFFKrA3Hjpp8X/uZEgybPpfivhtGfXr3pt69g6lBWSW51epMvaTvr7xFC4/yA1TPLx3d2j2eGrk4UN0JBwxthi55PrWx8aK3fit48z6Dvu9uQy69V4twiqPvhvURsSTiJ7gBlVW6Ua3OvQzfX3lrIR0tyT1/9uzRRNL4us5kp5STdeWe9PWxNONtatFe/To4kCp0X0bnRF5yp2gCI8XT8IfE07/UPD3eLSSFJ1q9NQZjx44t8BmGboGm94/kXuj5xZd4tYIMKZcu455DE0z4djwa2BpHF08GVZOB+LBHS7Tp0gzlzx5FdKoOctdysLu0Axs27ECSrSecZVrEHjmCO1LzbyCHR72mCFSLv5zcUcZeJpaxClq08oACSnh6lhElCJosDaA9jaNRqdDJXVHO7hJ2bNiAHUm28HSWQRt7BEfumKrMp0Dl3r3RxEaPq79twr57t/HHpl1IJRe069kV7sVsOYcGI7Du1Hn8uWkxpg7phbbV3aG6l4jdX/ZH6Ef7kZlrXA69tBy2ecthV2g5FN6voF97J9C1bVi9PQ1XN67CrnQZPLr2Q/fSepw+GoVUnRyu5exwaccGbNiRBFtPZ8i0sTgiKsjfPDIVmgz4ED1atEGXZuVx9lg0UvWm6XZuNExn53V/OvaE5JzHmiHtEbzgHnqtWI8pTaWbAHpcX/s2Og2PQt2ZvyE2MQHRGyai8l4xbOQfSCcZrF28ULFiRfEpB1drQO1cNv+7my2/qvR8ysH5Ne+hXfACZPZcjnVTmhrfClFYQSGau8KkC1Lj/yGzhouXFDviU84V1lDDuWzedzfY/tdeAWX/H6qGmH4sDRmpJ/Ft0xiM6jwQPyXn4tLKIXh3VxPMX9wfPtIdyaKkePJ8SDz9W82TKZF5uLweGGVdmnbqEd1Gd8JpVDW14baFzK4eTcrrfhGK74GRk1vf9ZSX7Gn2jyA/pZhe7UYVfH3Jt8AnsPdSuqjN64Gxosoj9pEh+cvaTG96iGVUtadF16WcMq+MgioN3S1VSiMqWxmuet0qFK7TN7A3Lb2ke/AhXl0yfRPkQHJFeXp7wyrq5yEneelXaLXZt0Z0lHr4R5r50Wgau+y4cZkMsuj0ly+Ss8ikrPxG0v47j14OSdraV8ldLifXl+fT7BftSaaoREN23RNjNLR/RGVD75ParULh6X0DqffSS6TNv4XkRn3XSdNIxHQj/cjqIdMV06nEHoMuZT990sGLrMu2pWl7bxTYpjfo20525Bz8PaWYhkgxc/WbjmTv3ItWSj1iefgWEpPoUmj/Jx3I07ostZ22lwzNWp6MldTTxYl6/CL1x5joLtLcVtZU8d0/TAOM+BYSM0d3dSG1VTtQ6PcRNLmOiqxLv0D+/v6Gj5+XOO8pnam8fx0atrXwm0r/pVtITzhvuo3dkwZjQWwuHEuXgvLen/j8nU9wyPS8q0wug0y6apBuXRkH5VOq7j8yZFXBG+UUcshcumB+1FmcPXsWx3+ehiEjJ+KTkZ3FOFNB4YGLEDNDDKwqwFtMKJe5oMv8KEOdZ4//jGlDRmLiJyPR2cvMppB7ILhPOzjjCrZMnIZfbwDunXoiyNk0vhA5rBK34vNPPsfsSbOw/mreA04q2NqaeqjEystMyyGTluPL48Uuh3PQGwipIEP6jk8wa18mrAJ74bXm0m9AWKGCdzkoxLZ06TIfUdL0Z4/j52lDMHLiJxjZ2avA7+socX+ziunElbxCLIM03fG/HpyOr/H/ocwjmNmtKz690hHLwrfio5al729TvR5arQ4ylUrslTxy2NjbQaHVQPMfeB6O/Zdk4sjMbuj26RUEhe3Dto9aokzBA9S6Oqr75iAu+hRyTIOQdQIxZxTwrxZoGsCY0b3fx6Bxrdfx080CDY1Gg1xxXlCrS6Fh7yEY1KcrOnbsaPi0r+EOhbo86nVoh1qe5rpk/iNMiczD5fXAyOzIM6Am1axZ+FOn4ycUqSFK3f4u+atkJHcLokUn99HYmtYkk1lTjdH7SHqUI/foeKqmBCnc29HoL8JoV3Jufg+Mx5tb83tgSJtAiztJD+WqyTtoFM35cjKF+om65KUoeEUyafN7V6zI74EemA7me2DE94TFQeSikJHaO4hGzfmSJof6kbVMTqWCV5D0XJzZ16jT1lJfd9PzP4py9Na2Alc8Rd3dRe/7Gx/itfZqQN36vkF9uzWkctYyse1UVHXMQbGOxuVwlhe/HEa5dGRcNVJJz9KI7dBwRpzh1VuJNmExdXJRkEztTUGj5tCXk0PJT8xDXiqYVkgV5PfAeNCbW/J6YIzTBUkPDxc3HfsHNHRsUl2yVvtR7xnLKCwsLP+zfOUBMV5LZ+a2Jien+jRqa4Lhtzoyz62nIXUcyKXzEkos9AhSBkWtmkvfHyrYg8OeJ5pjk6iutZr8es+gZQViKWz5Sjpgemki7rPm5FCmLc04eItyNUn06wf1yMmtGy0zjC8gI4pWzf2eDt3gaHpe6RKlc4YtVR20jhKyRfSkHKGvgiuSqtzrtP5Wgd5fkweegSnoPxRP//whXvFRVBhMO5K30Fs+4uQtd6H2C86Iw0tshD0jKFApI5mqCg3flSZa7HAaW8POcHtJSgYG7bhHV80lMIL20ib6oKUXqU0Pw8qUHtR01GYy/ARCfnLyOAmMNOgSbfqgJXnm/dCcTEkeTUfRZmOlxfwOTAZtG1ieFNJ6vvAe/WFuhxaQeWoFDWrqJRKSvO0jEgQbL2r6znd0Ku9ummE5WpBXMcuRRxs3kxqLBENm144WJBYcp6VLmz6gFp7GW3XSPJQeTWnU5iTjCa+YBKbY6TZd5hPlP6WJpDGBVqZ9XvgjLzvAWCb3PK0a0ojKKK3I2sGeVHI1ebb4gDZdKnLCYc85DUWOCSRx3ftgPMnL0oBtpi79rBha1NOP7OVKUqutyMq5Ng1anZB/ocPYfVpKXD+cGrsrycrajmysFOTgF0KzD6WabfsfmsD8h5TsnxJ4kjTXER99ARlOlVA9wB1Ffhi7CC3SEk7hzNVcOPtUR4D7w0uXlDYtAafOXEWus08JliEDv75dDV2/TYb30D9w6suWEMnJI+iQkXwG8RdTkat0hXdgADztH7xB83jLYYY2DQmnzuBqrjN8qgegxJvn707HngjNjTM4eS4FsjL+qO7rVvJ/goOxB+iQnhCD+GtyeFWrjgoOfCOYPUS2OP/GnMcdm/KoWrU8zJyWLMr/P4GxGHcR+f1X2BkTgR8WbsU5XXWMO3gUH9dXFfeUDWOMMcb+TziBKVYmVvb0RJ81dwC1F9pM+AVrPmoOF9NYxhhjjP17OIF5CG3GNVy6fBvKsj4o7/wffhKbMcYYe85wAsMYY4wxi8NPfDHGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwOJzCMMcYYszicwDDGGGPM4nACwxhjjDGLwwkMY4wxxiwM8D+Py9YOTDQ8eQAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6OhOPkvWI9"
      },
      "source": [
        "####Scoring of the TIPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjK9WvyvdZt"
      },
      "source": [
        "The score of the person's measure of the big five personalities are determined by finding the two answers that relate to the respective personality. Add one score to the reversed score of the other. The average of these two score is the metric of that person possessing such a personality trait."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY02y715otK0"
      },
      "source": [
        "###Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6xawE-BcjD"
      },
      "source": [
        "The Goal of this project is to Predict the severity of a person's Depression, Anxiety, and Stress Target Variables using personal information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jXaRgKgokQI"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiykW2H1orUK"
      },
      "source": [
        "###[Psychology Foundation of Australia](http://www.psychologyfoundation.org.au/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ho9H7ccpTnL"
      },
      "source": [
        "[\"The Psychology Foundation of Australia is a grouping of research-oriented schools of Psychology formed to defend rigorous academic and scientific standards in the teaching, research and training in the discipline of Psychology and its professional practice within Australia.\"](http://www.psychologyfoundation.org.au/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmW2flopfSY"
      },
      "source": [
        "####[Depression Anxiety Stress Scales Responses](https://www.kaggle.com/datasets/lucasgreenwell/depression-anxiety-stress-scales-responses?select=codebook.txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq-bj39hpu67"
      },
      "source": [
        "- Collected March 7th, 2023 via direct download of csv file.\n",
        "- Data was collected with an on-line wersion of the Depression Anxiety Stress Scales (DASS).\n",
        "- The survey was open to anyone and people were motivated to take it to get personalized results. At the end of the test they also were given the option to complete a short research survey. This datatset comes from those who agreed to complete the research survey and answered yes to the question \"Have you given accurate answers and may they be used for research?\" at the end.\n",
        "- This data was collected 2017 - 2019.\n",
        "- Important Features:\n",
        "    - Q(number)A (int): The User's answer to question (number).\n",
        "    - Country (str): ISO country code of where the user connected from.\n",
        "    - TIPI(number) (int): Answer to Ten Item Personality inventory Question (number).\n",
        "    - Education (int): The highest education level the User has completed.\n",
        "    - Urban (int): The Type of area the User lived in as a child.\n",
        "    - Gender (int): The Gender of the User.\n",
        "    - Age (int): The Age of the User.\n",
        "    - Hand (int): The hand that the User writes with.\n",
        "    - Religon (int): What religon the User practices.\n",
        "    - Orientation (int): The Sexual orientation of the User.\n",
        "    - Race (int): The Race of the User.\n",
        "    - Married (int): The Marital status of the User.\n",
        "    - Family Size (int): The amount of childeren the User's mother had.\n",
        "\n",
        "- [More Information on the DASS Questionnaire](http://www2.psy.unsw.edu.au/dass/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8LjDIfXb-D6"
      },
      "source": [
        "###Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdibTo4ScApT"
      },
      "source": [
        "Here are the major data wrangling challenges and solutions.\n",
        "\n",
        "1.   The Data set has entries from many international users randomly taken across the globe.\n",
        "\n",
        "  *   The only rows grabbed from the data set stored in Google's BigQuery is the rows that have the 'US' string in the Country feature since it is easier to gain censis data to improve the model.\n",
        "  *   An International look will be contemplated during the project.\n",
        "\n",
        "2.   All of the answers from the Questionnaire (i.e. 'q1a', 'q2a', ect.) is stored alternatively to common computer science format.\n",
        "\n",
        "  *   Shifted the threshold of the stored answers to match the standard range of zero to four.\n",
        "\n",
        "3.   The Data set stores the ansers to each question in their own spearate column.\n",
        "\n",
        "  *   Added all data from columns 'Q3A', 'Q5A', 'Q10A', 'Q13A', 'Q16A', 'Q17A', 'Q21A', 'Q24A', 'Q26A', 'Q31A', 'Q34A', 'Q37A', 'Q38A', and 'Q42A'. A new Feature column was created to store this score as the severty of the user's Depression. This Severty scal is as such: 0 for Normal (Score: 0-9), 1 for mild (score: 10-13), 2 for Moderate (Score: 14-20), 3 for Severe (score: 21-27), and 4 for Extremely Severe (Score: 28+). \n",
        "\n",
        "  *   Added all data from columns 'Q2A', 'Q4A', 'Q7A', 'Q9A', 'Q15A', 'Q19A', 'Q20A', 'Q23A', 'Q25A', 'Q28A', 'Q30A', 'Q36A', 'Q40A', and 'Q41A'. A new Feature column was created to store this score as the severty of the user's Anxiety. This Severty scal is as such: 0 for Normal (Score: 0-7), 1 for mild (score: 8-9), 2 for Moderate (Score: 10-14), 3 for Severe (score: 15-19), and 4 for Extremely Severe (Score: 20+).\n",
        "\n",
        "  *   Added all data from columns 'Q1A', 'Q6A', 'Q8A', 'Q11A', 'Q12A', 'Q14A', 'Q18A', 'Q22A', 'Q27A', 'Q29A', 'Q32A', 'Q33A', 'Q35A', and 'Q39A'. A new Feature column was created to store this score as the severty of the user's Stress. This Severty scal is as such: 0 for Normal (Score: 0-14), 1 for mild (score: 15-18), 2 for Moderate (Score: 19-25), 3 for Severe (score: 26-33), and 4 for Extremely Severe (Score: 34+).\n",
        "\n",
        "4.   The Data set also consists of the results from a Ten Item Personality Test that was administered with the online questionnaire. \n",
        "\n",
        "  *   Each entry relates to one of five categories. Each category has two questions associated with it, one for and one against. The against question's score is reversed (i.e. if the input is 2 then swap it to 6) and take the average of these two answers to get the category score.\n",
        "\n",
        "5.   The dataset consists of alot of data with general categorical features.\n",
        "\n",
        "  *   Took the Data stored in these catigorical features and created a new column for each catigory to make it binary.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WED6CShzWx_E"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TqBYxIw5EH4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be9536a-bab5-437b-dffb-677db4b33b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for helpers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -q 'git+https://github.com/drscook/helpers'\n",
        "! pip install -q --upgrade pandas scikit-learn # pandas 2.0 just released on April 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IzzDlsw5ETqA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91711732-a7cc-47e3-db36-78fe427a5467"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2.52 ms (2023-04-23T18:55:43/2023-04-23T18:55:43)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%reload_ext autotime\n",
        "from helpers.common_imports import *\n",
        "from helpers import utilities as ut\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pathlib, joblib, google.colab\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, BaggingRegressor, HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay, confusion_matrix, RocCurveDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rEAu9GuAEhGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28def7c9-f346-441b-a5fc-dcca683f1993"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2 min 4 s (2023-04-23T18:42:37/2023-04-23T18:44:41)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Pull from Big Query.\n",
        "bq = ut.BigQuery()\n",
        "tbl = f\"tarletondatascience2022.wallinger.Depression_Anxiety_Stress_Scales\"\n",
        "qry = f\"\"\"\n",
        "select\n",
        "    *\n",
        "from\n",
        "    {tbl}\n",
        "where\n",
        "    country = 'US'\n",
        "\"\"\"\n",
        "\n",
        "df = bq.qry_to_df(qry)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Global Variables.\n",
        "randnum = 500 #Setting a random number to get similar results.\n",
        "Holdout_size = 0.3 #Sets the Holdout portion.\n",
        "C = np.linspace(0.1,1,num=10) #Creates a Cost Varaible for the SVC Models.\n",
        "\n",
        "#List Containing Class names for the verification of the Classification Models.\n",
        "DepClassNames =  ['No Depression', 'Mild Depression', 'Moderate Depression', 'Severe Depression', 'Extremely Severe Depression']\n",
        "AnxClassNames =  ['No Anxiety', 'Mild Anxiety', 'Moderate Anxiety', 'Severe Anxiety', 'Extremely Severe Anxiety']\n",
        "StsClassNames =  ['No Stress', 'Mild Stress', 'Moderate Stress', 'Severe Stress', 'Extremely Severe Stress']\n",
        "\n",
        "#Creates a both to save the models in.\n",
        "root = pathlib.Path(\"/content/drive\")\n",
        "google.colab.drive.mount(str(root))\n",
        "\n",
        "path = root / \"MyDrive/DataScience_Models\"\n",
        "path.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "Oruq0u2zufPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a4ab1778-8be5-4248-d403-ea6738a3933c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 998 ms (2023-04-23T18:58:44/2023-04-23T18:58:45)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checking the Data"
      ],
      "metadata": {
        "id": "gqgMOLDpKHFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is fairly good right away. No missing values in all features and targets, Large row count, and besides some minor formatting issues. Only one typo was found but since this may effect the normalization range, I chaged to more likely input. Also found some outliers in the faimlysize but unsure if I would be removing too much variance."
      ],
      "metadata": {
        "id": "FMIUfamV1YL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixes the age error.\n",
        "WorngAge = 117\n",
        "CorrectAge = 17\n",
        "df.at[4741, 'age'] = CorrectAge\n",
        "\n",
        "#Verifies that the fix is indeed in place.\n",
        "print(df.loc[df['age'] == WorngAge, 'age'])\n",
        "\n",
        "print(df.loc[df['familysize'] > 20, 'familysize'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "oPY6K9HC2uG0",
        "outputId": "0d85fedf-1c54-457a-aed9-d726f3754d67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 11 ms (2023-04-23T18:44:42/2023-04-23T18:44:42)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], Name: age, dtype: Int64)\n",
            "index\n",
            "6765     62\n",
            "8052    133\n",
            "Name: familysize, dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ut.pprint(df)"
      ],
      "metadata": {
        "id": "u8_HbjgwKMCc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "d5349901-53b2-4819-9c4d-dfc13319b506"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 229 ms (2023-04-23T18:44:42/2023-04-23T18:44:42)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       q1a  q1i    q1e  q2a  q2i    q2e  q3a  q3i    q3e  q4a  q4i   q4e  q5a   \n",
              "index                                                                           \n",
              "0        1   18   6116    1   28   3193    2    2  12542    1    8  6150    3  \\\n",
              "1        1   10   2901    1    1   9036    1   22   3775    1   34  2027    1   \n",
              "2        3    5   6729    2    4   4793    2   20   3328    1   16  2199    3   \n",
              "3        1    9   2236    1   10   1437    2    7   2570    1   17  2188    1   \n",
              "4        1   35   3404    2   28  10935    1    5   3593    1   13  5086    1   \n",
              "...    ...  ...    ...  ...  ...    ...  ...  ...    ...  ...  ...   ...  ...   \n",
              "8202     4   17   3069    4    6   3561    4   25   2200    3   21  3045    4   \n",
              "8203     3   30   3626    1   38   4161    2   27   2640    3    2  7369    4   \n",
              "8204     4   16   2000    2   10   2055    4   22   2229    3    9  3527    4   \n",
              "8205     4   38   2350    2   29   2316    4   20   2087    4   30  2614    4   \n",
              "8206     3   32  28897    4    4   3025    2   30   3248    2   16  8048    2   \n",
              "\n",
              "       q5i   q5e  q6a  q6i    q6e  q7a  q7i   q7e  q8a  q8i   q8e  q9a  q9i   \n",
              "index                                                                         \n",
              "0       40  6428    1    4  17001    1   33  2944    3    7  8626    3   14  \\\n",
              "1       38  2714    1    3   3626    1   11  4496    1   27  3453    1   41   \n",
              "2       22  2513    1   21   2220    1   13  2489    2   32  2570    1    8   \n",
              "3       27  3372    1    2   1975    1    6  1579    1   37  1736    1   26   \n",
              "4       17  2810    1   10   3677    1   29  4919    1    6  3288    1   38   \n",
              "...    ...   ...  ...  ...    ...  ...  ...   ...  ...  ...   ...  ...  ...   \n",
              "8202     1  8671    4   30   2186    4   32  1527    4    4  2236    4   26   \n",
              "8203    12  2607    4   15   2122    4   19  2473    3   32  1538    4   37   \n",
              "8204     6  1633    4   24   1032    2   21  3567    4    4  2608    4   31   \n",
              "8205    19  1320    4   35   8479    4   36  1443    4   21  1320    4    8   \n",
              "8206     8  3559    4   34   2219    4    5  4553    2   21  1890    1   36   \n",
              "\n",
              "         q9e  q10a  q10i  q10e  q11a  q11i   q11e  q12a  q12i  q12e  q13a   \n",
              "index                                                                       \n",
              "0       9639     2    20  6175     1    34   6008     2    21  9267     1  \\\n",
              "1       4619     1    21  2540     1    20   2946     1     6  3508     1   \n",
              "2       5790     2    18  4348     1     1   5951     1    26  1973     1   \n",
              "3      11058     2     8  3389     1    38   2105     1    23  2218     1   \n",
              "4       8824     1    31  5750     1    21   2475     1    37  2422     1   \n",
              "...      ...   ...   ...   ...   ...   ...    ...   ...   ...   ...   ...   \n",
              "8202    1265     4    20  2232     4    34   1676     4    40  1716     4   \n",
              "8203    4779     2    28  2741     4    13   2389     3    34  2690     4   \n",
              "8204    2936     4    19  1848     4    12   2448     2     8  4936     4   \n",
              "8205    2862     4    28  1488     4     1  12252     4    37  1481     4   \n",
              "8206    4561     3    11  4071     3    23   2909     3    17  5179     4   \n",
              "\n",
              "       q13i  q13e  q14a  q14i   q14e  q15a  q15i  q15e  q16a  q16i  q16e   \n",
              "index                                                                      \n",
              "0        41  5290     3     1  25694     2     9  7634     4    37  8513  \\\n",
              "1        14  1727     1    31   4508     1     4  2490     1     5  3725   \n",
              "2        11  2679     4    33   3425     1    23  1931     3    28  4472   \n",
              "3        19  1803     1    31   3058     2    29  4186     3     5  2184   \n",
              "4        36  1638     1    24  15928     1    12  2770     1    26  4719   \n",
              "...     ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   \n",
              "8202     16  2553     4    39   1797     4    33  1585     4    23  3475   \n",
              "8203     18  1320     4    14   3376     3     5  1621     2    29  2038   \n",
              "8204      3  1423     4    40   3063     4    33  1103     4    11  2240   \n",
              "8205     10  1291     4    12   2200     4    33  3329     4    23  1575   \n",
              "8206     14  2190     2    28  52923     1    41  2126     2    18  3341   \n",
              "\n",
              "       q17a  q17i  q17e  q18a  q18i  q18e  q19a  q19i   q19e  q20a  q20i   \n",
              "index                                                                      \n",
              "0         2    25  9078     1    15  4381     1    23   6647     2    36  \\\n",
              "1         1    28  4826     1    17  2425     1    13   3002     1    23   \n",
              "2         1    24  1450     2    40  2006     1    25   3613     1    19   \n",
              "3         1    11  3906     1    14  2010     1    35   3128     1    30   \n",
              "4         1    39  3639     1    32  1745     1    11   8400     1    23   \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...    ...   ...   ...   \n",
              "8202      4    14  2722     4    18  1785     3    29   4253     4    11   \n",
              "8203      4    40  2406     1    26  3810     3     9   3910     3    17   \n",
              "8204      4    27  1832     4    23  1383     2    37   3498     4     7   \n",
              "8205      4    18  1286     3     6  2491     2    34   3308     4    16   \n",
              "8206      2    24  3949     4    29  4455     3     2  19120     3     7   \n",
              "\n",
              "       q20e  q21a  q21i  q21e  q22a  q22i  q22e  q23a  q23i  q23e  q24a  q24i   \n",
              "index                                                                           \n",
              "0      6250     1    39  3842     1    16  7876     1    27  3124     2    12  \\\n",
              "1      2371     1    37  4058     1    25  2721     1     7  2511     1    33   \n",
              "2      1166     1    15  3214     1    10  4843     1    27  1216     3    41   \n",
              "3      2375     2    42  1957     1    13  1567     1    24  1334     3     4   \n",
              "4      2408     1    30  3548     1    42  4149     1    20  3043     1    41   \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202   2077     4     8  1918     4    15  1533     3    22  2939     4    37   \n",
              "8203   2106     3    41  3241     3    21  2975     2    25  1722     2     3   \n",
              "8204   1760     4    34  1438     4    38  1422     2    15  2296     4    29   \n",
              "8205   1336     4    13  1555     4    15  1270     1     2  3946     4    26   \n",
              "8206   4429     1    39  3372     3    31  3409     1     9  2538     1    10   \n",
              "\n",
              "       q24e  q25a  q25i   q25e  q26a  q26i  q26e  q27a  q27i  q27e  q28a   \n",
              "index                                                                      \n",
              "0      6836     1    31  12063     1     3  9264     1    35  3957     1  \\\n",
              "1      2981     1     9   4554     1    18  1894     1    40  1541     1   \n",
              "2      4086     2    37   5170     1    35  1703     2     9  2482     1   \n",
              "3      2903     1    25   2473     1    41  1397     1    20  2119     1   \n",
              "4      3698     1    27  27429     1    15  2660     1    19  2621     1   \n",
              "...     ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202   2699     4     2   8376     4    10  1452     4    35  1325     4   \n",
              "8203   2456     2     6   3008     4    35  2506     2    22  2991     3   \n",
              "8204   1289     4    32   2384     4     5  2207     4    14  1632     4   \n",
              "8205   1604     4    27   6550     4    41  1244     4    39  1256     4   \n",
              "8206   2541     4    35   8110     2    25  2701     4     6  2408     3   \n",
              "\n",
              "       q28i  q28e  q29a  q29i   q29e  q30a  q30i   q30e  q31a  q31i   q31e   \n",
              "index                                                                        \n",
              "0        42  2537     3    17  10880     2     5   8462     2    32   5615  \\\n",
              "1        12  3354     1    29   3129     1    24   5256     1     2   5345   \n",
              "2        29  1741     1    14   2681     1    12   2779     3    38   2302   \n",
              "3         1  4983     1    15   3193     1    12   2454     1    18   6983   \n",
              "4        22  2219     1     7   2827     1     4   4980     1    25   2623   \n",
              "...     ...   ...   ...   ...    ...   ...   ...    ...   ...   ...    ...   \n",
              "8202      9  3080     4    13   2771     4    31   2336     4    41   2938   \n",
              "8203      4  2173     4     8   2574     4    31   6601     2    23   2957   \n",
              "8204     36  1544     4    20   2351     4    35   1824     4     1   5042   \n",
              "8205     24  1223     4    40   1841     3     7   3922     4    31   2839   \n",
              "8206     37  2909     4    42   4100     2    33  13153     2    13  19343   \n",
              "\n",
              "       q32a  q32i   q32e  q33a  q33i  q33e  q34a  q34i  q34e  q35a  q35i   \n",
              "index                                                                      \n",
              "0         1    30  11412     4     6  5112     1    29  3070     3    10  \\\n",
              "1         1    36   3058     1    39  1690     1    30  1686     1    26   \n",
              "2         2    30   3895     1    36  1313     1    34  1426     4    31   \n",
              "3         1     3   3281     1    40  2984     1    34  1676     1    22   \n",
              "4         1     2  11105     1    33  2245     1    18  5499     1    16   \n",
              "...     ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202      4    19   3676     4    42  1717     4    24  1301     4    28   \n",
              "8203      4    16   4044     4     7  2222     3     1  4782     3    11   \n",
              "8204      4    39   1528     4    26  1368     4    17  1344     4    41   \n",
              "8205      4    25   2615     4    42  1342     4    11  1194     2     9   \n",
              "8206      3    22   5189     3    27  3088     4    15  2900     2    12   \n",
              "\n",
              "        q35e  q36a  q36i  q36e  q37a  q37i   q37e  q38a  q38i  q38e  q39a   \n",
              "index                                                                       \n",
              "0      13377     2    38  4506     2    24  17227     2    13  7844     1  \\\n",
              "1       4135     1    42  1852     1    19   4181     1    15  1898     1   \n",
              "2       3803     1     3  2430     1    39   2035     2     7  2754     3   \n",
              "3       5516     1    36  1483     2    33   3556     2    16  2299     2   \n",
              "4       6972     1     9  1761     1     8   8120     1     1  6261     1   \n",
              "...      ...   ...   ...   ...   ...   ...    ...   ...   ...   ...   ...   \n",
              "8202    4973     4    12  1492     4    27   2865     4     7  1791     4   \n",
              "8203    6651     2    39  1604     3    33   3008     4    20  2206     3   \n",
              "8204    1889     4     2  4773     4    30   2886     4    28  1592     4   \n",
              "8205    5597     4    22  1214     4     4   1743     4    32  1833     4   \n",
              "8206   23056     1    19  1677     2     1  15366     1    26  2718     4   \n",
              "\n",
              "       q39i   q39e  q40a  q40i  q40e  q41a  q41i  q41e  q42a  q42i   q42e   \n",
              "index                                                                       \n",
              "0        26  20253     1    22  8528     1    11  4370     2    19  10310  \\\n",
              "1        32   1389     1    35  4266     1    16  1534     1     8   3449   \n",
              "2         6   2830     1    42  2357     1    17  3662     4     2   3372   \n",
              "3        32   2794     1    39  2537     1    28  1524     1    21   2944   \n",
              "4        34   2259     1     3  3951     1    14  2576     1    40  11750   \n",
              "...     ...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...   \n",
              "8202      5   1858     4     3  6011     4    38  1735     4    36   2849   \n",
              "8203     24   2240     4    42  2424     4    36  2106     3    10   4078   \n",
              "8204     42   1208     4    13  2679     4    25  1320     4    18   1920   \n",
              "8205     14   1371     4     3  3110     4     5  3029     4    17   4222   \n",
              "8206     40   4331     4     3  3769     2    20  4261     1    38   4709   \n",
              "\n",
              "      country  source  introelapse  testelapse  surveyelapse  tipi1  tipi2   \n",
              "index                                                                        \n",
              "0          US       2            4         349           213      2      1  \\\n",
              "1          US       2            2         143           228      3      5   \n",
              "2          US       2           73         128            89      2      7   \n",
              "3          US       0           42         121           171      3      3   \n",
              "4          US       2            5         221           183      7      4   \n",
              "...       ...     ...          ...         ...           ...    ...    ...   \n",
              "8202       US       2            2         115            78      1      5   \n",
              "8203       US       0           50         128           144      2      5   \n",
              "8204       US       1            4         120           221      3      6   \n",
              "8205       US       2           22         116            86      2      4   \n",
              "8206       US       2            2         300           234      6      5   \n",
              "\n",
              "       tipi3  tipi4  tipi5  tipi6  tipi7  tipi8  tipi9  tipi10  vcl1  vcl2   \n",
              "index                                                                        \n",
              "0          6      1      7      7      7      2      6       7     1     1  \\\n",
              "1          6      1      6      5      3      2      7       2     1     1   \n",
              "2          7      1      6      5      4      7      6       2     1     1   \n",
              "3          5      1      5      5      5      5      6       5     1     1   \n",
              "4          6      1      6      3      4      2      6       1     1     0   \n",
              "...      ...    ...    ...    ...    ...    ...    ...     ...   ...   ...   \n",
              "8202       1      7      4      7      6      7      4       1     1     1   \n",
              "8203       6      7      5      5      5      4      1       4     1     1   \n",
              "8204       6      7      2      6      4      2      2       1     1     1   \n",
              "8205       1      7      2      4      2      6      1       5     1     1   \n",
              "8206       4      5      6      3      5      2      5       7     1     1   \n",
              "\n",
              "       vcl3  vcl4  vcl5  vcl6  vcl7  vcl8  vcl9  vcl10  vcl11  vcl12  vcl13   \n",
              "index                                                                         \n",
              "0         0     1     1     0     0     0     0      1      0      0      0  \\\n",
              "1         1     1     1     0     1     1     0      1      1      1      1   \n",
              "2         1     1     1     0     0     0     0      1      0      0      1   \n",
              "3         0     1     1     0     0     0     0      1      0      0      0   \n",
              "4         0     1     1     0     0     0     0      1      0      0      1   \n",
              "...     ...   ...   ...   ...   ...   ...   ...    ...    ...    ...    ...   \n",
              "8202      0     1     1     0     0     0     0      1      0      0      1   \n",
              "8203      1     1     1     0     0     0     0      1      0      0      1   \n",
              "8204      1     1     1     0     0     0     0      1      1      0      1   \n",
              "8205      0     1     1     0     0     0     0      1      0      0      0   \n",
              "8206      0     1     1     0     0     0     0      1      0      0      1   \n",
              "\n",
              "       vcl14  vcl15  vcl16  education  urban  gender  engnat  age  screensize   \n",
              "index                                                                           \n",
              "0          0      1      1          2      3       2       2   20           2  \\\n",
              "1          1      1      1          3      2       1       1   34           2   \n",
              "2          1      1      1          2      1       1       1   19           2   \n",
              "3          1      1      1          2      3       2       1   16           1   \n",
              "4          1      1      1          2      2       1       1   18           2   \n",
              "...      ...    ...    ...        ...    ...     ...     ...  ...         ...   \n",
              "8202       1      1      1          3      3       2       1   22           2   \n",
              "8203       1      1      1          1      3       3       1   18           2   \n",
              "8204       1      1      1          4      2       2       1   46           2   \n",
              "8205       0      1      1          1      3       2       1   14           2   \n",
              "8206       1      1      1          1      2       2       1   16           2   \n",
              "\n",
              "       uniquenetworklocation  hand  religion  orientation  race  voted   \n",
              "index                                                                    \n",
              "0                          1     1         4            1    70      2  \\\n",
              "1                          1     1         1            1    60      1   \n",
              "2                          1     3         2            1    60      2   \n",
              "3                          1     1         1            5    70      2   \n",
              "4                          2     1         7            1    30      2   \n",
              "...                      ...   ...       ...          ...   ...    ...   \n",
              "8202                       1     1         7            1    10      1   \n",
              "8203                       1     1         2            4    70      2   \n",
              "8204                       1     1         4            1    60      1   \n",
              "8205                       1     1         1            5    60      2   \n",
              "8206                       1     1         1            1    70      2   \n",
              "\n",
              "       married  familysize                              major  \n",
              "index                                                          \n",
              "0            1           4                               <NA>  \n",
              "1            3           2  Biology and Philosophy dual major  \n",
              "2            1           1                   Computer Science  \n",
              "3            1           3                               <NA>  \n",
              "4            1           2                               <NA>  \n",
              "...        ...         ...                                ...  \n",
              "8202         1           3                            English  \n",
              "8203         1           2                               <NA>  \n",
              "8204         2           6                          Marketing  \n",
              "8205         1           3                               <NA>  \n",
              "8206         1           3                               <NA>  \n",
              "\n",
              "[8207 rows x 172 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-540eb61e-00e4-41b8-9784-5f465ed0a316\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q1a</th>\n",
              "      <th>q1i</th>\n",
              "      <th>q1e</th>\n",
              "      <th>q2a</th>\n",
              "      <th>q2i</th>\n",
              "      <th>q2e</th>\n",
              "      <th>q3a</th>\n",
              "      <th>q3i</th>\n",
              "      <th>q3e</th>\n",
              "      <th>q4a</th>\n",
              "      <th>q4i</th>\n",
              "      <th>q4e</th>\n",
              "      <th>q5a</th>\n",
              "      <th>q5i</th>\n",
              "      <th>q5e</th>\n",
              "      <th>q6a</th>\n",
              "      <th>q6i</th>\n",
              "      <th>q6e</th>\n",
              "      <th>q7a</th>\n",
              "      <th>q7i</th>\n",
              "      <th>q7e</th>\n",
              "      <th>q8a</th>\n",
              "      <th>q8i</th>\n",
              "      <th>q8e</th>\n",
              "      <th>q9a</th>\n",
              "      <th>q9i</th>\n",
              "      <th>q9e</th>\n",
              "      <th>q10a</th>\n",
              "      <th>q10i</th>\n",
              "      <th>q10e</th>\n",
              "      <th>q11a</th>\n",
              "      <th>q11i</th>\n",
              "      <th>q11e</th>\n",
              "      <th>q12a</th>\n",
              "      <th>q12i</th>\n",
              "      <th>q12e</th>\n",
              "      <th>q13a</th>\n",
              "      <th>q13i</th>\n",
              "      <th>q13e</th>\n",
              "      <th>q14a</th>\n",
              "      <th>q14i</th>\n",
              "      <th>q14e</th>\n",
              "      <th>q15a</th>\n",
              "      <th>q15i</th>\n",
              "      <th>q15e</th>\n",
              "      <th>q16a</th>\n",
              "      <th>q16i</th>\n",
              "      <th>q16e</th>\n",
              "      <th>q17a</th>\n",
              "      <th>q17i</th>\n",
              "      <th>q17e</th>\n",
              "      <th>q18a</th>\n",
              "      <th>q18i</th>\n",
              "      <th>q18e</th>\n",
              "      <th>q19a</th>\n",
              "      <th>q19i</th>\n",
              "      <th>q19e</th>\n",
              "      <th>q20a</th>\n",
              "      <th>q20i</th>\n",
              "      <th>q20e</th>\n",
              "      <th>q21a</th>\n",
              "      <th>q21i</th>\n",
              "      <th>q21e</th>\n",
              "      <th>q22a</th>\n",
              "      <th>q22i</th>\n",
              "      <th>q22e</th>\n",
              "      <th>q23a</th>\n",
              "      <th>q23i</th>\n",
              "      <th>q23e</th>\n",
              "      <th>q24a</th>\n",
              "      <th>q24i</th>\n",
              "      <th>q24e</th>\n",
              "      <th>q25a</th>\n",
              "      <th>q25i</th>\n",
              "      <th>q25e</th>\n",
              "      <th>q26a</th>\n",
              "      <th>q26i</th>\n",
              "      <th>q26e</th>\n",
              "      <th>q27a</th>\n",
              "      <th>q27i</th>\n",
              "      <th>q27e</th>\n",
              "      <th>q28a</th>\n",
              "      <th>q28i</th>\n",
              "      <th>q28e</th>\n",
              "      <th>q29a</th>\n",
              "      <th>q29i</th>\n",
              "      <th>q29e</th>\n",
              "      <th>q30a</th>\n",
              "      <th>q30i</th>\n",
              "      <th>q30e</th>\n",
              "      <th>q31a</th>\n",
              "      <th>q31i</th>\n",
              "      <th>q31e</th>\n",
              "      <th>q32a</th>\n",
              "      <th>q32i</th>\n",
              "      <th>q32e</th>\n",
              "      <th>q33a</th>\n",
              "      <th>q33i</th>\n",
              "      <th>q33e</th>\n",
              "      <th>q34a</th>\n",
              "      <th>q34i</th>\n",
              "      <th>q34e</th>\n",
              "      <th>q35a</th>\n",
              "      <th>q35i</th>\n",
              "      <th>q35e</th>\n",
              "      <th>q36a</th>\n",
              "      <th>q36i</th>\n",
              "      <th>q36e</th>\n",
              "      <th>q37a</th>\n",
              "      <th>q37i</th>\n",
              "      <th>q37e</th>\n",
              "      <th>q38a</th>\n",
              "      <th>q38i</th>\n",
              "      <th>q38e</th>\n",
              "      <th>q39a</th>\n",
              "      <th>q39i</th>\n",
              "      <th>q39e</th>\n",
              "      <th>q40a</th>\n",
              "      <th>q40i</th>\n",
              "      <th>q40e</th>\n",
              "      <th>q41a</th>\n",
              "      <th>q41i</th>\n",
              "      <th>q41e</th>\n",
              "      <th>q42a</th>\n",
              "      <th>q42i</th>\n",
              "      <th>q42e</th>\n",
              "      <th>country</th>\n",
              "      <th>source</th>\n",
              "      <th>introelapse</th>\n",
              "      <th>testelapse</th>\n",
              "      <th>surveyelapse</th>\n",
              "      <th>tipi1</th>\n",
              "      <th>tipi2</th>\n",
              "      <th>tipi3</th>\n",
              "      <th>tipi4</th>\n",
              "      <th>tipi5</th>\n",
              "      <th>tipi6</th>\n",
              "      <th>tipi7</th>\n",
              "      <th>tipi8</th>\n",
              "      <th>tipi9</th>\n",
              "      <th>tipi10</th>\n",
              "      <th>vcl1</th>\n",
              "      <th>vcl2</th>\n",
              "      <th>vcl3</th>\n",
              "      <th>vcl4</th>\n",
              "      <th>vcl5</th>\n",
              "      <th>vcl6</th>\n",
              "      <th>vcl7</th>\n",
              "      <th>vcl8</th>\n",
              "      <th>vcl9</th>\n",
              "      <th>vcl10</th>\n",
              "      <th>vcl11</th>\n",
              "      <th>vcl12</th>\n",
              "      <th>vcl13</th>\n",
              "      <th>vcl14</th>\n",
              "      <th>vcl15</th>\n",
              "      <th>vcl16</th>\n",
              "      <th>education</th>\n",
              "      <th>urban</th>\n",
              "      <th>gender</th>\n",
              "      <th>engnat</th>\n",
              "      <th>age</th>\n",
              "      <th>screensize</th>\n",
              "      <th>uniquenetworklocation</th>\n",
              "      <th>hand</th>\n",
              "      <th>religion</th>\n",
              "      <th>orientation</th>\n",
              "      <th>race</th>\n",
              "      <th>voted</th>\n",
              "      <th>married</th>\n",
              "      <th>familysize</th>\n",
              "      <th>major</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>6116</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>3193</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12542</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>6150</td>\n",
              "      <td>3</td>\n",
              "      <td>40</td>\n",
              "      <td>6428</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>17001</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2944</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8626</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>9639</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>6175</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>6008</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>9267</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>5290</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25694</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>7634</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>8513</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>9078</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>4381</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>6647</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "      <td>6250</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3842</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>7876</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3124</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>6836</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>12063</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>9264</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3957</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2537</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>10880</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8462</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>5615</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>11412</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5112</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3070</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13377</td>\n",
              "      <td>2</td>\n",
              "      <td>38</td>\n",
              "      <td>4506</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>17227</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>7844</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>20253</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>8528</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4370</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>10310</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>349</td>\n",
              "      <td>213</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2901</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9036</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>3775</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2027</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2714</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3626</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4496</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3453</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>4619</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2540</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>2946</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3508</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1727</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>4508</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2490</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3725</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>4826</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2425</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>3002</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2371</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>4058</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2721</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2511</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2981</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>4554</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>1894</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1541</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3354</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3129</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>5256</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5345</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>3058</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>1690</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1686</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>4135</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>1852</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>4181</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1898</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1389</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>4266</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1534</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3449</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>143</td>\n",
              "      <td>228</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>Biology and Philosophy dual major</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6729</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4793</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>3328</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>2199</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>2513</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2220</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>2489</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>2570</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5790</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>4348</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5951</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>1973</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>2679</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>3425</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1931</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>4472</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1450</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>2006</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>3613</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1166</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>3214</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>4843</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>1216</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>4086</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>5170</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>1703</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>2482</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>1741</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2681</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2779</td>\n",
              "      <td>3</td>\n",
              "      <td>38</td>\n",
              "      <td>2302</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>3895</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1313</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1426</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>3803</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2430</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>2035</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2754</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2830</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2357</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>3662</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3372</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>73</td>\n",
              "      <td>128</td>\n",
              "      <td>89</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2236</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1437</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2570</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2188</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3372</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1975</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1579</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>1736</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>11058</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3389</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2105</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2218</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1803</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>3058</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>4186</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2184</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3906</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2010</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3128</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>2375</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>1957</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1567</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1334</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2903</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2473</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1397</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>2119</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4983</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>3193</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2454</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>6983</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3281</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>2984</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1676</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>5516</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1483</td>\n",
              "      <td>2</td>\n",
              "      <td>33</td>\n",
              "      <td>3556</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>2299</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>2794</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>2537</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1524</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2944</td>\n",
              "      <td>US</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>121</td>\n",
              "      <td>171</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3404</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>10935</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3593</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5086</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2810</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3677</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>4919</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3288</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>8824</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>5750</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2475</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>2422</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1638</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>15928</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2770</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>4719</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3639</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1745</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>8400</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2408</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>3548</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4149</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>3043</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>3698</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>27429</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>2660</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>2621</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>2219</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2827</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4980</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2623</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>11105</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2245</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>5499</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>6972</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1761</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8120</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6261</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3951</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2576</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>11750</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>221</td>\n",
              "      <td>183</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>3069</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3561</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>2200</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>3045</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>8671</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2186</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>1527</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2236</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1265</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2232</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>1676</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>1716</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>2553</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1797</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>1585</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>3475</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>2722</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1785</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>4253</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>2077</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1918</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>1533</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>2939</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>2699</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8376</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1452</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>1325</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>3080</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2771</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2336</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>2938</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>3676</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1717</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1301</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>4973</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1492</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>2865</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1791</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1858</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6011</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>1735</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>2849</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>115</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8203</th>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "      <td>3626</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>4161</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "      <td>2640</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7369</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2607</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>2122</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>2473</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>1538</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>4779</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>2741</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2389</td>\n",
              "      <td>3</td>\n",
              "      <td>34</td>\n",
              "      <td>2690</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>3376</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1621</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>2038</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>2406</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>3810</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3910</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>2106</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>3241</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>2975</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>1722</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2456</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3008</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>2506</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>2991</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2173</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2574</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>6601</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>2957</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4044</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2222</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4782</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>6651</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>1604</td>\n",
              "      <td>3</td>\n",
              "      <td>33</td>\n",
              "      <td>3008</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2206</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>2240</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>2424</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>2106</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>4078</td>\n",
              "      <td>US</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8204</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>2000</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>2055</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>2229</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3527</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1633</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1032</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>3567</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2608</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2936</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1848</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2448</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4936</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1423</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>3063</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>1103</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>2240</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>1832</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>1383</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>3498</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1760</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>1438</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>1422</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>2296</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>1289</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>2384</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2207</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>1632</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>1544</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2351</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>1824</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5042</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1528</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1368</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>1344</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>1889</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4773</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2886</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>1592</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1208</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2679</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1920</td>\n",
              "      <td>US</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>120</td>\n",
              "      <td>221</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>Marketing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8205</th>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>2350</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>2316</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2087</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2614</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>8479</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>1443</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2862</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>1488</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>12252</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>1481</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1291</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2200</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>3329</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>1575</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1286</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2491</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>3308</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>1336</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>1555</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>1270</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3946</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1604</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>6550</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>1244</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1256</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1223</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>1841</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3922</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2839</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>2615</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1342</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>1194</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5597</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>1214</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1743</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>1833</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>1371</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3110</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3029</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>4222</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>116</td>\n",
              "      <td>86</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8206</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>28897</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3025</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>3248</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>8048</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3559</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>2219</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4553</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>1890</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>4561</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>4071</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>2909</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>5179</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>2190</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>52923</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>2126</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>3341</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>3949</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>4455</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19120</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4429</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3372</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>3409</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2538</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2541</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>8110</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>2701</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2408</td>\n",
              "      <td>3</td>\n",
              "      <td>37</td>\n",
              "      <td>2909</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>4100</td>\n",
              "      <td>2</td>\n",
              "      <td>33</td>\n",
              "      <td>13153</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>19343</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>5189</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>3088</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>2900</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>23056</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1677</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>15366</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>2718</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>4331</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3769</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>4261</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>4709</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>300</td>\n",
              "      <td>234</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8207 rows  172 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-540eb61e-00e4-41b8-9784-5f465ed0a316')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-540eb61e-00e4-41b8-9784-5f465ed0a316 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-540eb61e-00e4-41b8-9784-5f465ed0a316');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOplAYr41FQ3"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms the Target data into a usable form."
      ],
      "metadata": {
        "id": "P_fLFjz3U-YA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Liw04BKW-o9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "902214f2-106f-444d-8a58-b1a8f0514a7c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 50.7 ms (2023-04-23T18:44:42/2023-04-23T18:44:42)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Finds the Depression, Anxiety, and Stress score on each row and assigns a score.\n",
        "Modeldf = pd.DataFrame()\n",
        "\n",
        "DepressionTarg = ['q3a', 'q5a', 'q10a', 'q13a', 'q16a', 'q17a', 'q21a', 'q24a', 'q26a', 'q31a', 'q34a', 'q37a', 'q38a', 'q42a']\n",
        "AnxietyTarg = ['q2a', 'q4a', 'q7a', 'q9a', 'q15a', 'q19a', 'q20a', 'q23a', 'q25a', 'q28a', 'q30a', 'q36a', 'q40a', 'q41a']\n",
        "StressTarg = ['q1a', 'q6a', 'q8a', 'q11a', 'q12a', 'q14a', 'q18a', 'q22a', 'q27a', 'q29a', 'q32a', 'q33a', 'q35a', 'q39a']\n",
        "\n",
        "features = []\n",
        "targets = []\n",
        "\n",
        "Depression = 0\n",
        "Anxiety = 0\n",
        "Stress = 0\n",
        "\n",
        "#Decrements the saved answers by one due to incorrrect storing in the Dataframe.\n",
        "for i in DepressionTarg:\n",
        "  Depression = sum([Depression, (df[i]-1)])\n",
        "\n",
        "for i in AnxietyTarg:\n",
        "  Anxiety = sum([Anxiety, (df[i]-1)])\n",
        "\n",
        "for i in StressTarg:\n",
        "  Stress = sum([Stress, (df[i]-1)])\n",
        "\n",
        "Modeldf['Depression_Score'] = Depression\n",
        "Modeldf['Anxiety_Score'] = Anxiety\n",
        "Modeldf['Stress_Score'] = Stress\n",
        "\n",
        "targets = targets + ['Depression_Score', 'Anxiety_Score', 'Stress_Score']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the catigorical features and breaks it up into multiple binary features."
      ],
      "metadata": {
        "id": "2HBzsKutVEZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eoT9U6O7ah9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4946f571-a36c-489c-e488-ffbd1cabb37a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 594 ms (2023-04-23T18:44:42/2023-04-23T18:44:43)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the eduction column into more binary columns.\n",
        "Modeldf['No_Degree'] = 0\n",
        "Modeldf['HighSchool'] = 0\n",
        "Modeldf['University'] = 0\n",
        "Modeldf['Graduate'] = 0\n",
        "\n",
        "ELoc = df.columns.get_loc('education')\n",
        "\n",
        "NDLoc = Modeldf.columns.get_loc('No_Degree')\n",
        "HSLoc = Modeldf.columns.get_loc('HighSchool')\n",
        "ULoc = Modeldf.columns.get_loc('University')\n",
        "GLoc = Modeldf.columns.get_loc('Graduate')\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,ELoc] == 1:\n",
        "    Modeldf.iat[i,NDLoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 2:\n",
        "    Modeldf.iat[i,HSLoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 3:\n",
        "    Modeldf.iat[i,ULoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 4:\n",
        "    Modeldf.iat[i,GLoc] = 1\n",
        "\n",
        "features = features + ['No_Degree', 'HighSchool', 'University', 'Graduate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s-00CbDVgxyb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d3753d8c-84c8-4c70-9d67-3b259bd6e142"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 963 ms (2023-04-23T18:44:43/2023-04-23T18:44:44)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the urban column into more binary columns.\n",
        "Modeldf['Rual'] = 0\n",
        "Modeldf['Suburban'] = 0\n",
        "Modeldf['Urban'] = 0\n",
        "\n",
        "Urloc = df.columns.get_loc('urban')\n",
        "\n",
        "RLoc = Modeldf.columns.get_loc('Rual')\n",
        "SULoc = Modeldf.columns.get_loc('Suburban')\n",
        "ULoc = Modeldf.columns.get_loc('Urban')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Urloc] == 1:\n",
        "    Modeldf.iat[i,RLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Urloc] == 2:\n",
        "    Modeldf.iat[i,SULoc] = 1\n",
        "\n",
        "  elif df.iat[i,Urloc] == 3:\n",
        "    Modeldf.iat[i,ULoc] = 1\n",
        "\n",
        "features = features + ['Rual', 'Suburban', 'Urban']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lhea8Nl6I7MI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e587ee2e-55ce-43b9-a715-cad0ed8f008a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 530 ms (2023-04-23T18:44:44/2023-04-23T18:44:45)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the gender column into more binary columns.\n",
        "Modeldf['Male'] = 0\n",
        "Modeldf['Female'] = 0\n",
        "Modeldf['Other_Gender'] = 0\n",
        "\n",
        "gloc = df.columns.get_loc('gender')\n",
        "\n",
        "mLoc = Modeldf.columns.get_loc('Male')\n",
        "fLoc = Modeldf.columns.get_loc('Female')\n",
        "oLoc = Modeldf.columns.get_loc('Other_Gender')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,gloc] == 1:\n",
        "    Modeldf.iat[i,mLoc] = 1\n",
        "\n",
        "  elif df.iat[i,gloc] == 2:\n",
        "    Modeldf.iat[i,fLoc] = 1\n",
        "\n",
        "  elif df.iat[i,gloc] == 3:\n",
        "    Modeldf.iat[i,oLoc] = 1\n",
        "\n",
        "features = features + ['Male', 'Female', 'Other_Gender']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9DN6yasKKI1G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "918c3127-5246-4cf3-9634-a7faa9f96e23"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 378 ms (2023-04-23T18:44:45/2023-04-23T18:44:45)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Normalizes the engnat column into more binary columns.\n",
        "Modeldf['Engnat'] = 0\n",
        "\n",
        "eloc = df.columns.get_loc('engnat')\n",
        "\n",
        "newELoc = Modeldf.columns.get_loc('Engnat')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,eloc] == 1:\n",
        "    Modeldf.iat[i,newELoc] = 1\n",
        "\n",
        "features = features + ['Engnat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jerGqHteKw22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ae176b16-45c3-4f9f-f544-5bc39f7a2380"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 438 ms (2023-04-23T18:44:45/2023-04-23T18:44:46)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the hand column into more binary columns.\n",
        "Modeldf['Right_h'] = 0\n",
        "Modeldf['Left_h'] = 0\n",
        "Modeldf['Ambidextrous'] = 0\n",
        "\n",
        "hloc = df.columns.get_loc('hand')\n",
        "\n",
        "rLoc = Modeldf.columns.get_loc('Right_h')\n",
        "lLoc = Modeldf.columns.get_loc('Left_h')\n",
        "aLoc = Modeldf.columns.get_loc('Ambidextrous')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,hloc] == 1:\n",
        "    Modeldf.iat[i,rLoc] = 1\n",
        "\n",
        "  elif df.iat[i,hloc] == 2:\n",
        "    Modeldf.iat[i,lLoc] = 1\n",
        "\n",
        "  elif df.iat[i,hloc] == 3:\n",
        "    Modeldf.iat[i,aLoc] = 1\n",
        "\n",
        "features = features + ['Right_h', 'Left_h', 'Ambidextrous']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KyScZFDvLp_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fa95a546-7224-448d-d4fe-13be0e789955"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 1.74 s (2023-04-23T18:44:46/2023-04-23T18:44:47)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the religion column into more binary columns.\n",
        "Modeldf['Agnostic'] = 0\n",
        "Modeldf['Atheist'] = 0\n",
        "Modeldf['Buddhist'] = 0\n",
        "Modeldf['Catholic'] = 0\n",
        "Modeldf['Mormon'] = 0\n",
        "Modeldf['Protestant'] = 0\n",
        "Modeldf['Other_Christian'] = 0\n",
        "Modeldf['Hindu'] = 0\n",
        "Modeldf['Jewish'] = 0\n",
        "Modeldf['Muslim'] = 0\n",
        "Modeldf['Sikh'] = 0\n",
        "Modeldf['Other_Religion'] = 0\n",
        "\n",
        "Reloc = df.columns.get_loc('religion')\n",
        "\n",
        "agLoc = Modeldf.columns.get_loc('Agnostic')\n",
        "athLoc = Modeldf.columns.get_loc('Atheist')\n",
        "bLoc = Modeldf.columns.get_loc('Buddhist')\n",
        "caLoc = Modeldf.columns.get_loc('Catholic')\n",
        "moLoc = Modeldf.columns.get_loc('Mormon')\n",
        "pLoc = Modeldf.columns.get_loc('Protestant')\n",
        "ocLoc = Modeldf.columns.get_loc('Other_Christian')\n",
        "hLoc = Modeldf.columns.get_loc('Hindu')\n",
        "jLoc = Modeldf.columns.get_loc('Jewish')\n",
        "muLoc = Modeldf.columns.get_loc('Muslim')\n",
        "siLoc = Modeldf.columns.get_loc('Sikh')\n",
        "orLoc = Modeldf.columns.get_loc('Other_Religion')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Reloc] == 1:\n",
        "    Modeldf.iat[i,agLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 2:\n",
        "    Modeldf.iat[i,athLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 3:\n",
        "    Modeldf.iat[i,bLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 4:\n",
        "    Modeldf.iat[i,caLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 5:\n",
        "    Modeldf.iat[i,moLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 6:\n",
        "    Modeldf.iat[i,pLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 7:\n",
        "    Modeldf.iat[i,ocLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 8:\n",
        "    Modeldf.iat[i,hLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 9:\n",
        "    Modeldf.iat[i,jLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 10:\n",
        "    Modeldf.iat[i,muLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 11:\n",
        "    Modeldf.iat[i,siLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 12:\n",
        "    Modeldf.iat[i,orLoc] = 1\n",
        "\n",
        "features = features + ['Agnostic', 'Atheist', 'Buddhist', 'Catholic', 'Mormon', 'Protestant', 'Other_Christian', 'Hindu', 'Jewish', 'Muslim', 'Sikh', 'Other_Religion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UPS6mn02Nmgb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "290d9799-bf16-493e-d09f-a0d017faed63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 908 ms (2023-04-23T18:44:48/2023-04-23T18:44:48)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the orientation column into more binary columns.\n",
        "Modeldf['Heterosexual'] = 0\n",
        "Modeldf['Bisexual'] = 0\n",
        "Modeldf['Homosexual'] = 0\n",
        "Modeldf['Asexual'] = 0\n",
        "Modeldf['Other_Orientation'] = 0\n",
        "\n",
        "Orloc = df.columns.get_loc('orientation')\n",
        "\n",
        "HeLoc = Modeldf.columns.get_loc('Heterosexual')\n",
        "BLoc = Modeldf.columns.get_loc('Bisexual')\n",
        "HoLoc = Modeldf.columns.get_loc('Homosexual')\n",
        "AsLoc = Modeldf.columns.get_loc('Asexual')\n",
        "OtLoc = Modeldf.columns.get_loc('Other_Orientation')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Orloc] == 1:\n",
        "    Modeldf.iat[i,HeLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 2:\n",
        "    Modeldf.iat[i,BLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 3:\n",
        "    Modeldf.iat[i,HoLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 4:\n",
        "    Modeldf.iat[i,AsLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 5:\n",
        "    Modeldf.iat[i,OtLoc] = 1\n",
        "\n",
        "features = features + ['Heterosexual', 'Bisexual', 'Homosexual', 'Asexual', 'Other_Orientation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S7q_UpEwOXMj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "62cb2ac9-6da7-478a-9eb7-2770f15a2314"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2.29 s (2023-04-23T18:44:49/2023-04-23T18:44:51)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the race column into more binary columns.\n",
        "Modeldf['Asian'] = 0\n",
        "Modeldf['Arab'] = 0\n",
        "Modeldf['Black'] = 0\n",
        "Modeldf['Indigenous_Australian'] = 0\n",
        "Modeldf['Native_American'] = 0\n",
        "Modeldf['White'] = 0\n",
        "Modeldf['Other_Race'] = 0\n",
        "\n",
        "Raloc = df.columns.get_loc('race')\n",
        "\n",
        "AsLoc = Modeldf.columns.get_loc('Asian')\n",
        "ArhLoc = Modeldf.columns.get_loc('Arab')\n",
        "BlLoc = Modeldf.columns.get_loc('Black')\n",
        "IaLoc = Modeldf.columns.get_loc('Indigenous_Australian')\n",
        "NaLoc = Modeldf.columns.get_loc('Native_American')\n",
        "WLoc = Modeldf.columns.get_loc('White')\n",
        "OrLoc = Modeldf.columns.get_loc('Other_Race')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Raloc] == 10:\n",
        "    Modeldf.iat[i,AsLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 20:\n",
        "    Modeldf.iat[i,ArhLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 30:\n",
        "    Modeldf.iat[i,BlLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 40:\n",
        "    Modeldf.iat[i,IaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 50:\n",
        "    Modeldf.iat[i,NaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 60:\n",
        "    Modeldf.iat[i,WLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 70:\n",
        "    Modeldf.iat[i,OrLoc] = 1\n",
        "\n",
        "features = features + ['Asian', 'Arab', 'Black', 'Indigenous_Australian', 'Native_American', 'White', 'Other_Race']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DuU_NNexPsn-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "17e8645d-8ed5-472b-af28-df9eb2afa6e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 391 ms (2023-04-23T18:44:51/2023-04-23T18:44:51)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Normalizes the voted column into more binary columns.\n",
        "Modeldf['Voted'] = 0\n",
        "\n",
        "vloc = df.columns.get_loc('voted')\n",
        "\n",
        "newVLoc = Modeldf.columns.get_loc('Voted')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,vloc] == 1:\n",
        "    Modeldf.iat[i,newVLoc] = 1\n",
        "\n",
        "features = features + ['Voted']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "45qg7RY3QGCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "54972f0a-135d-47ff-e69c-f97d0b873d6c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 699 ms (2023-04-23T18:44:51/2023-04-23T18:44:52)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Breaks the married column into more binary columns.\n",
        "Modeldf['Never_married'] = 0\n",
        "Modeldf['Married'] = 0\n",
        "Modeldf['Previously_married'] = 0\n",
        "\n",
        "mloc = df.columns.get_loc('married')\n",
        "\n",
        "NmLoc = Modeldf.columns.get_loc('Never_married')\n",
        "MaLoc = Modeldf.columns.get_loc('Married')\n",
        "PmLoc = Modeldf.columns.get_loc('Previously_married')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,mloc] == 1:\n",
        "    Modeldf.iat[i,NmLoc] = 1\n",
        "\n",
        "  elif df.iat[i,mloc] == 2:\n",
        "    Modeldf.iat[i,MaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,mloc] == 3:\n",
        "    Modeldf.iat[i,PmLoc] = 1\n",
        "\n",
        "features = features + ['Never_married', 'Married', 'Previously_married']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the data from the TIPI questions and changes it to a usable form."
      ],
      "metadata": {
        "id": "yxFUdCRTVXJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BEl8ZDPR1C3m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ebdbc3a5-ba97-4085-de3d-e16fef217b9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 4.98 s (2023-04-23T18:44:52/2023-04-23T18:44:57)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Changing the TIPI answers to a usable form.\n",
        "#Only Run this cell ONCE!\n",
        "NomScore = ['tipi1', 'tipi7', 'tipi3', 'tipi9', 'tipi5']\n",
        "RevScore = ['tipi6', 'tipi2', 'tipi8', 'tipi4', 'tipi10']\n",
        "\n",
        "Rev1 = df.columns.get_loc('tipi6')\n",
        "Rev2 = df.columns.get_loc('tipi2')\n",
        "Rev3 = df.columns.get_loc('tipi8')\n",
        "Rev4 = df.columns.get_loc('tipi4')\n",
        "Rev5 = df.columns.get_loc('tipi10')\n",
        "\n",
        "RevLocs = [Rev1, Rev2, Rev3, Rev4, Rev5]\n",
        "\n",
        "for j in RevLocs:\n",
        "  for i in range(len(df)):\n",
        "    if df.iat[i,j] == 1:\n",
        "      df.iat[i,j] = 7\n",
        "\n",
        "    elif df.iat[i,j] == 2:\n",
        "      df.iat[i,j] = 6\n",
        "\n",
        "    elif df.iat[i,j] == 3:\n",
        "      df.iat[i,j] = 5\n",
        "\n",
        "    elif df.iat[i,j] == 4:\n",
        "      df.iat[i,j] = 4\n",
        "\n",
        "    elif df.iat[i,j] == 5:\n",
        "      df.iat[i,j] = 3\n",
        "\n",
        "    elif df.iat[i,j] == 6:\n",
        "      df.iat[i,j] = 2\n",
        "\n",
        "    elif df.iat[i,j] == 7:\n",
        "      df.iat[i,j] = 1\n",
        "\n",
        "df['Extraversion'] = (df[NomScore[0]] + df[RevScore[0]])/2\n",
        "df['Agreeableness'] = (df[NomScore[1]] + df[RevScore[1]])/2\n",
        "df['Conscientiousness'] = (df[NomScore[2]] + df[RevScore[2]])/2\n",
        "df['Emotional_Stability'] = (df[NomScore[3]] + df[RevScore[3]])/2\n",
        "df['Openness'] = (df[NomScore[4]] + df[RevScore[4]])/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1bmmJH41o8v"
      },
      "source": [
        "###Normalize the data so all continuous values is on a 0 to 1 scale to match the other features which are categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "drDSk9utUxNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7c79d228-2604-4be4-dba7-ddcf51b462a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 43.2 ms (2023-04-23T18:44:57/2023-04-23T18:44:57)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cf899db73ca0>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sidedf['Extraversion'] = df['Extraversion']\n",
            "<ipython-input-19-cf899db73ca0>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sidedf['Agreeableness'] = df['Agreeableness']\n",
            "<ipython-input-19-cf899db73ca0>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sidedf['Conscientiousness'] = df['Conscientiousness']\n",
            "<ipython-input-19-cf899db73ca0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sidedf['Emotional_Stability'] = df['Emotional_Stability']\n",
            "<ipython-input-19-cf899db73ca0>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sidedf['Openness'] = df['Openness']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           age  familysize  Extraversion  Agreeableness  Conscientiousness   \n",
              "0     0.081395    0.030075      0.214286       1.000000           0.857143  \\\n",
              "1     0.244186    0.015038      0.428571       0.428571           0.857143   \n",
              "2     0.069767    0.007519      0.357143       0.357143           0.571429   \n",
              "3     0.034884    0.022556      0.428571       0.714286           0.571429   \n",
              "4     0.058140    0.015038      0.857143       0.571429           0.857143   \n",
              "...        ...         ...           ...            ...                ...   \n",
              "8202  0.104651    0.022556      0.142857       0.642857           0.142857   \n",
              "8203  0.058140    0.015038      0.357143       0.571429           0.714286   \n",
              "8204  0.383721    0.045113      0.357143       0.428571           0.857143   \n",
              "8205  0.011628    0.022556      0.428571       0.428571           0.214286   \n",
              "8206  0.034884    0.022556      0.785714       0.571429           0.714286   \n",
              "\n",
              "      Emotional_Stability  Openness  \n",
              "0                0.928571  0.571429  \n",
              "1                1.000000  0.857143  \n",
              "2                0.928571  0.857143  \n",
              "3                0.928571  0.571429  \n",
              "4                0.928571  0.928571  \n",
              "...                   ...       ...  \n",
              "8202             0.357143  0.785714  \n",
              "8203             0.142857  0.642857  \n",
              "8204             0.214286  0.642857  \n",
              "8205             0.142857  0.357143  \n",
              "8206             0.571429  0.500000  \n",
              "\n",
              "[8207 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa6c340d-b9c6-4d0b-831f-adaebbe7f55c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>familysize</th>\n",
              "      <th>Extraversion</th>\n",
              "      <th>Agreeableness</th>\n",
              "      <th>Conscientiousness</th>\n",
              "      <th>Emotional_Stability</th>\n",
              "      <th>Openness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.244186</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>0.104651</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8203</th>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8204</th>\n",
              "      <td>0.383721</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8205</th>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8206</th>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8207 rows  7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa6c340d-b9c6-4d0b-831f-adaebbe7f55c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa6c340d-b9c6-4d0b-831f-adaebbe7f55c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa6c340d-b9c6-4d0b-831f-adaebbe7f55c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Normailizes the non-categorical data.\n",
        "normal = MinMaxScaler()\n",
        "\n",
        "sidedf = pd.DataFrame()\n",
        "sidedf = df[['age', 'familysize']]   \n",
        "sidedf['Extraversion'] = df['Extraversion']\n",
        "sidedf['Agreeableness'] = df['Agreeableness']\n",
        "sidedf['Conscientiousness'] = df['Conscientiousness']\n",
        "sidedf['Emotional_Stability'] = df['Emotional_Stability']\n",
        "sidedf['Openness'] = df['Openness']\n",
        "                      \n",
        "Modeldf['Age'] = 0\n",
        "Modeldf['FamilySize'] = 0\n",
        "\n",
        "sidedf = pd.DataFrame(normal.fit_transform(sidedf), columns=['age','familysize', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness'])\n",
        "\n",
        "Modeldf['Age'] = sidedf['age']\n",
        "Modeldf['FamilySize'] = sidedf['familysize']\n",
        "Modeldf['Extraversion'] = sidedf['Extraversion']\n",
        "Modeldf['Agreeableness'] = sidedf['Agreeableness']\n",
        "Modeldf['Conscientiousness'] = sidedf['Conscientiousness']\n",
        "Modeldf['Emotional_Stability'] = sidedf['Emotional_Stability']\n",
        "Modeldf['Openness'] = sidedf['Openness']\n",
        "ut.pprint(sidedf)\n",
        "features = features + ['Age', 'FamilySize', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the data for classification and Regression Approach"
      ],
      "metadata": {
        "id": "K8kLqWylSwKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the target data can be a catigorical value or a continus value, I decided try some regrssive models and then classifers to see what gives better results."
      ],
      "metadata": {
        "id": "ttDD5kY-UGE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classdf = Modeldf\n",
        "Regressiondf = Modeldf"
      ],
      "metadata": {
        "id": "5IkZS27CJD-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d1fce6a-920f-4b7f-c36f-e65325ea28b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 4.43 ms (2023-04-23T18:44:57/2023-04-23T18:44:57)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Approch"
      ],
      "metadata": {
        "id": "sUxogZ5dbXoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Examining the Data before Regression"
      ],
      "metadata": {
        "id": "fS-Q_DA0cnH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Data is checked and verified that all preprocessing steps executed correctly."
      ],
      "metadata": {
        "id": "HsYkYykd2aKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.sum(Classdf[['Depression_Score']] <= 9) )\n",
        "print( np.sum( Classdf[['Depression_Score']] == 10) + np.sum(Classdf[['Depression_Score']] == 11) + np.sum(Classdf[['Depression_Score']] == 12) + np.sum(Classdf[['Depression_Score']] == 13) )\n",
        "print( np.sum(Classdf[['Depression_Score']] == 14) + np.sum(Classdf[['Depression_Score']] == 15) + np.sum(Classdf[['Depression_Score']] == 16) + np.sum(Classdf[['Depression_Score']] == 17) + np.sum(Classdf[['Depression_Score']] == 18) + np.sum(Classdf[['Depression_Score']] == 19) + np.sum(Classdf[['Depression_Score']] == 20) )\n",
        "print( np.sum(Classdf[['Depression_Score']] == 21) + np.sum(Classdf[['Depression_Score']] == 22) + np.sum(Classdf[['Depression_Score']] == 23) + np.sum(Classdf[['Depression_Score']] == 24) +  + np.sum(Classdf[['Depression_Score']] == 25) + np.sum(Classdf[['Depression_Score']] == 26) + np.sum(Classdf[['Depression_Score']] == 27) )\n",
        "print( np.sum(Classdf[['Depression_Score']] >= 28))\n",
        "\n",
        "Regressiondf = Modeldf\n",
        "display(Regressiondf.describe(include='all'))\n",
        "ut.pprint(Regressiondf)"
      ],
      "metadata": {
        "id": "P6pZ7-C4cnH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "outputId": "c9792612-c795-4364-fd0c-23fa55ca527f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 277 ms (2023-04-23T18:44:58/2023-04-23T18:44:58)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depression_Score    1736\n",
            "dtype: int64\n",
            "Depression_Score    678\n",
            "dtype: int64\n",
            "Depression_Score    1386\n",
            "dtype: int64\n",
            "Depression_Score    1287\n",
            "dtype: int64\n",
            "Depression_Score    3120\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Depression_Score  Anxiety_Score  Stress_Score    No_Degree   \n",
              "count            8207.0         8207.0        8207.0  8207.000000  \\\n",
              "mean          22.060436      15.794687      22.07189     0.270135   \n",
              "std           12.678143      10.721868     10.743251     0.444057   \n",
              "min                 0.0            0.0           0.0     0.000000   \n",
              "25%                11.0            7.0          14.0     0.000000   \n",
              "50%                22.0           14.0          22.0     0.000000   \n",
              "75%                33.0           23.0          30.0     1.000000   \n",
              "max                42.0           42.0          42.0     1.000000   \n",
              "\n",
              "        HighSchool   University     Graduate         Rual     Suburban   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.430974     0.202876     0.083343     0.193493     0.458145   \n",
              "std       0.495243     0.402165     0.276418     0.395060     0.498275   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "             Urban         Male       Female  Other_Gender       Engnat   \n",
              "count  8207.000000  8207.000000  8207.000000   8207.000000  8207.000000  \\\n",
              "mean      0.339344     0.291946     0.666870      0.038869     0.909955   \n",
              "std       0.473516     0.454685     0.471361      0.193295     0.286264   \n",
              "min       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000      0.000000     1.000000   \n",
              "50%       0.000000     0.000000     1.000000      0.000000     1.000000   \n",
              "75%       1.000000     1.000000     1.000000      0.000000     1.000000   \n",
              "max       1.000000     1.000000     1.000000      1.000000     1.000000   \n",
              "\n",
              "           Right_h       Left_h  Ambidextrous     Agnostic      Atheist   \n",
              "count  8207.000000  8207.000000   8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.859023     0.101133      0.036067     0.197027     0.191178   \n",
              "std       0.348019     0.301523      0.186468     0.397777     0.393253   \n",
              "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "25%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "50%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "75%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000      1.000000     1.000000     1.000000   \n",
              "\n",
              "          Buddhist     Catholic       Mormon  Protestant  Other_Christian   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.00000      8207.000000  \\\n",
              "mean      0.013038     0.144511     0.010966     0.08968         0.171073   \n",
              "std       0.113443     0.351628     0.104150     0.28574         0.376596   \n",
              "min       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.00000         1.000000   \n",
              "\n",
              "             Hindu       Jewish       Muslim         Sikh  Other_Religion   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.000000     8207.000000  \\\n",
              "mean      0.005239     0.013525     0.024248     0.001218        0.120141   \n",
              "std       0.072198     0.115515     0.153826     0.034887        0.325147   \n",
              "min       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000        1.000000   \n",
              "\n",
              "       Heterosexual     Bisexual   Homosexual     Asexual  Other_Orientation   \n",
              "count   8207.000000  8207.000000  8207.000000  8207.00000        8207.000000  \\\n",
              "mean       0.610454     0.204947     0.061411     0.04155           0.061533   \n",
              "std        0.487677     0.403687     0.240097     0.19957           0.240320   \n",
              "min        0.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "25%        0.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "50%        1.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "75%        1.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "max        1.000000     1.000000     1.000000     1.00000           1.000000   \n",
              "\n",
              "             Asian         Arab        Black  Indigenous_Australian   \n",
              "count  8207.000000  8207.000000  8207.000000            8207.000000  \\\n",
              "mean      0.086755     0.004386     0.046424               0.000122   \n",
              "std       0.281493     0.066089     0.210414               0.011038   \n",
              "min       0.000000     0.000000     0.000000               0.000000   \n",
              "25%       0.000000     0.000000     0.000000               0.000000   \n",
              "50%       0.000000     0.000000     0.000000               0.000000   \n",
              "75%       0.000000     0.000000     0.000000               0.000000   \n",
              "max       1.000000     1.000000     1.000000               1.000000   \n",
              "\n",
              "       Native_American        White   Other_Race        Voted  Never_married   \n",
              "count      8207.000000  8207.000000  8207.000000  8207.000000    8207.000000  \\\n",
              "mean          0.017546     0.717193     0.127574     0.295601       0.783234   \n",
              "std           0.131302     0.450391     0.333635     0.456340       0.412067   \n",
              "min           0.000000     0.000000     0.000000     0.000000       0.000000   \n",
              "25%           0.000000     0.000000     0.000000     0.000000       1.000000   \n",
              "50%           0.000000     1.000000     0.000000     0.000000       1.000000   \n",
              "75%           0.000000     1.000000     0.000000     1.000000       1.000000   \n",
              "max           1.000000     1.000000     1.000000     1.000000       1.000000   \n",
              "\n",
              "           Married  Previously_married          Age   FamilySize   \n",
              "count  8207.000000         8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.149872            0.062020     0.135942     0.020582   \n",
              "std       0.356968            0.241207     0.144729     0.016407   \n",
              "min       0.000000            0.000000     0.000000     0.000000   \n",
              "25%       0.000000            0.000000     0.034884     0.015038   \n",
              "50%       0.000000            0.000000     0.081395     0.015038   \n",
              "75%       0.000000            0.000000     0.174419     0.022556   \n",
              "max       1.000000            1.000000     1.000000     1.000000   \n",
              "\n",
              "       Extraversion  Agreeableness  Conscientiousness  Emotional_Stability   \n",
              "count   8207.000000    8207.000000        8207.000000          8207.000000  \\\n",
              "mean       0.449416       0.650412           0.617261             0.431165   \n",
              "std        0.242194       0.201655           0.233309             0.227893   \n",
              "min        0.000000       0.000000           0.000000             0.000000   \n",
              "25%        0.214286       0.500000           0.428571             0.285714   \n",
              "50%        0.428571       0.642857           0.642857             0.357143   \n",
              "75%        0.642857       0.785714           0.785714             0.571429   \n",
              "max        1.000000       1.000000           1.000000             1.000000   \n",
              "\n",
              "          Openness  \n",
              "count  8207.000000  \n",
              "mean      0.687096  \n",
              "std       0.207827  \n",
              "min       0.000000  \n",
              "25%       0.571429  \n",
              "50%       0.714286  \n",
              "75%       0.857143  \n",
              "max       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-360137c6-2f6b-4869-91eb-8a3528d45882\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Depression_Score</th>\n",
              "      <th>Anxiety_Score</th>\n",
              "      <th>Stress_Score</th>\n",
              "      <th>No_Degree</th>\n",
              "      <th>HighSchool</th>\n",
              "      <th>University</th>\n",
              "      <th>Graduate</th>\n",
              "      <th>Rual</th>\n",
              "      <th>Suburban</th>\n",
              "      <th>Urban</th>\n",
              "      <th>Male</th>\n",
              "      <th>Female</th>\n",
              "      <th>Other_Gender</th>\n",
              "      <th>Engnat</th>\n",
              "      <th>Right_h</th>\n",
              "      <th>Left_h</th>\n",
              "      <th>Ambidextrous</th>\n",
              "      <th>Agnostic</th>\n",
              "      <th>Atheist</th>\n",
              "      <th>Buddhist</th>\n",
              "      <th>Catholic</th>\n",
              "      <th>Mormon</th>\n",
              "      <th>Protestant</th>\n",
              "      <th>Other_Christian</th>\n",
              "      <th>Hindu</th>\n",
              "      <th>Jewish</th>\n",
              "      <th>Muslim</th>\n",
              "      <th>Sikh</th>\n",
              "      <th>Other_Religion</th>\n",
              "      <th>Heterosexual</th>\n",
              "      <th>Bisexual</th>\n",
              "      <th>Homosexual</th>\n",
              "      <th>Asexual</th>\n",
              "      <th>Other_Orientation</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Arab</th>\n",
              "      <th>Black</th>\n",
              "      <th>Indigenous_Australian</th>\n",
              "      <th>Native_American</th>\n",
              "      <th>White</th>\n",
              "      <th>Other_Race</th>\n",
              "      <th>Voted</th>\n",
              "      <th>Never_married</th>\n",
              "      <th>Married</th>\n",
              "      <th>Previously_married</th>\n",
              "      <th>Age</th>\n",
              "      <th>FamilySize</th>\n",
              "      <th>Extraversion</th>\n",
              "      <th>Agreeableness</th>\n",
              "      <th>Conscientiousness</th>\n",
              "      <th>Emotional_Stability</th>\n",
              "      <th>Openness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.00000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.00000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>22.060436</td>\n",
              "      <td>15.794687</td>\n",
              "      <td>22.07189</td>\n",
              "      <td>0.270135</td>\n",
              "      <td>0.430974</td>\n",
              "      <td>0.202876</td>\n",
              "      <td>0.083343</td>\n",
              "      <td>0.193493</td>\n",
              "      <td>0.458145</td>\n",
              "      <td>0.339344</td>\n",
              "      <td>0.291946</td>\n",
              "      <td>0.666870</td>\n",
              "      <td>0.038869</td>\n",
              "      <td>0.909955</td>\n",
              "      <td>0.859023</td>\n",
              "      <td>0.101133</td>\n",
              "      <td>0.036067</td>\n",
              "      <td>0.197027</td>\n",
              "      <td>0.191178</td>\n",
              "      <td>0.013038</td>\n",
              "      <td>0.144511</td>\n",
              "      <td>0.010966</td>\n",
              "      <td>0.08968</td>\n",
              "      <td>0.171073</td>\n",
              "      <td>0.005239</td>\n",
              "      <td>0.013525</td>\n",
              "      <td>0.024248</td>\n",
              "      <td>0.001218</td>\n",
              "      <td>0.120141</td>\n",
              "      <td>0.610454</td>\n",
              "      <td>0.204947</td>\n",
              "      <td>0.061411</td>\n",
              "      <td>0.04155</td>\n",
              "      <td>0.061533</td>\n",
              "      <td>0.086755</td>\n",
              "      <td>0.004386</td>\n",
              "      <td>0.046424</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.017546</td>\n",
              "      <td>0.717193</td>\n",
              "      <td>0.127574</td>\n",
              "      <td>0.295601</td>\n",
              "      <td>0.783234</td>\n",
              "      <td>0.149872</td>\n",
              "      <td>0.062020</td>\n",
              "      <td>0.135942</td>\n",
              "      <td>0.020582</td>\n",
              "      <td>0.449416</td>\n",
              "      <td>0.650412</td>\n",
              "      <td>0.617261</td>\n",
              "      <td>0.431165</td>\n",
              "      <td>0.687096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12.678143</td>\n",
              "      <td>10.721868</td>\n",
              "      <td>10.743251</td>\n",
              "      <td>0.444057</td>\n",
              "      <td>0.495243</td>\n",
              "      <td>0.402165</td>\n",
              "      <td>0.276418</td>\n",
              "      <td>0.395060</td>\n",
              "      <td>0.498275</td>\n",
              "      <td>0.473516</td>\n",
              "      <td>0.454685</td>\n",
              "      <td>0.471361</td>\n",
              "      <td>0.193295</td>\n",
              "      <td>0.286264</td>\n",
              "      <td>0.348019</td>\n",
              "      <td>0.301523</td>\n",
              "      <td>0.186468</td>\n",
              "      <td>0.397777</td>\n",
              "      <td>0.393253</td>\n",
              "      <td>0.113443</td>\n",
              "      <td>0.351628</td>\n",
              "      <td>0.104150</td>\n",
              "      <td>0.28574</td>\n",
              "      <td>0.376596</td>\n",
              "      <td>0.072198</td>\n",
              "      <td>0.115515</td>\n",
              "      <td>0.153826</td>\n",
              "      <td>0.034887</td>\n",
              "      <td>0.325147</td>\n",
              "      <td>0.487677</td>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.240097</td>\n",
              "      <td>0.19957</td>\n",
              "      <td>0.240320</td>\n",
              "      <td>0.281493</td>\n",
              "      <td>0.066089</td>\n",
              "      <td>0.210414</td>\n",
              "      <td>0.011038</td>\n",
              "      <td>0.131302</td>\n",
              "      <td>0.450391</td>\n",
              "      <td>0.333635</td>\n",
              "      <td>0.456340</td>\n",
              "      <td>0.412067</td>\n",
              "      <td>0.356968</td>\n",
              "      <td>0.241207</td>\n",
              "      <td>0.144729</td>\n",
              "      <td>0.016407</td>\n",
              "      <td>0.242194</td>\n",
              "      <td>0.201655</td>\n",
              "      <td>0.233309</td>\n",
              "      <td>0.227893</td>\n",
              "      <td>0.207827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>22.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>33.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174419</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-360137c6-2f6b-4869-91eb-8a3528d45882')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-360137c6-2f6b-4869-91eb-8a3528d45882 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-360137c6-2f6b-4869-91eb-8a3528d45882');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Depression_Score  Anxiety_Score  Stress_Score  No_Degree  HighSchool   \n",
              "index                                                                         \n",
              "0                    13              6            12          0           1  \\\n",
              "1                     0              0             0          0           0   \n",
              "2                    14              2            14          0           1   \n",
              "3                     9              1             1          0           1   \n",
              "4                     0              1             0          0           1   \n",
              "...                 ...            ...           ...        ...         ...   \n",
              "8202                 42             39            42          0           0   \n",
              "8203                 28             28            31          1           0   \n",
              "8204                 42             33            40          0           0   \n",
              "8205                 42             34            39          1           0   \n",
              "8206                 15             21            30          1           0   \n",
              "\n",
              "       University  Graduate  Rual  Suburban  Urban  Male  Female   \n",
              "index                                                              \n",
              "0               0         0     0         0      1     0       1  \\\n",
              "1               1         0     0         1      0     1       0   \n",
              "2               0         0     1         0      0     1       0   \n",
              "3               0         0     0         0      1     0       1   \n",
              "4               0         0     0         1      0     1       0   \n",
              "...           ...       ...   ...       ...    ...   ...     ...   \n",
              "8202            1         0     0         0      1     0       1   \n",
              "8203            0         0     0         0      1     0       0   \n",
              "8204            0         1     0         1      0     0       1   \n",
              "8205            0         0     0         0      1     0       1   \n",
              "8206            0         0     0         1      0     0       1   \n",
              "\n",
              "       Other_Gender  Engnat  Right_h  Left_h  Ambidextrous  Agnostic  Atheist   \n",
              "index                                                                           \n",
              "0                 0       0        1       0             0         0        0  \\\n",
              "1                 0       1        1       0             0         1        0   \n",
              "2                 0       1        0       0             1         0        1   \n",
              "3                 0       1        1       0             0         1        0   \n",
              "4                 0       1        1       0             0         0        0   \n",
              "...             ...     ...      ...     ...           ...       ...      ...   \n",
              "8202              0       1        1       0             0         0        0   \n",
              "8203              1       1        1       0             0         0        1   \n",
              "8204              0       1        1       0             0         0        0   \n",
              "8205              0       1        1       0             0         1        0   \n",
              "8206              0       1        1       0             0         1        0   \n",
              "\n",
              "       Buddhist  Catholic  Mormon  Protestant  Other_Christian  Hindu  Jewish   \n",
              "index                                                                           \n",
              "0             0         1       0           0                0      0       0  \\\n",
              "1             0         0       0           0                0      0       0   \n",
              "2             0         0       0           0                0      0       0   \n",
              "3             0         0       0           0                0      0       0   \n",
              "4             0         0       0           0                1      0       0   \n",
              "...         ...       ...     ...         ...              ...    ...     ...   \n",
              "8202          0         0       0           0                1      0       0   \n",
              "8203          0         0       0           0                0      0       0   \n",
              "8204          0         1       0           0                0      0       0   \n",
              "8205          0         0       0           0                0      0       0   \n",
              "8206          0         0       0           0                0      0       0   \n",
              "\n",
              "       Muslim  Sikh  Other_Religion  Heterosexual  Bisexual  Homosexual   \n",
              "index                                                                     \n",
              "0           0     0               0             1         0           0  \\\n",
              "1           0     0               0             1         0           0   \n",
              "2           0     0               0             1         0           0   \n",
              "3           0     0               0             0         0           0   \n",
              "4           0     0               0             1         0           0   \n",
              "...       ...   ...             ...           ...       ...         ...   \n",
              "8202        0     0               0             1         0           0   \n",
              "8203        0     0               0             0         0           0   \n",
              "8204        0     0               0             1         0           0   \n",
              "8205        0     0               0             0         0           0   \n",
              "8206        0     0               0             1         0           0   \n",
              "\n",
              "       Asexual  Other_Orientation  Asian  Arab  Black  Indigenous_Australian   \n",
              "index                                                                          \n",
              "0            0                  0      0     0      0                      0  \\\n",
              "1            0                  0      0     0      0                      0   \n",
              "2            0                  0      0     0      0                      0   \n",
              "3            0                  1      0     0      0                      0   \n",
              "4            0                  0      0     0      1                      0   \n",
              "...        ...                ...    ...   ...    ...                    ...   \n",
              "8202         0                  0      1     0      0                      0   \n",
              "8203         1                  0      0     0      0                      0   \n",
              "8204         0                  0      0     0      0                      0   \n",
              "8205         0                  1      0     0      0                      0   \n",
              "8206         0                  0      0     0      0                      0   \n",
              "\n",
              "       Native_American  White  Other_Race  Voted  Never_married  Married   \n",
              "index                                                                      \n",
              "0                    0      0           1      0              1        0  \\\n",
              "1                    0      1           0      1              0        0   \n",
              "2                    0      1           0      0              1        0   \n",
              "3                    0      0           1      0              1        0   \n",
              "4                    0      0           0      0              1        0   \n",
              "...                ...    ...         ...    ...            ...      ...   \n",
              "8202                 0      0           0      1              1        0   \n",
              "8203                 0      0           1      0              1        0   \n",
              "8204                 0      1           0      1              0        1   \n",
              "8205                 0      1           0      0              1        0   \n",
              "8206                 0      0           1      0              1        0   \n",
              "\n",
              "       Previously_married       Age  FamilySize  Extraversion  Agreeableness   \n",
              "index                                                                          \n",
              "0                       0  0.081395    0.030075      0.214286       1.000000  \\\n",
              "1                       1  0.244186    0.015038      0.428571       0.428571   \n",
              "2                       0  0.069767    0.007519      0.357143       0.357143   \n",
              "3                       0  0.034884    0.022556      0.428571       0.714286   \n",
              "4                       0  0.058140    0.015038      0.857143       0.571429   \n",
              "...                   ...       ...         ...           ...            ...   \n",
              "8202                    0  0.104651    0.022556      0.142857       0.642857   \n",
              "8203                    0  0.058140    0.015038      0.357143       0.571429   \n",
              "8204                    0  0.383721    0.045113      0.357143       0.428571   \n",
              "8205                    0  0.011628    0.022556      0.428571       0.428571   \n",
              "8206                    0  0.034884    0.022556      0.785714       0.571429   \n",
              "\n",
              "       Conscientiousness  Emotional_Stability  Openness  \n",
              "index                                                    \n",
              "0               0.857143             0.928571  0.571429  \n",
              "1               0.857143             1.000000  0.857143  \n",
              "2               0.571429             0.928571  0.857143  \n",
              "3               0.571429             0.928571  0.571429  \n",
              "4               0.857143             0.928571  0.928571  \n",
              "...                  ...                  ...       ...  \n",
              "8202            0.142857             0.357143  0.785714  \n",
              "8203            0.714286             0.142857  0.642857  \n",
              "8204            0.857143             0.214286  0.642857  \n",
              "8205            0.214286             0.142857  0.357143  \n",
              "8206            0.714286             0.571429  0.500000  \n",
              "\n",
              "[8207 rows x 52 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-332069db-98e9-4487-b499-39849ae57894\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Depression_Score</th>\n",
              "      <th>Anxiety_Score</th>\n",
              "      <th>Stress_Score</th>\n",
              "      <th>No_Degree</th>\n",
              "      <th>HighSchool</th>\n",
              "      <th>University</th>\n",
              "      <th>Graduate</th>\n",
              "      <th>Rual</th>\n",
              "      <th>Suburban</th>\n",
              "      <th>Urban</th>\n",
              "      <th>Male</th>\n",
              "      <th>Female</th>\n",
              "      <th>Other_Gender</th>\n",
              "      <th>Engnat</th>\n",
              "      <th>Right_h</th>\n",
              "      <th>Left_h</th>\n",
              "      <th>Ambidextrous</th>\n",
              "      <th>Agnostic</th>\n",
              "      <th>Atheist</th>\n",
              "      <th>Buddhist</th>\n",
              "      <th>Catholic</th>\n",
              "      <th>Mormon</th>\n",
              "      <th>Protestant</th>\n",
              "      <th>Other_Christian</th>\n",
              "      <th>Hindu</th>\n",
              "      <th>Jewish</th>\n",
              "      <th>Muslim</th>\n",
              "      <th>Sikh</th>\n",
              "      <th>Other_Religion</th>\n",
              "      <th>Heterosexual</th>\n",
              "      <th>Bisexual</th>\n",
              "      <th>Homosexual</th>\n",
              "      <th>Asexual</th>\n",
              "      <th>Other_Orientation</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Arab</th>\n",
              "      <th>Black</th>\n",
              "      <th>Indigenous_Australian</th>\n",
              "      <th>Native_American</th>\n",
              "      <th>White</th>\n",
              "      <th>Other_Race</th>\n",
              "      <th>Voted</th>\n",
              "      <th>Never_married</th>\n",
              "      <th>Married</th>\n",
              "      <th>Previously_married</th>\n",
              "      <th>Age</th>\n",
              "      <th>FamilySize</th>\n",
              "      <th>Extraversion</th>\n",
              "      <th>Agreeableness</th>\n",
              "      <th>Conscientiousness</th>\n",
              "      <th>Emotional_Stability</th>\n",
              "      <th>Openness</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.244186</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>42</td>\n",
              "      <td>39</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.104651</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8203</th>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8204</th>\n",
              "      <td>42</td>\n",
              "      <td>33</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.383721</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8205</th>\n",
              "      <td>42</td>\n",
              "      <td>34</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8206</th>\n",
              "      <td>15</td>\n",
              "      <td>21</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8207 rows  52 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-332069db-98e9-4487-b499-39849ae57894')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-332069db-98e9-4487-b499-39849ae57894 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-332069db-98e9-4487-b499-39849ae57894');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Depression Model"
      ],
      "metadata": {
        "id": "cM2tmWsQVQ-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = Regressiondf[features]\n",
        "\n",
        "#Depression Model.\n",
        "D = Regressiondf[targets[0]]\n",
        "\n",
        "X_Regset, X_HoldRegset, D_Regset, D_HoldRegset = train_test_split(X, D, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "0D2kg4ojVPBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb5f254f-367b-4946-a7a0-8eed6f62c63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 23.5 ms (2023-04-23T02:00:56/2023-04-23T02:00:56)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgdfN2fYO-i"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "mURufTDvYPl9",
        "outputId": "9ad9f870-b12a-4cfc-db6a-2f01fec5a5d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 47.7 s (2023-04-23T02:00:56/2023-04-23T02:01:44)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the KNReg Model is: 16.377122967048805%\n",
            "{'algorithm': 'ball_tree', 'n_neighbors': 50, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [25,50,75],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "DepRegModel1 = GridSearchCV(KNReg, KNParaReg)\n",
        "DepRegModel1.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc1 = DepRegModel1.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {DModel_Acc1*100}%')\n",
        "print(DepRegModel1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "-dvWxx7RpMyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I spent my focus here since random forrest is both an esemble methoid and will better fit the categorical data."
      ],
      "metadata": {
        "id": "xO5JgQ1iE2GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[29,30,33],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [5,6,7],\n",
        "    'min_samples_split': [19,20,23],\n",
        "    'random_state':[randnum]\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "DepRegModel2 = GridSearchCV(RFReg, RFParaReg)\n",
        "DepRegModel2.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc2 = DepRegModel2.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {DModel_Acc2*100}%')\n",
        "print(DepRegModel2.best_params_)"
      ],
      "metadata": {
        "id": "uwQMjDZCPtg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5aae5656-57d6-4b40-c31b-813a000a82c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 46 min 40 s (2023-04-23T02:01:44/2023-04-23T02:48:25)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the RFReg Model is: 30.97660464642025%\n",
            "{'criterion': 'squared_error', 'max_depth': 7, 'min_samples_split': 23, 'n_estimators': 30, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "ubXFQB9R4iUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error'],\n",
        "    'learning_rate':[0.1, 0.2, 0.3],\n",
        "    'max_iter':[ 125, 150, 175],\n",
        "    'max_depth':[ 2, 3, 4],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "DepRegModel3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "DepRegModel3.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc3 = DepRegModel3.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc3*100}%')\n",
        "print(DepRegModel3.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "aR4-HcGZ4r4E",
        "outputId": "97051791-8ef1-4257-b971-2ef03dbcc909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2 min 55 s (2023-04-23T02:48:25/2023-04-23T02:51:20)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 32.75948636966688%\n",
            "{'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 2, 'max_iter': 175, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awobaYn3BcJD"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "YvXHN-3XBcJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [DModel_Acc1, DModel_Acc2, DModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestDepRegParams = DepRegModel1.best_params_\n",
        "\n",
        "  DepRegNei = BestDepRegParams['n_neighbors']\n",
        "  DepRegAlg = BestDepRegParams['algorithm']\n",
        "  DepRegWei = BestDepRegParams['weights']\n",
        "\n",
        "  FastDepRegMod = KNeighborsClassifier(n_neighbors=DepRegNei, algorithm=DepRegAlg, weights=DepRegWei, random_state=randnum)\n",
        "\n",
        "  FinalDepRegMod = BaggingRegressor(estimator=FastDepRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepRegMod.fit(X_Regset, D_Regset)\n",
        "\n",
        "  DRegMod_Acc = FinalDepRegMod.score(X_HoldRegset, D_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {DRegMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepRegModel'\n",
        "  joblib.dump(FinalDepRegMod, file)\n",
        "\n",
        "elif BestMod == DModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepRegModel'\n",
        "  joblib.dump(DepRegModel2, file)\n",
        "\n",
        "elif BestMod == DModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc3*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepRegModel'\n",
        "  joblib.dump(DepRegModel3, file)"
      ],
      "metadata": {
        "id": "QVNiUcsWBTsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22e126af-99ea-48f9-f796-84c2999a86b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 5.38 ms (2023-04-23T02:51:20/2023-04-23T02:51:20)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 32.75948636966688%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Anxiety Model"
      ],
      "metadata": {
        "id": "fSYPMPxnVY30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Anxiety Model.\n",
        "A = Regressiondf[targets[1]]\n",
        "randnum = 500\n",
        "\n",
        "X_Regset, X_HoldRegset, A_Regset, A_HoldRegset= train_test_split(X, A, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "6EqkYBt8Vd9F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dd32dfd-583d-42bb-bd5b-33b06a34cb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 18.2 ms (2023-04-23T02:51:20/2023-04-23T02:51:20)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz4ZqPeIZU3o"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAyJHtiAZU9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53e22b3c-4be8-4dfe-f81f-7f256d769afb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 1 min 5 s (2023-04-23T02:51:21/2023-04-23T02:52:26)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the KNReg Model is: 20.770151296374607%\n",
            "{'algorithm': 'kd_tree', 'n_neighbors': 250, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [225, 250, 275],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "AnxRegModel1 = GridSearchCV(KNReg, KNParaReg)\n",
        "AnxRegModel1.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc1 = AnxRegModel1.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {AModel_Acc1*100}%')\n",
        "print(AnxRegModel1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "-gZ2J8YzawXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[27,30,32],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [7,8,9],\n",
        "    'min_samples_split': [+8,9,10],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "AnxRegModel2 = GridSearchCV(RFReg, RFParaReg)\n",
        "AnxRegModel2.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc2 = AnxRegModel2.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {AModel_Acc2*100}%')\n",
        "print(AnxRegModel2.best_params_)"
      ],
      "metadata": {
        "id": "vFHHfRgMawXY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "424f4213-b44c-4300-b941-3a5408e605c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 49 min 45 s (2023-04-23T02:52:26/2023-04-23T03:42:12)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the RFReg Model is: 38.82669353516937%\n",
            "{'criterion': 'squared_error', 'max_depth': 7, 'min_samples_split': 10, 'n_estimators': 32, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "9UVXrkGd6Gqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error'],\n",
        "    'learning_rate':[0.2,0.3,0.4],\n",
        "    'max_iter':[10,25,50,75],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "AnxRegModel3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "AnxRegModel3.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc3 = AnxRegModel3.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc3*100}%')\n",
        "print(AnxRegModel3.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "gxdxWOzl6Gqz",
        "outputId": "fd855dd5-4f45-4e52-c6cb-abb0ee4f5ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2 min 6 s (2023-04-23T03:42:12/2023-04-23T03:44:18)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 40.72300334239149%\n",
            "{'learning_rate': 0.2, 'loss': 'squared_error', 'max_depth': 2, 'max_iter': 75, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NsXxBFawXZ"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "bynBzRhsawXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [AModel_Acc1, AModel_Acc2, AModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestAnxRegParams = AnxRegModel1.best_params_\n",
        "\n",
        "  AnxRegNei = BestAnxRegParams['n_neighbors']\n",
        "  AnxRegAlg = BestAnxRegParams['algorithm']\n",
        "  AnxRegWei = BestAnxRegParams['weights']\n",
        "\n",
        "  FastAnxRegMod = KNeighborsClassifier(n_neighbors=AnxRegNei, algorithm=AnxRegAlg, weights=AnxRegWei, random_state=randnum)\n",
        "\n",
        "  FinalAnxRegMod = BaggingRegressor(estimator=FastAnxRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxRegMod.fit(X_Regset, A_Regset)\n",
        "\n",
        "  ARegMod_Acc = FinalAnxRegMod.score(X_HoldRegset, A_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {ARegMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxRegModel'\n",
        "  joblib.dump(FinalAnxRegMod, file)\n",
        "\n",
        "elif BestMod == AModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxRegModel'\n",
        "  joblib.dump(AnxRegModel2, file)\n",
        "\n",
        "elif BestMod == AModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc3*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxRegModel'\n",
        "  joblib.dump(AnxRegModel3, file)"
      ],
      "metadata": {
        "id": "kuwFDHDcawXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "786f3912-6034-47d9-ad28-0441b7b4c362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 1.2 ms (2023-04-23T03:44:18/2023-04-23T03:44:18)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 40.72300334239149%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stress Model"
      ],
      "metadata": {
        "id": "R9Rfe78eVeZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = Regressiondf[targets[2]]\n",
        "\n",
        "X_Regset, X_HoldRegset, S_Regset, S_HoldRegset = train_test_split(X, S, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "DQ_GxFbXVhnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24775509-06bd-4aed-8b4b-a82b7e2a87fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 19.3 ms (2023-04-23T03:44:19/2023-04-23T03:44:19)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klj14ZxLZ_tr"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MJAvIVxaA1t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "02f46489-064f-441c-9eee-392789540870"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 44 s (2023-04-23T03:44:19/2023-04-23T03:45:03)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the KNReg Model is: 20.302601527794973%\n",
            "{'algorithm': 'ball_tree', 'n_neighbors': 50, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [25, 50, 75],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "StsRegMod1 = GridSearchCV(KNReg, KNParaReg)\n",
        "StsRegMod1.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc1 = StsRegMod1.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {SModel_Acc1*100}%')\n",
        "print(StsRegMod1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "KWboEvpua0yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[10,20,30],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [8,9,10],\n",
        "    'min_samples_split': [9,10,13,15],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "StsRegMod2 = GridSearchCV(RFReg, RFParaReg)\n",
        "StsRegMod2.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc2 = StsRegMod2.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {SModel_Acc2*100}%')\n",
        "print(StsRegMod2.best_params_)"
      ],
      "metadata": {
        "id": "Qx0X65ZVa0yx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9d9d8e2d-ca6d-419d-9042-6423655d2cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 47 min 20 s (2023-04-23T03:45:03/2023-04-23T04:32:23)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the RFReg Model is: 45.45698100283987%\n",
            "{'criterion': 'squared_error', 'max_depth': 8, 'min_samples_split': 15, 'n_estimators': 30, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "PBszTAQs6kie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error', 'poisson', 'quantile'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "StsRegMod3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "StsRegMod3.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc3 = StsRegMod3.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc3*100}%')\n",
        "print(StsRegMod3.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5FwYsPkJ6kie",
        "outputId": "273319e8-d8bb-4f96-b967-af32da56d961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 14 min 48 s (2023-04-23T04:32:23/2023-04-23T04:47:11)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "300 fits failed out of a total of 1200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "300 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 407, in fit\n",
            "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 1524, in _get_loss\n",
            "    return _LOSSES[self.loss](\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/_loss/loss.py\", line 610, in __init__\n",
            "    check_scalar(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\", line 1498, in check_scalar\n",
            "    raise TypeError(\n",
            "TypeError: quantile must be an instance of float, not NoneType.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [ 0.45015599  0.45130857  0.45062095  0.44989074  0.44437856  0.43998875\n",
            "  0.43323765  0.42662626  0.43557675  0.43269819  0.42198039  0.41212154\n",
            "  0.4432582   0.44457842  0.44450279  0.44357474  0.44312541  0.44124711\n",
            "  0.43797545  0.43560832  0.43805959  0.43498139  0.43011792  0.42771364\n",
            "  0.44852283  0.4490339   0.44791035  0.4477651   0.44191575  0.43784761\n",
            "  0.4287081   0.41950368  0.43660148  0.43073307  0.4191238   0.41055618\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "  0.4462024   0.44534431  0.44085791  0.43638215  0.40575774  0.39110145\n",
            "  0.37810505  0.3557117   0.3829486   0.36887159  0.34563604  0.32583086\n",
            "  0.44253589  0.44149373  0.43973229  0.4377349   0.41762463  0.41392931\n",
            "  0.40803849  0.40454366  0.40510784  0.40244608  0.39514688  0.39005767\n",
            "  0.44559477  0.44279221  0.43695054  0.43278427  0.40917352  0.39614477\n",
            "  0.37264432  0.35547316  0.37261205  0.35652588  0.3303015   0.31436396\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "  0.43632446  0.43363059  0.42888456  0.42369211  0.35102065  0.33240315\n",
            "  0.30425127  0.28184896  0.31632074  0.2949949   0.26709165  0.24527473\n",
            "  0.43454748  0.4345697   0.43019645  0.4281941   0.39504017  0.39018267\n",
            "  0.38372302  0.38002597  0.37708719  0.37317577  0.36537839  0.36411234\n",
            "  0.43542082  0.43247291  0.42431395  0.41984539  0.3435447   0.32321069\n",
            "  0.28676511  0.2633146   0.30089278  0.28095676  0.24542004  0.2224313\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "  0.42829334  0.42244904  0.41741658  0.41095887  0.27987552  0.2574973\n",
            "  0.22652843  0.20301816  0.23129619  0.20723111  0.17617868  0.15805428\n",
            "  0.42428259  0.42251294  0.4194301   0.4192152   0.36526431  0.35740479\n",
            "  0.34832902  0.34128285  0.33280528  0.32522848  0.31608299  0.31166618\n",
            "  0.42402294  0.41623594  0.40967262  0.39659723  0.26074005  0.23097998\n",
            "  0.18974033  0.15055213  0.22048077  0.18410243  0.13910486  0.11727423\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "  0.41503798  0.4092003   0.40082418  0.39207451  0.20245767  0.16125341\n",
            "  0.11912514  0.09033108  0.14912799  0.12065614  0.08767426  0.06853777\n",
            "  0.41379478  0.41146071  0.40652999  0.40613953  0.31285423  0.30345045\n",
            "  0.29574735  0.28773258  0.28692495  0.27870666  0.26887188  0.26368435\n",
            "  0.40358373  0.39583333  0.38078923  0.3732933   0.17234645  0.13427522\n",
            "  0.07016531  0.01464653  0.10495727  0.07144703  0.03067424 -0.00422906\n",
            "         nan         nan         nan         nan         nan         nan\n",
            "         nan         nan         nan         nan         nan         nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 46.10166966758875%\n",
            "{'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 2, 'max_iter': 100, 'random_state': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bizRVBha0yx"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "dK9-G7Zoa0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [SModel_Acc1, SModel_Acc2, SModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == SModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestStsRegParams = StsRegMod1.best_params_\n",
        "\n",
        "  StsRegNei = BestStsRegParams['n_neighbors']\n",
        "  StsRegAlg = BestStsRegParams['algorithm']\n",
        "  StsRegWei = BestStsRegParams['weights']\n",
        "\n",
        "  FastStsRegMod = KNeighborsRegressor(n_neighbors=StsRegNei, algorithm=StsRegAlg, weights=StsRegWei, random_state=randnum)\n",
        "\n",
        "  FinalStsRegMod = BaggingRegressor(estimator=FastStsRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsRegMod.fit(X_Regset, S_Regset)\n",
        "\n",
        "  SRegMod_Acc = FinalStsRegMod.score(X_HoldRegset, S_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {SRegMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsRegModel'\n",
        "  joblib.dump(FinalStsRegMod, file)\n",
        "\n",
        "elif BestMod == SModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsRegModel'\n",
        "  joblib.dump(StsRegMod2, file)\n",
        "\n",
        "elif BestMod == SModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc3*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsRegModel'\n",
        "  joblib.dump(StsRegMod3, file)"
      ],
      "metadata": {
        "id": "I1KPVntxa0yx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "97b6b8d3-e1b9-47f0-981b-3c82859505a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 9.13 ms (2023-04-23T04:47:11/2023-04-23T04:47:11)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 46.10166966758875%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clssification Approach"
      ],
      "metadata": {
        "id": "R9ML79I2ZZ9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classdf = Modeldf\n",
        "#Changes the target scores to match the target classes for Classification.\n",
        "#Grabs the location of the target columns.\n",
        "Dloc = Classdf.columns.get_loc('Depression_Score')\n",
        "Aloc = Classdf.columns.get_loc('Anxiety_Score')\n",
        "Sloc = Classdf.columns.get_loc('Stress_Score')\n",
        "\n",
        "for i in range(len(Classdf)):\n",
        "  #Depression.\n",
        "  if Classdf.iat[i,Dloc] <= 9:\n",
        "    Classdf.iat[i,Dloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 13:\n",
        "    Classdf.iat[i,Dloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 20:\n",
        "    Classdf.iat[i,Dloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 27:\n",
        "    Classdf.iat[i,Dloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] >= 28:\n",
        "    Classdf.iat[i,Dloc] = 4\n",
        "\n",
        "  #Anxeity.\n",
        "  if Classdf.iat[i,Aloc] <= 7:\n",
        "    Classdf.iat[i,Aloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 9:\n",
        "    Classdf.iat[i,Aloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 14:\n",
        "    Classdf.iat[i,Aloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 19:\n",
        "    Classdf.iat[i,Aloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] >= 20:\n",
        "    Classdf.iat[i,Aloc] = 4\n",
        "\n",
        "  #Stress.\n",
        "  if Classdf.iat[i,Sloc] <= 14:\n",
        "    Classdf.iat[i,Sloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 18:\n",
        "    Classdf.iat[i,Sloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 25:\n",
        "    Classdf.iat[i,Sloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 33:\n",
        "    Classdf.iat[i,Sloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] >= 34:\n",
        "    Classdf.iat[i,Sloc] = 4"
      ],
      "metadata": {
        "id": "hTvg1NNdfCvr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a85462e1-919e-466f-f71d-4a5368501e1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 3.44 s (2023-04-23T18:45:38/2023-04-23T18:45:41)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Examining the Data before Classification"
      ],
      "metadata": {
        "id": "9U6PAiVncUZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Data is checked and verified that all preprocessing steps executed correctly."
      ],
      "metadata": {
        "id": "OXxiSh4U2Dgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(Classdf[['Depression_Score']] == 0))\n",
        "print(np.sum(Classdf[['Depression_Score']] == 1))\n",
        "print(np.sum(Classdf[['Depression_Score']] == 2))\n",
        "print(np.sum(Classdf[['Depression_Score']] == 3))\n",
        "print(np.sum(Classdf[['Depression_Score']] == 4))\n",
        "\n",
        "display(Classdf.describe(include='all'))\n",
        "ut.pprint(Classdf)"
      ],
      "metadata": {
        "id": "kltiyKXQcT6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "outputId": "361dbf91-3934-442d-ca19-63e03c57f58e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 222 ms (2023-04-23T18:45:41/2023-04-23T18:45:41)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depression_Score    1736\n",
            "dtype: int64\n",
            "Depression_Score    678\n",
            "dtype: int64\n",
            "Depression_Score    1386\n",
            "dtype: int64\n",
            "Depression_Score    1287\n",
            "dtype: int64\n",
            "Depression_Score    3120\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Depression_Score  Anxiety_Score  Stress_Score    No_Degree   \n",
              "count            8207.0         8207.0        8207.0  8207.000000  \\\n",
              "mean           2.411478        2.23943      1.915438     0.270135   \n",
              "std            1.561145       1.625208      1.444957     0.444057   \n",
              "min                 0.0            0.0           0.0     0.000000   \n",
              "25%                 1.0            0.0           0.0     0.000000   \n",
              "50%                 3.0            2.0           2.0     0.000000   \n",
              "75%                 4.0            4.0           3.0     1.000000   \n",
              "max                 4.0            4.0           4.0     1.000000   \n",
              "\n",
              "        HighSchool   University     Graduate         Rual     Suburban   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.430974     0.202876     0.083343     0.193493     0.458145   \n",
              "std       0.495243     0.402165     0.276418     0.395060     0.498275   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "             Urban         Male       Female  Other_Gender       Engnat   \n",
              "count  8207.000000  8207.000000  8207.000000   8207.000000  8207.000000  \\\n",
              "mean      0.339344     0.291946     0.666870      0.038869     0.909955   \n",
              "std       0.473516     0.454685     0.471361      0.193295     0.286264   \n",
              "min       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000      0.000000     1.000000   \n",
              "50%       0.000000     0.000000     1.000000      0.000000     1.000000   \n",
              "75%       1.000000     1.000000     1.000000      0.000000     1.000000   \n",
              "max       1.000000     1.000000     1.000000      1.000000     1.000000   \n",
              "\n",
              "           Right_h       Left_h  Ambidextrous     Agnostic      Atheist   \n",
              "count  8207.000000  8207.000000   8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.859023     0.101133      0.036067     0.197027     0.191178   \n",
              "std       0.348019     0.301523      0.186468     0.397777     0.393253   \n",
              "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "25%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "50%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "75%       1.000000     0.000000      0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000      1.000000     1.000000     1.000000   \n",
              "\n",
              "          Buddhist     Catholic       Mormon  Protestant  Other_Christian   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.00000      8207.000000  \\\n",
              "mean      0.013038     0.144511     0.010966     0.08968         0.171073   \n",
              "std       0.113443     0.351628     0.104150     0.28574         0.376596   \n",
              "min       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.00000         0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.00000         1.000000   \n",
              "\n",
              "             Hindu       Jewish       Muslim         Sikh  Other_Religion   \n",
              "count  8207.000000  8207.000000  8207.000000  8207.000000     8207.000000  \\\n",
              "mean      0.005239     0.013525     0.024248     0.001218        0.120141   \n",
              "std       0.072198     0.115515     0.153826     0.034887        0.325147   \n",
              "min       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000        1.000000   \n",
              "\n",
              "       Heterosexual     Bisexual   Homosexual     Asexual  Other_Orientation   \n",
              "count   8207.000000  8207.000000  8207.000000  8207.00000        8207.000000  \\\n",
              "mean       0.610454     0.204947     0.061411     0.04155           0.061533   \n",
              "std        0.487677     0.403687     0.240097     0.19957           0.240320   \n",
              "min        0.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "25%        0.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "50%        1.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "75%        1.000000     0.000000     0.000000     0.00000           0.000000   \n",
              "max        1.000000     1.000000     1.000000     1.00000           1.000000   \n",
              "\n",
              "             Asian         Arab        Black  Indigenous_Australian   \n",
              "count  8207.000000  8207.000000  8207.000000            8207.000000  \\\n",
              "mean      0.086755     0.004386     0.046424               0.000122   \n",
              "std       0.281493     0.066089     0.210414               0.011038   \n",
              "min       0.000000     0.000000     0.000000               0.000000   \n",
              "25%       0.000000     0.000000     0.000000               0.000000   \n",
              "50%       0.000000     0.000000     0.000000               0.000000   \n",
              "75%       0.000000     0.000000     0.000000               0.000000   \n",
              "max       1.000000     1.000000     1.000000               1.000000   \n",
              "\n",
              "       Native_American        White   Other_Race        Voted  Never_married   \n",
              "count      8207.000000  8207.000000  8207.000000  8207.000000    8207.000000  \\\n",
              "mean          0.017546     0.717193     0.127574     0.295601       0.783234   \n",
              "std           0.131302     0.450391     0.333635     0.456340       0.412067   \n",
              "min           0.000000     0.000000     0.000000     0.000000       0.000000   \n",
              "25%           0.000000     0.000000     0.000000     0.000000       1.000000   \n",
              "50%           0.000000     1.000000     0.000000     0.000000       1.000000   \n",
              "75%           0.000000     1.000000     0.000000     1.000000       1.000000   \n",
              "max           1.000000     1.000000     1.000000     1.000000       1.000000   \n",
              "\n",
              "           Married  Previously_married          Age   FamilySize   \n",
              "count  8207.000000         8207.000000  8207.000000  8207.000000  \\\n",
              "mean      0.149872            0.062020     0.135942     0.020582   \n",
              "std       0.356968            0.241207     0.144729     0.016407   \n",
              "min       0.000000            0.000000     0.000000     0.000000   \n",
              "25%       0.000000            0.000000     0.034884     0.015038   \n",
              "50%       0.000000            0.000000     0.081395     0.015038   \n",
              "75%       0.000000            0.000000     0.174419     0.022556   \n",
              "max       1.000000            1.000000     1.000000     1.000000   \n",
              "\n",
              "       Extraversion  Agreeableness  Conscientiousness  Emotional_Stability   \n",
              "count   8207.000000    8207.000000        8207.000000          8207.000000  \\\n",
              "mean       0.449416       0.650412           0.617261             0.431165   \n",
              "std        0.242194       0.201655           0.233309             0.227893   \n",
              "min        0.000000       0.000000           0.000000             0.000000   \n",
              "25%        0.214286       0.500000           0.428571             0.285714   \n",
              "50%        0.428571       0.642857           0.642857             0.357143   \n",
              "75%        0.642857       0.785714           0.785714             0.571429   \n",
              "max        1.000000       1.000000           1.000000             1.000000   \n",
              "\n",
              "          Openness  \n",
              "count  8207.000000  \n",
              "mean      0.687096  \n",
              "std       0.207827  \n",
              "min       0.000000  \n",
              "25%       0.571429  \n",
              "50%       0.714286  \n",
              "75%       0.857143  \n",
              "max       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8663c583-3ad0-4bdc-8a39-6e49fbb3fbe3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Depression_Score</th>\n",
              "      <th>Anxiety_Score</th>\n",
              "      <th>Stress_Score</th>\n",
              "      <th>No_Degree</th>\n",
              "      <th>HighSchool</th>\n",
              "      <th>University</th>\n",
              "      <th>Graduate</th>\n",
              "      <th>Rual</th>\n",
              "      <th>Suburban</th>\n",
              "      <th>Urban</th>\n",
              "      <th>Male</th>\n",
              "      <th>Female</th>\n",
              "      <th>Other_Gender</th>\n",
              "      <th>Engnat</th>\n",
              "      <th>Right_h</th>\n",
              "      <th>Left_h</th>\n",
              "      <th>Ambidextrous</th>\n",
              "      <th>Agnostic</th>\n",
              "      <th>Atheist</th>\n",
              "      <th>Buddhist</th>\n",
              "      <th>Catholic</th>\n",
              "      <th>Mormon</th>\n",
              "      <th>Protestant</th>\n",
              "      <th>Other_Christian</th>\n",
              "      <th>Hindu</th>\n",
              "      <th>Jewish</th>\n",
              "      <th>Muslim</th>\n",
              "      <th>Sikh</th>\n",
              "      <th>Other_Religion</th>\n",
              "      <th>Heterosexual</th>\n",
              "      <th>Bisexual</th>\n",
              "      <th>Homosexual</th>\n",
              "      <th>Asexual</th>\n",
              "      <th>Other_Orientation</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Arab</th>\n",
              "      <th>Black</th>\n",
              "      <th>Indigenous_Australian</th>\n",
              "      <th>Native_American</th>\n",
              "      <th>White</th>\n",
              "      <th>Other_Race</th>\n",
              "      <th>Voted</th>\n",
              "      <th>Never_married</th>\n",
              "      <th>Married</th>\n",
              "      <th>Previously_married</th>\n",
              "      <th>Age</th>\n",
              "      <th>FamilySize</th>\n",
              "      <th>Extraversion</th>\n",
              "      <th>Agreeableness</th>\n",
              "      <th>Conscientiousness</th>\n",
              "      <th>Emotional_Stability</th>\n",
              "      <th>Openness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.0</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.00000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.00000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "      <td>8207.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.411478</td>\n",
              "      <td>2.23943</td>\n",
              "      <td>1.915438</td>\n",
              "      <td>0.270135</td>\n",
              "      <td>0.430974</td>\n",
              "      <td>0.202876</td>\n",
              "      <td>0.083343</td>\n",
              "      <td>0.193493</td>\n",
              "      <td>0.458145</td>\n",
              "      <td>0.339344</td>\n",
              "      <td>0.291946</td>\n",
              "      <td>0.666870</td>\n",
              "      <td>0.038869</td>\n",
              "      <td>0.909955</td>\n",
              "      <td>0.859023</td>\n",
              "      <td>0.101133</td>\n",
              "      <td>0.036067</td>\n",
              "      <td>0.197027</td>\n",
              "      <td>0.191178</td>\n",
              "      <td>0.013038</td>\n",
              "      <td>0.144511</td>\n",
              "      <td>0.010966</td>\n",
              "      <td>0.08968</td>\n",
              "      <td>0.171073</td>\n",
              "      <td>0.005239</td>\n",
              "      <td>0.013525</td>\n",
              "      <td>0.024248</td>\n",
              "      <td>0.001218</td>\n",
              "      <td>0.120141</td>\n",
              "      <td>0.610454</td>\n",
              "      <td>0.204947</td>\n",
              "      <td>0.061411</td>\n",
              "      <td>0.04155</td>\n",
              "      <td>0.061533</td>\n",
              "      <td>0.086755</td>\n",
              "      <td>0.004386</td>\n",
              "      <td>0.046424</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.017546</td>\n",
              "      <td>0.717193</td>\n",
              "      <td>0.127574</td>\n",
              "      <td>0.295601</td>\n",
              "      <td>0.783234</td>\n",
              "      <td>0.149872</td>\n",
              "      <td>0.062020</td>\n",
              "      <td>0.135942</td>\n",
              "      <td>0.020582</td>\n",
              "      <td>0.449416</td>\n",
              "      <td>0.650412</td>\n",
              "      <td>0.617261</td>\n",
              "      <td>0.431165</td>\n",
              "      <td>0.687096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.561145</td>\n",
              "      <td>1.625208</td>\n",
              "      <td>1.444957</td>\n",
              "      <td>0.444057</td>\n",
              "      <td>0.495243</td>\n",
              "      <td>0.402165</td>\n",
              "      <td>0.276418</td>\n",
              "      <td>0.395060</td>\n",
              "      <td>0.498275</td>\n",
              "      <td>0.473516</td>\n",
              "      <td>0.454685</td>\n",
              "      <td>0.471361</td>\n",
              "      <td>0.193295</td>\n",
              "      <td>0.286264</td>\n",
              "      <td>0.348019</td>\n",
              "      <td>0.301523</td>\n",
              "      <td>0.186468</td>\n",
              "      <td>0.397777</td>\n",
              "      <td>0.393253</td>\n",
              "      <td>0.113443</td>\n",
              "      <td>0.351628</td>\n",
              "      <td>0.104150</td>\n",
              "      <td>0.28574</td>\n",
              "      <td>0.376596</td>\n",
              "      <td>0.072198</td>\n",
              "      <td>0.115515</td>\n",
              "      <td>0.153826</td>\n",
              "      <td>0.034887</td>\n",
              "      <td>0.325147</td>\n",
              "      <td>0.487677</td>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.240097</td>\n",
              "      <td>0.19957</td>\n",
              "      <td>0.240320</td>\n",
              "      <td>0.281493</td>\n",
              "      <td>0.066089</td>\n",
              "      <td>0.210414</td>\n",
              "      <td>0.011038</td>\n",
              "      <td>0.131302</td>\n",
              "      <td>0.450391</td>\n",
              "      <td>0.333635</td>\n",
              "      <td>0.456340</td>\n",
              "      <td>0.412067</td>\n",
              "      <td>0.356968</td>\n",
              "      <td>0.241207</td>\n",
              "      <td>0.144729</td>\n",
              "      <td>0.016407</td>\n",
              "      <td>0.242194</td>\n",
              "      <td>0.201655</td>\n",
              "      <td>0.233309</td>\n",
              "      <td>0.227893</td>\n",
              "      <td>0.207827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174419</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8663c583-3ad0-4bdc-8a39-6e49fbb3fbe3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8663c583-3ad0-4bdc-8a39-6e49fbb3fbe3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8663c583-3ad0-4bdc-8a39-6e49fbb3fbe3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Depression_Score  Anxiety_Score  Stress_Score  No_Degree  HighSchool   \n",
              "index                                                                         \n",
              "0                     1              0             0          0           1  \\\n",
              "1                     0              0             0          0           0   \n",
              "2                     2              0             0          0           1   \n",
              "3                     0              0             0          0           1   \n",
              "4                     0              0             0          0           1   \n",
              "...                 ...            ...           ...        ...         ...   \n",
              "8202                  4              4             4          0           0   \n",
              "8203                  4              4             3          1           0   \n",
              "8204                  4              4             4          0           0   \n",
              "8205                  4              4             4          1           0   \n",
              "8206                  2              4             3          1           0   \n",
              "\n",
              "       University  Graduate  Rual  Suburban  Urban  Male  Female   \n",
              "index                                                              \n",
              "0               0         0     0         0      1     0       1  \\\n",
              "1               1         0     0         1      0     1       0   \n",
              "2               0         0     1         0      0     1       0   \n",
              "3               0         0     0         0      1     0       1   \n",
              "4               0         0     0         1      0     1       0   \n",
              "...           ...       ...   ...       ...    ...   ...     ...   \n",
              "8202            1         0     0         0      1     0       1   \n",
              "8203            0         0     0         0      1     0       0   \n",
              "8204            0         1     0         1      0     0       1   \n",
              "8205            0         0     0         0      1     0       1   \n",
              "8206            0         0     0         1      0     0       1   \n",
              "\n",
              "       Other_Gender  Engnat  Right_h  Left_h  Ambidextrous  Agnostic  Atheist   \n",
              "index                                                                           \n",
              "0                 0       0        1       0             0         0        0  \\\n",
              "1                 0       1        1       0             0         1        0   \n",
              "2                 0       1        0       0             1         0        1   \n",
              "3                 0       1        1       0             0         1        0   \n",
              "4                 0       1        1       0             0         0        0   \n",
              "...             ...     ...      ...     ...           ...       ...      ...   \n",
              "8202              0       1        1       0             0         0        0   \n",
              "8203              1       1        1       0             0         0        1   \n",
              "8204              0       1        1       0             0         0        0   \n",
              "8205              0       1        1       0             0         1        0   \n",
              "8206              0       1        1       0             0         1        0   \n",
              "\n",
              "       Buddhist  Catholic  Mormon  Protestant  Other_Christian  Hindu  Jewish   \n",
              "index                                                                           \n",
              "0             0         1       0           0                0      0       0  \\\n",
              "1             0         0       0           0                0      0       0   \n",
              "2             0         0       0           0                0      0       0   \n",
              "3             0         0       0           0                0      0       0   \n",
              "4             0         0       0           0                1      0       0   \n",
              "...         ...       ...     ...         ...              ...    ...     ...   \n",
              "8202          0         0       0           0                1      0       0   \n",
              "8203          0         0       0           0                0      0       0   \n",
              "8204          0         1       0           0                0      0       0   \n",
              "8205          0         0       0           0                0      0       0   \n",
              "8206          0         0       0           0                0      0       0   \n",
              "\n",
              "       Muslim  Sikh  Other_Religion  Heterosexual  Bisexual  Homosexual   \n",
              "index                                                                     \n",
              "0           0     0               0             1         0           0  \\\n",
              "1           0     0               0             1         0           0   \n",
              "2           0     0               0             1         0           0   \n",
              "3           0     0               0             0         0           0   \n",
              "4           0     0               0             1         0           0   \n",
              "...       ...   ...             ...           ...       ...         ...   \n",
              "8202        0     0               0             1         0           0   \n",
              "8203        0     0               0             0         0           0   \n",
              "8204        0     0               0             1         0           0   \n",
              "8205        0     0               0             0         0           0   \n",
              "8206        0     0               0             1         0           0   \n",
              "\n",
              "       Asexual  Other_Orientation  Asian  Arab  Black  Indigenous_Australian   \n",
              "index                                                                          \n",
              "0            0                  0      0     0      0                      0  \\\n",
              "1            0                  0      0     0      0                      0   \n",
              "2            0                  0      0     0      0                      0   \n",
              "3            0                  1      0     0      0                      0   \n",
              "4            0                  0      0     0      1                      0   \n",
              "...        ...                ...    ...   ...    ...                    ...   \n",
              "8202         0                  0      1     0      0                      0   \n",
              "8203         1                  0      0     0      0                      0   \n",
              "8204         0                  0      0     0      0                      0   \n",
              "8205         0                  1      0     0      0                      0   \n",
              "8206         0                  0      0     0      0                      0   \n",
              "\n",
              "       Native_American  White  Other_Race  Voted  Never_married  Married   \n",
              "index                                                                      \n",
              "0                    0      0           1      0              1        0  \\\n",
              "1                    0      1           0      1              0        0   \n",
              "2                    0      1           0      0              1        0   \n",
              "3                    0      0           1      0              1        0   \n",
              "4                    0      0           0      0              1        0   \n",
              "...                ...    ...         ...    ...            ...      ...   \n",
              "8202                 0      0           0      1              1        0   \n",
              "8203                 0      0           1      0              1        0   \n",
              "8204                 0      1           0      1              0        1   \n",
              "8205                 0      1           0      0              1        0   \n",
              "8206                 0      0           1      0              1        0   \n",
              "\n",
              "       Previously_married       Age  FamilySize  Extraversion  Agreeableness   \n",
              "index                                                                          \n",
              "0                       0  0.081395    0.030075      0.214286       1.000000  \\\n",
              "1                       1  0.244186    0.015038      0.428571       0.428571   \n",
              "2                       0  0.069767    0.007519      0.357143       0.357143   \n",
              "3                       0  0.034884    0.022556      0.428571       0.714286   \n",
              "4                       0  0.058140    0.015038      0.857143       0.571429   \n",
              "...                   ...       ...         ...           ...            ...   \n",
              "8202                    0  0.104651    0.022556      0.142857       0.642857   \n",
              "8203                    0  0.058140    0.015038      0.357143       0.571429   \n",
              "8204                    0  0.383721    0.045113      0.357143       0.428571   \n",
              "8205                    0  0.011628    0.022556      0.428571       0.428571   \n",
              "8206                    0  0.034884    0.022556      0.785714       0.571429   \n",
              "\n",
              "       Conscientiousness  Emotional_Stability  Openness  \n",
              "index                                                    \n",
              "0               0.857143             0.928571  0.571429  \n",
              "1               0.857143             1.000000  0.857143  \n",
              "2               0.571429             0.928571  0.857143  \n",
              "3               0.571429             0.928571  0.571429  \n",
              "4               0.857143             0.928571  0.928571  \n",
              "...                  ...                  ...       ...  \n",
              "8202            0.142857             0.357143  0.785714  \n",
              "8203            0.714286             0.142857  0.642857  \n",
              "8204            0.857143             0.214286  0.642857  \n",
              "8205            0.214286             0.142857  0.357143  \n",
              "8206            0.714286             0.571429  0.500000  \n",
              "\n",
              "[8207 rows x 52 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7e6cade-d004-4215-a25e-435915a832ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Depression_Score</th>\n",
              "      <th>Anxiety_Score</th>\n",
              "      <th>Stress_Score</th>\n",
              "      <th>No_Degree</th>\n",
              "      <th>HighSchool</th>\n",
              "      <th>University</th>\n",
              "      <th>Graduate</th>\n",
              "      <th>Rual</th>\n",
              "      <th>Suburban</th>\n",
              "      <th>Urban</th>\n",
              "      <th>Male</th>\n",
              "      <th>Female</th>\n",
              "      <th>Other_Gender</th>\n",
              "      <th>Engnat</th>\n",
              "      <th>Right_h</th>\n",
              "      <th>Left_h</th>\n",
              "      <th>Ambidextrous</th>\n",
              "      <th>Agnostic</th>\n",
              "      <th>Atheist</th>\n",
              "      <th>Buddhist</th>\n",
              "      <th>Catholic</th>\n",
              "      <th>Mormon</th>\n",
              "      <th>Protestant</th>\n",
              "      <th>Other_Christian</th>\n",
              "      <th>Hindu</th>\n",
              "      <th>Jewish</th>\n",
              "      <th>Muslim</th>\n",
              "      <th>Sikh</th>\n",
              "      <th>Other_Religion</th>\n",
              "      <th>Heterosexual</th>\n",
              "      <th>Bisexual</th>\n",
              "      <th>Homosexual</th>\n",
              "      <th>Asexual</th>\n",
              "      <th>Other_Orientation</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Arab</th>\n",
              "      <th>Black</th>\n",
              "      <th>Indigenous_Australian</th>\n",
              "      <th>Native_American</th>\n",
              "      <th>White</th>\n",
              "      <th>Other_Race</th>\n",
              "      <th>Voted</th>\n",
              "      <th>Never_married</th>\n",
              "      <th>Married</th>\n",
              "      <th>Previously_married</th>\n",
              "      <th>Age</th>\n",
              "      <th>FamilySize</th>\n",
              "      <th>Extraversion</th>\n",
              "      <th>Agreeableness</th>\n",
              "      <th>Conscientiousness</th>\n",
              "      <th>Emotional_Stability</th>\n",
              "      <th>Openness</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.244186</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.104651</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8203</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058140</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8204</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.383721</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8205</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011628</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8206</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.034884</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8207 rows  52 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7e6cade-d004-4215-a25e-435915a832ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7e6cade-d004-4215-a25e-435915a832ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7e6cade-d004-4215-a25e-435915a832ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzMSY3rX1KeF"
      },
      "source": [
        "###Depression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ySi2uJX3iZ"
      },
      "source": [
        "####Data Split for Depression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NYWPRGeO2Aob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0231a25e-c29c-4726-e3d9-4cb976d75047"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 26.1 ms (2023-04-23T18:12:20/2023-04-23T18:12:20)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "X = Classdf[features]\n",
        "\n",
        "#Depression Model.\n",
        "D = Classdf[targets[0]]\n",
        "\n",
        "X_Claset, X_HoldClaset, D_Claset, D_HoldClaset = train_test_split(X, D, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRwfTEhjYD21"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkXfcitVX9wM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "2726c106-200c-438b-a273-4082be52f23e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 52.1 s (2023-04-23T04:47:14/2023-04-23T04:48:06)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the KNClass Model is: 42.34673162809582%\n",
            "{'algorithm': 'auto', 'n_neighbors': 100, 'weights': 'distance'}\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              No Depression       0.45      0.39      0.41       524\n",
            "            Mild Depression       0.00      0.00      0.00       227\n",
            "        Moderate Depression       0.22      0.03      0.05       409\n",
            "          Severe Depression       0.20      0.00      0.01       389\n",
            "Extremely Severe Depression       0.42      0.91      0.58       914\n",
            "\n",
            "                   accuracy                           0.42      2463\n",
            "                  macro avg       0.26      0.26      0.21      2463\n",
            "               weighted avg       0.32      0.42      0.31      2463\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "KNParaClass = {\n",
        "    'n_neighbors': [75, 100, 125],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNClass = KNeighborsClassifier()\n",
        "DepClaMod1 = GridSearchCV(KNClass, KNParaClass)\n",
        "DepClaMod1.fit(X_Claset, D_Claset)\n",
        "\n",
        "DModel_Acc1 = DepClaMod1.score(X_HoldClaset, D_HoldClaset)\n",
        "print(f'The Accuracy of the KNClass Model is: {DModel_Acc1*100}%')\n",
        "print(DepClaMod1.best_params_)\n",
        "\n",
        "DMod1Pred = DepClaMod1.predict(X_HoldClaset)\n",
        "print(classification_report(D_HoldClaset, DMod1Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D_HoldClaset, DMod1Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_85cpOTvYP5p"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYazB2REYQBt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "753f073d-ff64-4ca2-f80a-5e1edeb72edc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 4 min 48 s (2023-04-23T04:48:06/2023-04-23T04:52:55)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the Random Forest Model is: 46.691027202598455%\n",
            "{'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 7, 'n_estimators': 325, 'random_state': 500}\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              No Depression       0.47      0.62      0.54       524\n",
            "            Mild Depression       0.00      0.00      0.00       227\n",
            "        Moderate Depression       0.47      0.04      0.08       409\n",
            "          Severe Depression       0.67      0.01      0.01       389\n",
            "Extremely Severe Depression       0.46      0.88      0.61       914\n",
            "\n",
            "                   accuracy                           0.47      2463\n",
            "                  macro avg       0.42      0.31      0.25      2463\n",
            "               weighted avg       0.46      0.47      0.35      2463\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "RandForParaClass = {\n",
        "    'n_estimators': [275, 300, 325], \n",
        "    'criterion': ['gini'], \n",
        "    'max_depth': [10,11,12], \n",
        "    'min_samples_split': [7,8,9], \n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClass = RandomForestClassifier()\n",
        "DepClaMod2 = GridSearchCV(RandForClass, RandForParaClass)\n",
        "DepClaMod2.fit(X_Claset, D_Claset)\n",
        "\n",
        "DModel_Acc2 = DepClaMod2.score(X_HoldClaset, D_HoldClaset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "print(DepClaMod2.best_params_)\n",
        "\n",
        "DMod2Pred = DepClaMod2.predict(X_HoldClaset)\n",
        "print(classification_report(D_HoldClaset, DMod2Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D_HoldClaset, DMod2Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULKzJo2mYQMF"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv29r2lvYQSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "624eb6c1-128e-47a4-d773-e13df7b41424"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 967 s (2023-04-23T04:52:55/2023-04-23T04:52:55)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "    \n",
        "DepClaMod3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "DepClaMod3.fit(X_Claset, D_Claset)\n",
        "\n",
        "DModel_Acc3 = DepClaMod3.score(X_HoldClaset, D_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {DModel_Acc3*100}%')\n",
        "print(DepClaMod3.best_params_)\n",
        "\n",
        "DMod3Pred = DepClaMod3.predict(X_HoldClaset)\n",
        "print(classification_report(D_HoldClaset, DMod3Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D_HoldClaset, DMod3Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NCjepfHMgWVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "3aa2b591-eca5-47ce-febf-65e4e2b5509a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 2 min 7 s (2023-04-23T04:52:55/2023-04-23T04:55:02)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the SVC Model is: 46.20381648396265%\n",
            "{'C': 1.0, 'kernel': 'linear'}\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              No Depression       0.46      0.64      0.54       524\n",
            "            Mild Depression       0.00      0.00      0.00       227\n",
            "        Moderate Depression       1.00      0.00      0.00       409\n",
            "          Severe Depression       0.21      0.01      0.02       389\n",
            "Extremely Severe Depression       0.47      0.87      0.61       914\n",
            "\n",
            "                   accuracy                           0.46      2463\n",
            "                  macro avg       0.43      0.31      0.23      2463\n",
            "               weighted avg       0.47      0.46      0.34      2463\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3,4,5,6],\n",
        "    'C':C,\n",
        "    }\n",
        "    \n",
        "DepClaMod4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "DepClaMod4.fit(X_Claset, D_Claset)\n",
        "\n",
        "DModel_Acc4 = DepClaMod4.score(X_HoldClaset, D_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {DModel_Acc4*100}%')\n",
        "print(DepClaMod4.best_params_)\n",
        "\n",
        "DMod4Pred = DepClaMod4.predict(X_HoldClaset)\n",
        "print(classification_report(D_HoldClaset, DMod4Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D_HoldClaset, DMod4Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AJK-jh1EkNs2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "4df14593-cb58-4f35-ee29-583b2f0f1215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 10 min 22 s (2023-04-23T04:55:02/2023-04-23T05:05:25)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the SVC Model is: 46.041412911084045%\n",
            "{'C': 0.9, 'degree': 2, 'kernel': 'poly'}\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              No Depression       0.48      0.60      0.54       524\n",
            "            Mild Depression       0.00      0.00      0.00       227\n",
            "        Moderate Depression       0.22      0.03      0.06       409\n",
            "          Severe Depression       0.00      0.00      0.00       389\n",
            "Extremely Severe Depression       0.46      0.88      0.61       914\n",
            "\n",
            "                   accuracy                           0.46      2463\n",
            "                  macro avg       0.23      0.30      0.24      2463\n",
            "               weighted avg       0.31      0.46      0.35      2463\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "GXdcfytm7ERR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss'],\n",
        "    'learning_rate':[0.1,0.3,0.5],\n",
        "    'max_iter':[75,100,125],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "DepClaMod5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "DepClaMod5.fit(X_Claset, D_Claset)\n",
        "\n",
        "DModel_Acc5 = DepClaMod5.score(X_HoldClaset, D_HoldClaset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc5*100}%')\n",
        "print(DepClaMod5.best_params_)\n",
        "\n",
        "DMod5Pred = DepClaMod5.predict(X_HoldClaset)\n",
        "print(classification_report(D_HoldClaset, DMod5Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D_HoldClaset, DMod5Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2VWanf8n7ERR",
        "outputId": "ddc80af0-5052-423f-b2fa-662c3da37218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 1 h 3 min 5 s (2023-04-23T05:05:25/2023-04-23T06:08:30)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'auto' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'binary_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1998: FutureWarning: The loss 'categorical_crossentropy' was deprecated in v1.1 and will be removed in version 1.3. Use 'log_loss' which is equivalent.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "300 fits failed out of a total of 1200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "300 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 407, in fit\n",
            "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 2023, in _get_loss\n",
            "    raise ValueError(\n",
            "ValueError: loss='binary_crossentropy' is not defined for multiclass classification with n_classes=5, use loss='log_loss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.47980307 0.48102107 0.47719074 0.47405728 0.46413227 0.4609979\n",
            " 0.45455691 0.45003169 0.45403365 0.45264099 0.4437634  0.43732271\n",
            " 0.47980307 0.48102107 0.47719074 0.47405728 0.46413227 0.4609979\n",
            " 0.45455691 0.45003169 0.45403365 0.45264099 0.4437634  0.43732271\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.47980307 0.48102107 0.47719074 0.47405728 0.46413227 0.4609979\n",
            " 0.45455691 0.45003169 0.45403365 0.45264099 0.4437634  0.43732271\n",
            " 0.46953084 0.46465598 0.45925892 0.45786641 0.43210017 0.43192702\n",
            " 0.42687855 0.42322092 0.42844315 0.42757298 0.42896762 0.42722667\n",
            " 0.46953084 0.46465598 0.45925892 0.45786641 0.43210017 0.43192702\n",
            " 0.42687855 0.42322092 0.42844315 0.42757298 0.42896762 0.42722667\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.46953084 0.46465598 0.45925892 0.45786641 0.43210017 0.43192702\n",
            " 0.42687855 0.42322092 0.42844315 0.42757298 0.42896762 0.42722667\n",
            " 0.45438406 0.45299078 0.44620076 0.43888747 0.41590794 0.40911837\n",
            " 0.4145177  0.41399429 0.41434257 0.41695369 0.42269812 0.42426531\n",
            " 0.45438406 0.45299078 0.44620076 0.43888747 0.41590794 0.40911837\n",
            " 0.4145177  0.41399429 0.41434257 0.41695369 0.42269812 0.42426531\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.45438406 0.45299078 0.44620076 0.43888747 0.41590794 0.40911837\n",
            " 0.4145177  0.41399429 0.41434257 0.41695369 0.42269812 0.42426531\n",
            " 0.43278597 0.43156752 0.4228643  0.41242044 0.40842438 0.4068578\n",
            " 0.4014609  0.40302748 0.38092024 0.38335774 0.38283449 0.38492508\n",
            " 0.43278597 0.43156752 0.4228643  0.41242044 0.40842438 0.4068578\n",
            " 0.4014609  0.40302748 0.38092024 0.38335774 0.38283449 0.38492508\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.43278597 0.43156752 0.4228643  0.41242044 0.40842438 0.4068578\n",
            " 0.4014609  0.40302748 0.38092024 0.38335774 0.38283449 0.38492508\n",
            " 0.35235866 0.34557015 0.31423856 0.31284604 0.33165304 0.33130491\n",
            " 0.32956426 0.33113084 0.33947578 0.33947578 0.33947578 0.33947578\n",
            " 0.35235866 0.34557015 0.31423856 0.31284604 0.33165304 0.33130491\n",
            " 0.32956426 0.33113084 0.33947578 0.33947578 0.33947578 0.33947578\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.35235866 0.34557015 0.31423856 0.31284604 0.33165304 0.33130491\n",
            " 0.32956426 0.33113084 0.33947578 0.33947578 0.33947578 0.33947578]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of the HistGradientBoosting Model is: 45.87900933820544%\n",
            "{'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 2, 'max_iter': 100, 'random_state': 500}\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "              No Depression       0.46      0.61      0.52       524\n",
            "            Mild Depression       0.25      0.00      0.01       227\n",
            "        Moderate Depression       0.26      0.11      0.15       409\n",
            "          Severe Depression       0.17      0.01      0.02       389\n",
            "Extremely Severe Depression       0.49      0.83      0.61       914\n",
            "\n",
            "                   accuracy                           0.46      2463\n",
            "                  macro avg       0.32      0.31      0.26      2463\n",
            "               weighted avg       0.37      0.46      0.37      2463\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDsCVpWwYf6j"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5Kz7UEgs8an7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c3f53d8-fb37-4f26-be83-293ff4d5674a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre> 19 s (2023-04-23T18:13:35/2023-04-23T18:13:54)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "46/46 [==============================] - 3s 31ms/step - loss: 1.5792 - accuracy: 0.2986 - val_loss: 1.4953 - val_accuracy: 0.3890\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.5026 - accuracy: 0.3761 - val_loss: 1.4643 - val_accuracy: 0.4160\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.4749 - accuracy: 0.3919 - val_loss: 1.4449 - val_accuracy: 0.4256\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.4556 - accuracy: 0.4033 - val_loss: 1.4298 - val_accuracy: 0.4334\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.4385 - accuracy: 0.4111 - val_loss: 1.4165 - val_accuracy: 0.4413\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 1.4291 - accuracy: 0.4226 - val_loss: 1.4054 - val_accuracy: 0.4465\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 1.4181 - accuracy: 0.4274 - val_loss: 1.3962 - val_accuracy: 0.4552\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.4091 - accuracy: 0.4309 - val_loss: 1.3869 - val_accuracy: 0.4587\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 1.3942 - accuracy: 0.4390 - val_loss: 1.3772 - val_accuracy: 0.4648\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 1.3878 - accuracy: 0.4444 - val_loss: 1.3687 - val_accuracy: 0.4656\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3735 - accuracy: 0.4518 - val_loss: 1.3612 - val_accuracy: 0.4682\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3686 - accuracy: 0.4514 - val_loss: 1.3533 - val_accuracy: 0.4813\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.3625 - accuracy: 0.4538 - val_loss: 1.3484 - val_accuracy: 0.4848\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.3588 - accuracy: 0.4564 - val_loss: 1.3425 - val_accuracy: 0.4874\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3517 - accuracy: 0.4548 - val_loss: 1.3402 - val_accuracy: 0.4813\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.3439 - accuracy: 0.4670 - val_loss: 1.3354 - val_accuracy: 0.4943\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3451 - accuracy: 0.4649 - val_loss: 1.3324 - val_accuracy: 0.4935\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3423 - accuracy: 0.4666 - val_loss: 1.3285 - val_accuracy: 0.4970\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3327 - accuracy: 0.4707 - val_loss: 1.3258 - val_accuracy: 0.4952\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3294 - accuracy: 0.4720 - val_loss: 1.3246 - val_accuracy: 0.4996\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3306 - accuracy: 0.4725 - val_loss: 1.3268 - val_accuracy: 0.5013\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3325 - accuracy: 0.4705 - val_loss: 1.3227 - val_accuracy: 0.5004\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.3281 - accuracy: 0.4720 - val_loss: 1.3187 - val_accuracy: 0.4996\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.3222 - accuracy: 0.4744 - val_loss: 1.3209 - val_accuracy: 0.4996\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.3229 - accuracy: 0.4736 - val_loss: 1.3206 - val_accuracy: 0.4996\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 1.3190 - accuracy: 0.4720 - val_loss: 1.3211 - val_accuracy: 0.4987\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 1.3201 - accuracy: 0.4729 - val_loss: 1.3178 - val_accuracy: 0.5013\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.3191 - accuracy: 0.4746 - val_loss: 1.3172 - val_accuracy: 0.4996\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3165 - accuracy: 0.4723 - val_loss: 1.3207 - val_accuracy: 0.5022\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 1.3132 - accuracy: 0.4757 - val_loss: 1.3202 - val_accuracy: 0.5004\n",
            "The Accuracy of the Nural Network: 48.152658343315125\n",
            "77/77 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8dff830355e9>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mDMod6Pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNDepClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Test_Ten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mConMax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_Test_Ten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDMod6Pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConMax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDepClassNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Claset).astype(np.float32)\n",
        "D_Train_Ten = np.asarray(D_Claset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldClaset).astype(np.float32)\n",
        "D_Test_Ten = np.asarray(D_HoldClaset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNDepClass = Sequential()\n",
        "NNDepClass.add(Dense(32, activation='relu',input_shape=(None,49)))\n",
        "NNDepClass.add(Dropout(0.2))\n",
        "NNDepClass.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNDepClass.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNDepClass.fit(X_Train_Ten, D_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Dscore = NNDepClass.evaluate(X_Test_Ten, D_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Dscore[1]*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z872ZOvUYray"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "bSXG9qKk8gbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fJa2BXoxeQM"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [DModel_Acc1, DModel_Acc2, DModel_Acc3, DModel_Acc4, DModel_Acc5, Dscore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestDepClassParams = DepClaMod1.best_params_\n",
        "\n",
        "  DepClaNei = BestDepClassParams['n_neighbors']\n",
        "  DepClaAlg = BestDepClassParams['algorithm']\n",
        "  DepClaWei = BestDepClassParams['weights']\n",
        "\n",
        "  FastDepClassMod = KNeighborsClassifier(n_neighbors=DepClaNei, algorithm=DepClaAlg, weights=DepClaWei, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Claset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldClaset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(FinalDepClaMod, file)\n",
        "\n",
        "elif BestMod == DModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(DepClaMod2, file)\n",
        "\n",
        "elif BestMod == DModel_Acc3:\n",
        "  #The SVC is the best model\n",
        "  BestDepClassParams = DepClaMod3.best_params_\n",
        "\n",
        "  DepClaKer = BestDepClassParams['kernel']\n",
        "  DepClaC = BestDepClassParams['C']\n",
        "\n",
        "  FastDepClassMod = SVC(kernel=DepClaKer, C=DepClaC, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Claset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldClaset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(FinalDepClaMod, file)\n",
        "\n",
        "elif BestMod == DModel_Acc4:\n",
        "  #The SVC is the best model.\n",
        "  BestDepClassParams = DepClaMod4.best_params_\n",
        "\n",
        "  DepClaKer = BestDepClassParams['kernel']\n",
        "  DepClaDeg = BestDepClassParams['degree']\n",
        "  DepClaC = BestDepClassParams['C']\n",
        "\n",
        "  FastDepClassMod = SVC(kernel=DepClaKer, degree=DepClaDeg, C=DepClaC, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Claset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldClaset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(FinalDepClaMod, file)\n",
        "\n",
        "elif BestMod == DModel_Acc5:\n",
        "  #The HisGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc5*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(DepClaMod5, file)\n",
        "\n",
        "elif BestMod == Dscore[1]:\n",
        "  #The Nural Network is the best model\n",
        "  print(f'The Accuracy of the Nural Network Model is: {Dscore[1]*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestDepClassModel'\n",
        "  joblib.dump(NNDepClass, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkZipli1SPs"
      },
      "source": [
        "###Anxiety Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FfYWK6TY7Ij"
      },
      "source": [
        "####Data Split for Anxiety"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9772EsXRHfFR"
      },
      "outputs": [],
      "source": [
        "#Anxiety Model.\n",
        "A = Classdf[targets[1]]\n",
        "randnum = 500\n",
        "\n",
        "X_Claset, X_HoldClaset, A_Claset, A_HoldClaset = train_test_split(X, A, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEePMb3ZUrO"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZCDQ9iFZUxw"
      },
      "outputs": [],
      "source": [
        "KNClassPara = {\n",
        "    'n_neighbors': [75, 100, 125], \n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "    \n",
        "KNClass = KNeighborsClassifier()\n",
        "AnxCalModel1 = GridSearchCV(KNClass, KNClassPara)\n",
        "AnxCalModel1.fit(X_Claset, A_Claset)\n",
        "\n",
        "AModel_Acc1 = AnxCalModel1.score(X_HoldClaset, A_HoldClaset)\n",
        "print(f'The Accuracy of the KNClass Model is: {AModel_Acc1*100}%')\n",
        "print(AnxCalModel1.best_params_)\n",
        "\n",
        "AMod1Pred = AnxCalModel1.predict(X_HoldClaset)\n",
        "print(classification_report(A_HoldClaset, AMod1Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A_HoldClaset, AMod1Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfoOEjx2ZVEE"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUuZIWfbZVJq"
      },
      "outputs": [],
      "source": [
        "RandForClaPar = {\n",
        "    'n_estimators': [175, 200, 225], \n",
        "    'criterion': ['gini'], \n",
        "    'max_depth': [20,23,25], \n",
        "    'min_samples_split': [7,8,9],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClassMod = RandomForestClassifier()\n",
        "AnxCalModel2 = GridSearchCV(RandForClassMod, RandForClaPar)\n",
        "AnxCalModel2.fit(X_Claset, A_Claset)\n",
        "\n",
        "AModel_Acc2 = AnxCalModel2.score(X_HoldClaset, A_HoldClaset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "print(AnxCalModel2.best_params_)\n",
        "\n",
        "AMod2Pred = AnxCalModel2.predict(X_HoldClaset)\n",
        "print(classification_report(A_HoldClaset, AMod2Pred))\n",
        "\n",
        "ConMax = confusion_matrix(A_HoldClaset, AMod2Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4aZ1jS-ZVQC"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ],
      "metadata": {
        "id": "NrVIvOKsm39i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoAVyJ8DZVWT"
      },
      "outputs": [],
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "SVCClass = SVC()\n",
        "AnxCalModel3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "AnxCalModel3.fit(X_Claset, A_Claset)\n",
        "\n",
        "AModel_Acc3 = AnxCalModel3.score(X_HoldClaset, A_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {AModel_Acc3*100}%')\n",
        "print(AnxCalModel3.best_params_)\n",
        "\n",
        "AMod3Pred = AnxCalModel3.predict(X_HoldClaset)\n",
        "print(classification_report(A_HoldClaset, AMod3Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A_HoldClaset, AMod3Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "SVCClass = SVC()\n",
        "AnxCalModel4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "AnxCalModel4.fit(X_Claset, A_Claset)\n",
        "\n",
        "AModel_Acc4 = AnxCalModel4.score(X_HoldClaset, A_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {AModel_Acc4*100}%')\n",
        "print(AnxCalModel4.best_params_)\n",
        "\n",
        "AMod4Pred = AnxCalModel4.predict(X_HoldClaset)\n",
        "print(classification_report(A_HoldClaset, AMod4Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A_HoldClaset, AMod4Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94_j1iJ_nHBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "Gh91gq7_8hKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss'],\n",
        "    'learning_rate':[0.1,0.3,0.5],\n",
        "    'max_iter':[75,100,125],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "AnxClaModel5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "AnxClaModel5.fit(X_Claset, A_Claset)\n",
        "\n",
        "AModel_Acc5 = AnxClaModel5.score(X_HoldClaset, A_HoldClaset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc5*100}%')\n",
        "print(AnxClaModel5.best_params_)\n",
        "\n",
        "AMod5Pred = AnxClaModel5.predict(X_HoldClaset)\n",
        "print(classification_report(A_HoldClaset, AMod5Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A_HoldClaset, AMod5Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_f35mVNf8hKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBKbvEPZvDc"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12Pf1fTu1epw"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Claset).astype(np.float32)\n",
        "A_Train_Ten = np.asarray(A_Claset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldClaset).astype(np.float32)\n",
        "A_Test_Ten = np.asarray(A_HoldClaset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNAnxClassMod = Sequential()\n",
        "NNAnxClassMod.add(Dense(64, activation='relu',input_shape=(None,49)))\n",
        "NNAnxClassMod.add(Dropout(0.2))\n",
        "NNAnxClassMod.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNAnxClassMod.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNAnxClassMod.fit(X_Train_Ten, A_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Ascore = NNAnxClassMod.evaluate(X_Test_Ten, A_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Ascore[1]*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCeBf2-tZzy1"
      },
      "source": [
        "####Final Ensamble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "rwsoIEk1_D_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cckKeil31eiK"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [AModel_Acc1, AModel_Acc2, AModel_Acc3, AModel_Acc4, AModel_Acc5, Ascore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == AModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestAnxClassParams = AnxCalModel1.best_params_\n",
        "\n",
        "  AnxClaNei = BestAnxClassParams['n_neighbors']\n",
        "  AnxClaAlg = BestAnxClassParams['algorithm']\n",
        "  AnxClaWei = BestAnxClassParams['weights']\n",
        "\n",
        "  FastAnxClassMod = KNeighborsClassifier(n_neighbors=AnxClaNei, algorithm=AnxClaAlg, weights=AnxClaWei, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Claset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldClaset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(FinalAnxClaMod, file)\n",
        "\n",
        "elif BestMod == AModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(AnxCalModel2, file)\n",
        "\n",
        "elif BestMod == AModel_Acc3:\n",
        "  #The SVC is the best model.\n",
        "  BestAnxClassParams = AnxCalModel3.best_params_\n",
        "\n",
        "  AnxClaKer = BestAnxClassParams['kernel']\n",
        "  AnxClaC = BestAnxClassParams['C']\n",
        "\n",
        "  FastAnxClassMod = SVC(kernel=AnxClaKer, C=AnxClaC, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Claset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldClaset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(FinalAnxClaMod, file)\n",
        "\n",
        "elif BestMod == AModel_Acc4:\n",
        "  #The SVC is the best model.\n",
        "  BestAnxClassParams = AnxCalModel4.best_params_\n",
        "\n",
        "  AnxClaKer = BestAnxClassParams['kernel']\n",
        "  AnxClaDeg = BestAnxClassParams['degree']\n",
        "  AnxClaC = BestAnxClassParams['C']\n",
        "\n",
        "  FastAnxClassMod = SVC(kernel=AnxClaKer, degree=AnxClaDeg, C=AnxClaC, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Claset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldClaset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(FinalAnxClaMod, file)\n",
        "\n",
        "elif BestMod == AModel_Acc5:\n",
        "  #The HistGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc5*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(AnxClaModel5, file)\n",
        "\n",
        "elif BestMod == Ascore[1]:\n",
        "  #The Nural Network is the best model.\n",
        "  print(f'The Accuracy of the Nural Network Model is: {Ascore[1]*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestAnxClassModel'\n",
        "  joblib.dump(NNAnxClassMod, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emdL0Gl31Vqo"
      },
      "source": [
        "###Stress Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tIhIwb6aFX3"
      },
      "source": [
        "####Data Split for Stress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TavXFjBfHe-B"
      },
      "outputs": [],
      "source": [
        "S = Classdf[targets[2]]\n",
        "\n",
        "X_Claset, X_HoldClaset, S_Claset, S_HoldClaset = train_test_split(X, S, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxnkHoNZ_T7"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbXbMnkpZ_l7"
      },
      "outputs": [],
      "source": [
        "KNClassPara = {\n",
        "    'n_neighbors': [15, 20, 100, 300], \n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "    \n",
        "KNClass = KNeighborsClassifier()\n",
        "StrsClassModel1 = GridSearchCV(KNClass, KNClassPara)\n",
        "StrsClassModel1.fit(X_Claset, S_Claset)\n",
        "\n",
        "SModel_Acc1 = StrsClassModel1.score(X_HoldClaset, S_HoldClaset)\n",
        "print(f'The Accuracy of the KNClass Model is: {SModel_Acc1*100}%')\n",
        "print(StrsClassModel1.best_params_)\n",
        "\n",
        "SMod1Pred = StrsClassModel1.predict(X_HoldClaset)\n",
        "print(classification_report(S_HoldClaset, SMod1Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S_HoldClaset, SMod1Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTB5VansaBHE"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKvDZGAMaBwe"
      },
      "outputs": [],
      "source": [
        "RandForClassPara = {\n",
        "    'n_estimators': [100, 150, 200], \n",
        "    'criterion': ['gini', 'entropy', 'log_loss'], \n",
        "    'max_depth': [12,15,20], \n",
        "    'min_samples_split': [5,6,7,8], \n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClass = RandomForestClassifier()\n",
        "StrsClassModel2 = GridSearchCV(RandForClass, RandForClassPara)\n",
        "StrsClassModel2.fit(X_Claset, S_Claset)\n",
        "\n",
        "SModel_Acc2 = StrsClassModel2.score(X_HoldClaset, S_HoldClaset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "print(StrsClassModel2.best_params_)\n",
        "\n",
        "SMod2Pred = StrsClassModel2.predict(X_HoldClaset)\n",
        "print(classification_report(S_HoldClaset, SMod2Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S_HoldClaset, SMod2Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_1ji4OaDUQ"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYp1NPyDaDaL"
      },
      "outputs": [],
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "StrsClassModel3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "StrsClassModel3.fit(X_Claset, S_Claset)\n",
        "\n",
        "SModel_Acc3 = StrsClassModel3.score(X_HoldClaset, S_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {SModel_Acc3*100}%')\n",
        "print(StrsClassModel3.best_params_)\n",
        "\n",
        "SMod3Pred = StrsClassModel3.predict(X_HoldClaset)\n",
        "print(classification_report(S_HoldClaset, SMod3Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S_HoldClaset, SMod3Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-v8_ZFfsmMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3,4,5,6],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "StrsClassModel4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "StrsClassModel4.fit(X_Claset, S_Claset)\n",
        "\n",
        "SModel_Acc4 = StrsClassModel4.score(X_HoldClaset, S_HoldClaset)\n",
        "print(f'The Accuracy of the SVC Model is: {SModel_Acc4*100}%')\n",
        "print(StrsClassModel4.best_params_)\n",
        "\n",
        "SMod4Pred = StrsClassModel4.predict(X_HoldClaset)\n",
        "print(classification_report(S_HoldClaset, SMod4Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S_HoldClaset, SMod4Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OFejHq5CsmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "PObxcmC-9jFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss'],\n",
        "    'learning_rate':[0.1,0.3,0.5],\n",
        "    'max_iter':[75,100,125],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "StsClassModel5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "StsClassModel5.fit(X_Claset, S_Claset)\n",
        "\n",
        "SModel_Acc5 = StsClassModel5.score(X_HoldClaset, S_HoldClaset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc5*100}%')\n",
        "print(StsClassModel5.best_params_)\n",
        "\n",
        "SMod5Pred = StsClassModel5.predict(X_HoldClaset)\n",
        "print(classification_report(S_HoldClaset, SMod5Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S_HoldClaset, SMod5Pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MVwiOMhM9jFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVvvKfDbyRq"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fyrco7d1fJC"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Claset).astype(np.float32)\n",
        "S_Train_Ten = np.asarray(S_Claset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldClaset).astype(np.float32)\n",
        "S_Test_Ten = np.asarray(S_HoldClaset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNStsClassMod = Sequential()\n",
        "NNStsClassMod.add(Dense(32, activation='relu',input_shape=(None,49)))\n",
        "NNStsClassMod.add(Dropout(0.2))\n",
        "NNStsClassMod.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNStsClassMod.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNStsClassMod.fit(X_Train_Ten, S_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Sscore = NNStsClassMod.evaluate(X_Test_Ten, S_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Sscore[1]*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LozrfKKbypZ"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "uKfxIRgL_4nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2cYWls11fGq"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [SModel_Acc1, SModel_Acc2, SModel_Acc3, SModel_Acc4, SModel_Acc5, Sscore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == SModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestStsClassParams = StrsClassModel1.best_params_\n",
        "\n",
        "  StsClaNei = BestStsClassParams['n_neighbors']\n",
        "  StsClaAlg = BestStsClassParams['algorithm']\n",
        "  StsClaWei = BestStsClassParams['weights']\n",
        "\n",
        "  FastStsClassMod = KNeighborsClassifier(n_neighbors=StsClaNei, algorithm=StsClaAlg, weights=StsClaWei, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Claset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldClaset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(FinalStsClaMod, file)\n",
        "\n",
        "elif BestMod == SModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(StrsClassModel2, file)\n",
        "\n",
        "elif BestMod == SModel_Acc3:\n",
        "  #The SVC is the best model.\n",
        "  BestStsClassParams = StrsClassModel3.best_params_\n",
        "\n",
        "  StsClaKer = BestStsClassParams['kernel']\n",
        "  StsClaC = BestStsClassParams['C']\n",
        "\n",
        "  FastStsClassMod = SVC(kernel=StsClaKer, C=StsClaC, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Claset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldClaset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(FinalStsClaMod, file)\n",
        "\n",
        "elif BestMod == SModel_Acc4:\n",
        "  #The Polynomial SVC is the best model.\n",
        "  BestStsClassParams = StrsClassModel4.best_params_\n",
        "\n",
        "  StsClaKer = BestStsClassParams['kernel']\n",
        "  StsClaDeg = BestStsClassParams['degree']\n",
        "  StsClaC = BestStsClassParams['C']\n",
        "\n",
        "  FastStsClassMod = SVC(kernel=StsClaKer, degree=StsClaDeg, C=StsClaC, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Claset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldClaset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(FinalStsClaMod, file)\n",
        "\n",
        "elif BestMod == SModel_Acc5:\n",
        "  #The HisGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc5*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(StsClassModel5, file)\n",
        "\n",
        "elif BestMod == Sscore[1]:\n",
        "  #The MLP is the best model.\n",
        "  print(f'The Accuracy of the MLP Model is: {Sscore[1]*100}%')\n",
        "\n",
        "  #Saves the best model\n",
        "  file = path / f'BestStsClassModel'\n",
        "  joblib.dump(NNStsClassMod, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "63RHuvMvNaxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yGPZMlPgNkWO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1s8-xWnQULif",
        "9pp71514hoxZ",
        "PB6OhOPkvWI9",
        "UY02y715otK0",
        "lOplAYr41FQ3",
        "63RHuvMvNaxe"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "10hfp4PoZEjrwMoAw5B1e4oKKVU0faOXS",
      "authorship_tag": "ABX9TyOKp/hXGda515w6ikvNxaGA",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}