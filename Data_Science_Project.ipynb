{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanWallinger/DataScienceProject/blob/main/Data_Science_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0riwwMKCn9_q"
      },
      "source": [
        "#Mental Health Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmiUqMl-oEdZ"
      },
      "source": [
        "\n",
        "\n",
        "*   Sean Wallinger\n",
        "*   Math 5364 Data Science 1\n",
        "*   Tarleton State University\n",
        "*   Spring 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qOsJ3ZbYvJ"
      },
      "source": [
        "DISCLAMER: This is NOT a diagnosis! These findings are based on common symptoms of mental issues. If you are concerned about your mental health seek professional medical help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONfsEzjQxddX"
      },
      "source": [
        "##Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2nA6MY3xgwg"
      },
      "source": [
        "The task of this project To create a model that can aid in the early dectetion of serious mental issues using data from the data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_VYpW2oL6j"
      },
      "source": [
        "##Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVMq3sMAV_P"
      },
      "source": [
        "For many years, Mental issues and Anxiety has effected people and caused many problems. The field of pycology has worked very hard to try to understand, identify sympotoms, and try to get patients the help they need before it is too late. But, identifing these big mental issues with simple symptoms is a very challenging task. Now the question can be asked, can past cases help future ones? Can past patterns and symptoms possibly help identify current patients who may not even know if they have an issue?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4khyxgT0P3p4"
      },
      "source": [
        "###Depression Anxiety Stress Scales (DASS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOM3UnsQhl4"
      },
      "source": [
        "The Psychology Foundation of Australia created the Depression Anxiety Stress Scales (DASS) and hosted the questionnaire on a public website in an effort to help educate the public on Depression, Anxiety, and Stress.\n",
        "\n",
        "The DASS is comprised of 42 questions which are divided into three sets of 14 questions which are related to the topics of Depression, Anxiety, and Stress. These questions have four possible responses which relate to a possible score as seen below:\n",
        "\n",
        "    Did not apply to me at all ---> 0\n",
        "    Applied to me to some degree, or some of the time ---> 1\n",
        "    Applied to me to a considerable degree, or a good part of time ---> 2\n",
        "    Applied to me very much, or most of the time ---- 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8-xWnQULif"
      },
      "source": [
        "####Question Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXfI9Z0cUYu7"
      },
      "source": [
        "![demo1.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pp71514hoxZ"
      },
      "source": [
        "###The Ten-Item Personality Test (TIPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRTVOEDphzN0"
      },
      "source": [
        "To help get an idea of the personality of each entry, a Ten-Item Personality Test was administered and resposes where measured. The TIPT asks ten questions about a persons behavor and the User responds on a 7 point scale as show below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAbASZYjOxe"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuaegifQjW9q"
      },
      "source": [
        "Two of the Ten questions relate to one of the big Five personality traits, the average of these finds a person's mesure in each of these personalities. The five big personalities are Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmv53LFEljNK"
      },
      "source": [
        "<img src = https://saylordotorg.github.io/text_principles-of-management-v1.1/section_06/2ca4a3e8a5ffc2a3ff954f79a1475071.jpg width = \"575\" height = \"383\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9mgS95IU_pY"
      },
      "source": [
        "###Scoring Scales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NLXNl-gVEKP"
      },
      "source": [
        "The Score for the DASS 42 questionair is derived by summing up all the questions that relate to the relative topic. The total score is an indicator of the severity of the self reported symptoms that relate to mental issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rbeXnOIfp8T"
      },
      "source": [
        "####Scoring Scale of the DASS 42 Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwjFAv1MfwNo"
      },
      "source": [
        "![Screenshot 2023-03-01 124519.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6OhOPkvWI9"
      },
      "source": [
        "####Scoring of the TIPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjK9WvyvdZt"
      },
      "source": [
        "The score of the person's measure of the big five personalities are determined by finding the two answers that relate to the respective personality. Add one score to the reversed score of the other. The average of these two score is the metric of that person possessing such a personality trait."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY02y715otK0"
      },
      "source": [
        "###Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6xawE-BcjD"
      },
      "source": [
        "The Goal of this project is to Predict the severity of a person's Depression, Anxiety, and Stress Target Variables using personal information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jXaRgKgokQI"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiykW2H1orUK"
      },
      "source": [
        "###[Psychology Foundation of Australia](http://www.psychologyfoundation.org.au/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ho9H7ccpTnL"
      },
      "source": [
        "[\"The Psychology Foundation of Australia is a grouping of research-oriented schools of Psychology formed to defend rigorous academic and scientific standards in the teaching, research and training in the discipline of Psychology and its professional practice within Australia.\"](http://www.psychologyfoundation.org.au/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmW2flopfSY"
      },
      "source": [
        "####[Depression Anxiety Stress Scales Responses](https://www.kaggle.com/datasets/lucasgreenwell/depression-anxiety-stress-scales-responses?select=codebook.txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq-bj39hpu67"
      },
      "source": [
        "- Collected March 7th, 2023 via direct download of csv file.\n",
        "- Data was collected with an on-line wersion of the Depression Anxiety Stress Scales (DASS).\n",
        "- The survey was open to anyone and people were motivated to take it to get personalized results. At the end of the test they also were given the option to complete a short research survey. This datatset comes from those who agreed to complete the research survey and answered yes to the question \"Have you given accurate answers and may they be used for research?\" at the end.\n",
        "- This data was collected 2017 - 2019.\n",
        "- Important Features:\n",
        "    - Q(number)A (int): The User's answer to question (number).\n",
        "    - Country (str): ISO country code of where the user connected from.\n",
        "    - TIPI(number) (int): Answer to Ten Item Personality inventory Question (number).\n",
        "    - Education (int): The highest education level the User has completed.\n",
        "    - Urban (int): The Type of area the User lived in as a child.\n",
        "    - Gender (int): The Gender of the User.\n",
        "    - Age (int): The Age of the User.\n",
        "    - Hand (int): The hand that the User writes with.\n",
        "    - Religon (int): What religon the User practices.\n",
        "    - Orientation (int): The Sexual orientation of the User.\n",
        "    - Race (int): The Race of the User.\n",
        "    - Married (int): The Marital status of the User.\n",
        "    - Family Size (int): The amount of childeren the User's mother had.\n",
        "\n",
        "- [More Information on the DASS Questionnaire](http://www2.psy.unsw.edu.au/dass/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8LjDIfXb-D6"
      },
      "source": [
        "###Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdibTo4ScApT"
      },
      "source": [
        "Here are the major data wrangling challenges and solutions.\n",
        "\n",
        "1.   The Data set has entries from many international users randomly taken across the globe.\n",
        "\n",
        "  *   The only rows grabbed from the data set stored in Google's BigQuery is the rows that have the 'US' string in the Country feature since it is easier to gain censis data to improve the model.\n",
        "  *   An International look will be contemplated during the project.\n",
        "\n",
        "2.   All of the answers from the Questionnaire (i.e. 'q1a', 'q2a', ect.) is stored alternatively to common computer science format.\n",
        "\n",
        "  *   Shifted the threshold of the stored answers to match the standard range of zero to four.\n",
        "\n",
        "3.   The Data set stores the ansers to each question in their own spearate column.\n",
        "\n",
        "  *   Added all data from columns 'Q3A', 'Q5A', 'Q10A', 'Q13A', 'Q16A', 'Q17A', 'Q21A', 'Q24A', 'Q26A', 'Q31A', 'Q34A', 'Q37A', 'Q38A', and 'Q42A'. A new Feature column was created to store this score as the severty of the user's Depression. This Severty scal is as such: 0 for Normal (Score: 0-9), 1 for mild (score: 10-13), 2 for Moderate (Score: 14-20), 3 for Severe (score: 21-27), and 4 for Extremely Severe (Score: 28+). \n",
        "\n",
        "  *   Added all data from columns 'Q2A', 'Q4A', 'Q7A', 'Q9A', 'Q15A', 'Q19A', 'Q20A', 'Q23A', 'Q25A', 'Q28A', 'Q30A', 'Q36A', 'Q40A', and 'Q41A'. A new Feature column was created to store this score as the severty of the user's Anxiety. This Severty scal is as such: 0 for Normal (Score: 0-7), 1 for mild (score: 8-9), 2 for Moderate (Score: 10-14), 3 for Severe (score: 15-19), and 4 for Extremely Severe (Score: 20+).\n",
        "\n",
        "  *   Added all data from columns 'Q1A', 'Q6A', 'Q8A', 'Q11A', 'Q12A', 'Q14A', 'Q18A', 'Q22A', 'Q27A', 'Q29A', 'Q32A', 'Q33A', 'Q35A', and 'Q39A'. A new Feature column was created to store this score as the severty of the user's Stress. This Severty scal is as such: 0 for Normal (Score: 0-14), 1 for mild (score: 15-18), 2 for Moderate (Score: 19-25), 3 for Severe (score: 26-33), and 4 for Extremely Severe (Score: 34+).\n",
        "\n",
        "4.   The Data set also consists of the results from a Ten Item Personality Test that was administered with the online questionnaire. \n",
        "\n",
        "  *   Each entry relates to one of five categories. Each category has two questions associated with it, one for and one against. The against question's score is reversed (i.e. if the input is 2 then swap it to 6) and take the average of these two answers to get the category score.\n",
        "\n",
        "5.   The dataset consists of alot of data with general categorical features.\n",
        "\n",
        "  *   Took the Data stored in these catigorical features and created a new column for each catigory to make it binary.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WED6CShzWx_E"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TqBYxIw5EH4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2761b5-e4a8-43f7-cc28-e5bb77de5dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for helpers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -q 'git+https://github.com/drscook/helpers'\n",
        "! pip install -q --upgrade pandas scikit-learn # pandas 2.0 just released on April 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IzzDlsw5ETqA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2caebcd-cb55-4fdf-f018-5cd4ad44527a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 8.05 s (2023-04-22T18:02:01/2023-04-22T18:02:09)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%reload_ext autotime\n",
        "from helpers.common_imports import *\n",
        "from helpers import utilities as ut\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, BaggingRegressor, HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay, confusion_matrix, RocCurveDisplay\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rEAu9GuAEhGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42f47151-5e37-4611-ee64-b083f4cd6c73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 1 h 8 min 3 s (2023-04-22T18:02:09/2023-04-22T19:10:13)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Pull from Big Query.\n",
        "bq = ut.BigQuery()\n",
        "tbl = f\"tarletondatascience2022.wallinger.Depression_Anxiety_Stress_Scales\"\n",
        "qry = f\"\"\"\n",
        "select\n",
        "    *\n",
        "from\n",
        "    {tbl}\n",
        "where\n",
        "    country = 'US'\n",
        "\"\"\"\n",
        "\n",
        "df = bq.qry_to_df(qry)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Global Variables.\n",
        "randnum = 500 #Setting a random number to get similar results.\n",
        "Holdout_size = 0.3 #Sets the Holdout portion.\n",
        "C = np.linspace(0.1,1,num=10) #Creates a Cost Varaible for the SVC Models.\n",
        "\n",
        "#List Containing Class names for the verification of the Classification Models.\n",
        "DepClassNames =  ['No Depression', 'Mild Depression', 'Moderate Depression', 'Severe Depression', 'Extremely Severe Depression']\n",
        "AnxClassNames =  ['No Anxiety', 'Mild Anxiety', 'Moderate Anxiety', 'Severe Anxiety', 'Extremely Severe Anxiety']\n",
        "StsClassNames =  ['No Stress', 'Mild Stress', 'Moderate Stress', 'Severe Stress', 'Extremely Severe Stress']"
      ],
      "metadata": {
        "id": "Oruq0u2zufPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1b806d9-bef0-432f-8e97-727d0c5cb0bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 9.51 ms (2023-04-22T19:10:13/2023-04-22T19:10:13)</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checking the Data"
      ],
      "metadata": {
        "id": "gqgMOLDpKHFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is fairly good right away. No missing values in all features and targets, Large row count, and besides some minor formatting issues. Only one typo was found but since this may effect the normalization range, I chaged to more likely input. Also found some outliers in the faimlysize but unsure if I would be removing too much variance."
      ],
      "metadata": {
        "id": "FMIUfamV1YL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixes the age error.\n",
        "WorngAge = 117\n",
        "CorrectAge = 17\n",
        "df.at[4741, 'age'] = CorrectAge\n",
        "\n",
        "#Verifies that the fix is indeed in place.\n",
        "print(df.loc[df['age'] == WorngAge, 'age'])\n",
        "\n",
        "print(df.loc[df['familysize'] > 20, 'familysize'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "oPY6K9HC2uG0",
        "outputId": "a13c730c-e18f-4f71-8a03-a65b3435062e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 22.4 ms (2023-04-22T19:10:13/2023-04-22T19:10:13)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], Name: age, dtype: Int64)\n",
            "index\n",
            "6765     62\n",
            "8052    133\n",
            "Name: familysize, dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ut.pprint(df)"
      ],
      "metadata": {
        "id": "u8_HbjgwKMCc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "c27db51f-7464-49f3-e93b-148881d967f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 371 ms (2023-04-22T19:10:13/2023-04-22T19:10:14)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       q1a  q1i    q1e  q2a  q2i    q2e  q3a  q3i    q3e  q4a  q4i   q4e  q5a   \n",
              "index                                                                           \n",
              "0        1   18   6116    1   28   3193    2    2  12542    1    8  6150    3  \\\n",
              "1        1   10   2901    1    1   9036    1   22   3775    1   34  2027    1   \n",
              "2        3    5   6729    2    4   4793    2   20   3328    1   16  2199    3   \n",
              "3        1    9   2236    1   10   1437    2    7   2570    1   17  2188    1   \n",
              "4        1   35   3404    2   28  10935    1    5   3593    1   13  5086    1   \n",
              "...    ...  ...    ...  ...  ...    ...  ...  ...    ...  ...  ...   ...  ...   \n",
              "8202     4   17   3069    4    6   3561    4   25   2200    3   21  3045    4   \n",
              "8203     3   30   3626    1   38   4161    2   27   2640    3    2  7369    4   \n",
              "8204     4   16   2000    2   10   2055    4   22   2229    3    9  3527    4   \n",
              "8205     4   38   2350    2   29   2316    4   20   2087    4   30  2614    4   \n",
              "8206     3   32  28897    4    4   3025    2   30   3248    2   16  8048    2   \n",
              "\n",
              "       q5i   q5e  q6a  q6i    q6e  q7a  q7i   q7e  q8a  q8i   q8e  q9a  q9i   \n",
              "index                                                                         \n",
              "0       40  6428    1    4  17001    1   33  2944    3    7  8626    3   14  \\\n",
              "1       38  2714    1    3   3626    1   11  4496    1   27  3453    1   41   \n",
              "2       22  2513    1   21   2220    1   13  2489    2   32  2570    1    8   \n",
              "3       27  3372    1    2   1975    1    6  1579    1   37  1736    1   26   \n",
              "4       17  2810    1   10   3677    1   29  4919    1    6  3288    1   38   \n",
              "...    ...   ...  ...  ...    ...  ...  ...   ...  ...  ...   ...  ...  ...   \n",
              "8202     1  8671    4   30   2186    4   32  1527    4    4  2236    4   26   \n",
              "8203    12  2607    4   15   2122    4   19  2473    3   32  1538    4   37   \n",
              "8204     6  1633    4   24   1032    2   21  3567    4    4  2608    4   31   \n",
              "8205    19  1320    4   35   8479    4   36  1443    4   21  1320    4    8   \n",
              "8206     8  3559    4   34   2219    4    5  4553    2   21  1890    1   36   \n",
              "\n",
              "         q9e  q10a  q10i  q10e  q11a  q11i   q11e  q12a  q12i  q12e  q13a   \n",
              "index                                                                       \n",
              "0       9639     2    20  6175     1    34   6008     2    21  9267     1  \\\n",
              "1       4619     1    21  2540     1    20   2946     1     6  3508     1   \n",
              "2       5790     2    18  4348     1     1   5951     1    26  1973     1   \n",
              "3      11058     2     8  3389     1    38   2105     1    23  2218     1   \n",
              "4       8824     1    31  5750     1    21   2475     1    37  2422     1   \n",
              "...      ...   ...   ...   ...   ...   ...    ...   ...   ...   ...   ...   \n",
              "8202    1265     4    20  2232     4    34   1676     4    40  1716     4   \n",
              "8203    4779     2    28  2741     4    13   2389     3    34  2690     4   \n",
              "8204    2936     4    19  1848     4    12   2448     2     8  4936     4   \n",
              "8205    2862     4    28  1488     4     1  12252     4    37  1481     4   \n",
              "8206    4561     3    11  4071     3    23   2909     3    17  5179     4   \n",
              "\n",
              "       q13i  q13e  q14a  q14i   q14e  q15a  q15i  q15e  q16a  q16i  q16e   \n",
              "index                                                                      \n",
              "0        41  5290     3     1  25694     2     9  7634     4    37  8513  \\\n",
              "1        14  1727     1    31   4508     1     4  2490     1     5  3725   \n",
              "2        11  2679     4    33   3425     1    23  1931     3    28  4472   \n",
              "3        19  1803     1    31   3058     2    29  4186     3     5  2184   \n",
              "4        36  1638     1    24  15928     1    12  2770     1    26  4719   \n",
              "...     ...   ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   \n",
              "8202     16  2553     4    39   1797     4    33  1585     4    23  3475   \n",
              "8203     18  1320     4    14   3376     3     5  1621     2    29  2038   \n",
              "8204      3  1423     4    40   3063     4    33  1103     4    11  2240   \n",
              "8205     10  1291     4    12   2200     4    33  3329     4    23  1575   \n",
              "8206     14  2190     2    28  52923     1    41  2126     2    18  3341   \n",
              "\n",
              "       q17a  q17i  q17e  q18a  q18i  q18e  q19a  q19i   q19e  q20a  q20i   \n",
              "index                                                                      \n",
              "0         2    25  9078     1    15  4381     1    23   6647     2    36  \\\n",
              "1         1    28  4826     1    17  2425     1    13   3002     1    23   \n",
              "2         1    24  1450     2    40  2006     1    25   3613     1    19   \n",
              "3         1    11  3906     1    14  2010     1    35   3128     1    30   \n",
              "4         1    39  3639     1    32  1745     1    11   8400     1    23   \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...    ...   ...   ...   \n",
              "8202      4    14  2722     4    18  1785     3    29   4253     4    11   \n",
              "8203      4    40  2406     1    26  3810     3     9   3910     3    17   \n",
              "8204      4    27  1832     4    23  1383     2    37   3498     4     7   \n",
              "8205      4    18  1286     3     6  2491     2    34   3308     4    16   \n",
              "8206      2    24  3949     4    29  4455     3     2  19120     3     7   \n",
              "\n",
              "       q20e  q21a  q21i  q21e  q22a  q22i  q22e  q23a  q23i  q23e  q24a  q24i   \n",
              "index                                                                           \n",
              "0      6250     1    39  3842     1    16  7876     1    27  3124     2    12  \\\n",
              "1      2371     1    37  4058     1    25  2721     1     7  2511     1    33   \n",
              "2      1166     1    15  3214     1    10  4843     1    27  1216     3    41   \n",
              "3      2375     2    42  1957     1    13  1567     1    24  1334     3     4   \n",
              "4      2408     1    30  3548     1    42  4149     1    20  3043     1    41   \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202   2077     4     8  1918     4    15  1533     3    22  2939     4    37   \n",
              "8203   2106     3    41  3241     3    21  2975     2    25  1722     2     3   \n",
              "8204   1760     4    34  1438     4    38  1422     2    15  2296     4    29   \n",
              "8205   1336     4    13  1555     4    15  1270     1     2  3946     4    26   \n",
              "8206   4429     1    39  3372     3    31  3409     1     9  2538     1    10   \n",
              "\n",
              "       q24e  q25a  q25i   q25e  q26a  q26i  q26e  q27a  q27i  q27e  q28a   \n",
              "index                                                                      \n",
              "0      6836     1    31  12063     1     3  9264     1    35  3957     1  \\\n",
              "1      2981     1     9   4554     1    18  1894     1    40  1541     1   \n",
              "2      4086     2    37   5170     1    35  1703     2     9  2482     1   \n",
              "3      2903     1    25   2473     1    41  1397     1    20  2119     1   \n",
              "4      3698     1    27  27429     1    15  2660     1    19  2621     1   \n",
              "...     ...   ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202   2699     4     2   8376     4    10  1452     4    35  1325     4   \n",
              "8203   2456     2     6   3008     4    35  2506     2    22  2991     3   \n",
              "8204   1289     4    32   2384     4     5  2207     4    14  1632     4   \n",
              "8205   1604     4    27   6550     4    41  1244     4    39  1256     4   \n",
              "8206   2541     4    35   8110     2    25  2701     4     6  2408     3   \n",
              "\n",
              "       q28i  q28e  q29a  q29i   q29e  q30a  q30i   q30e  q31a  q31i   q31e   \n",
              "index                                                                        \n",
              "0        42  2537     3    17  10880     2     5   8462     2    32   5615  \\\n",
              "1        12  3354     1    29   3129     1    24   5256     1     2   5345   \n",
              "2        29  1741     1    14   2681     1    12   2779     3    38   2302   \n",
              "3         1  4983     1    15   3193     1    12   2454     1    18   6983   \n",
              "4        22  2219     1     7   2827     1     4   4980     1    25   2623   \n",
              "...     ...   ...   ...   ...    ...   ...   ...    ...   ...   ...    ...   \n",
              "8202      9  3080     4    13   2771     4    31   2336     4    41   2938   \n",
              "8203      4  2173     4     8   2574     4    31   6601     2    23   2957   \n",
              "8204     36  1544     4    20   2351     4    35   1824     4     1   5042   \n",
              "8205     24  1223     4    40   1841     3     7   3922     4    31   2839   \n",
              "8206     37  2909     4    42   4100     2    33  13153     2    13  19343   \n",
              "\n",
              "       q32a  q32i   q32e  q33a  q33i  q33e  q34a  q34i  q34e  q35a  q35i   \n",
              "index                                                                      \n",
              "0         1    30  11412     4     6  5112     1    29  3070     3    10  \\\n",
              "1         1    36   3058     1    39  1690     1    30  1686     1    26   \n",
              "2         2    30   3895     1    36  1313     1    34  1426     4    31   \n",
              "3         1     3   3281     1    40  2984     1    34  1676     1    22   \n",
              "4         1     2  11105     1    33  2245     1    18  5499     1    16   \n",
              "...     ...   ...    ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "8202      4    19   3676     4    42  1717     4    24  1301     4    28   \n",
              "8203      4    16   4044     4     7  2222     3     1  4782     3    11   \n",
              "8204      4    39   1528     4    26  1368     4    17  1344     4    41   \n",
              "8205      4    25   2615     4    42  1342     4    11  1194     2     9   \n",
              "8206      3    22   5189     3    27  3088     4    15  2900     2    12   \n",
              "\n",
              "        q35e  q36a  q36i  q36e  q37a  q37i   q37e  q38a  q38i  q38e  q39a   \n",
              "index                                                                       \n",
              "0      13377     2    38  4506     2    24  17227     2    13  7844     1  \\\n",
              "1       4135     1    42  1852     1    19   4181     1    15  1898     1   \n",
              "2       3803     1     3  2430     1    39   2035     2     7  2754     3   \n",
              "3       5516     1    36  1483     2    33   3556     2    16  2299     2   \n",
              "4       6972     1     9  1761     1     8   8120     1     1  6261     1   \n",
              "...      ...   ...   ...   ...   ...   ...    ...   ...   ...   ...   ...   \n",
              "8202    4973     4    12  1492     4    27   2865     4     7  1791     4   \n",
              "8203    6651     2    39  1604     3    33   3008     4    20  2206     3   \n",
              "8204    1889     4     2  4773     4    30   2886     4    28  1592     4   \n",
              "8205    5597     4    22  1214     4     4   1743     4    32  1833     4   \n",
              "8206   23056     1    19  1677     2     1  15366     1    26  2718     4   \n",
              "\n",
              "       q39i   q39e  q40a  q40i  q40e  q41a  q41i  q41e  q42a  q42i   q42e   \n",
              "index                                                                       \n",
              "0        26  20253     1    22  8528     1    11  4370     2    19  10310  \\\n",
              "1        32   1389     1    35  4266     1    16  1534     1     8   3449   \n",
              "2         6   2830     1    42  2357     1    17  3662     4     2   3372   \n",
              "3        32   2794     1    39  2537     1    28  1524     1    21   2944   \n",
              "4        34   2259     1     3  3951     1    14  2576     1    40  11750   \n",
              "...     ...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...   \n",
              "8202      5   1858     4     3  6011     4    38  1735     4    36   2849   \n",
              "8203     24   2240     4    42  2424     4    36  2106     3    10   4078   \n",
              "8204     42   1208     4    13  2679     4    25  1320     4    18   1920   \n",
              "8205     14   1371     4     3  3110     4     5  3029     4    17   4222   \n",
              "8206     40   4331     4     3  3769     2    20  4261     1    38   4709   \n",
              "\n",
              "      country  source  introelapse  testelapse  surveyelapse  tipi1  tipi2   \n",
              "index                                                                        \n",
              "0          US       2            4         349           213      2      1  \\\n",
              "1          US       2            2         143           228      3      5   \n",
              "2          US       2           73         128            89      2      7   \n",
              "3          US       0           42         121           171      3      3   \n",
              "4          US       2            5         221           183      7      4   \n",
              "...       ...     ...          ...         ...           ...    ...    ...   \n",
              "8202       US       2            2         115            78      1      5   \n",
              "8203       US       0           50         128           144      2      5   \n",
              "8204       US       1            4         120           221      3      6   \n",
              "8205       US       2           22         116            86      2      4   \n",
              "8206       US       2            2         300           234      6      5   \n",
              "\n",
              "       tipi3  tipi4  tipi5  tipi6  tipi7  tipi8  tipi9  tipi10  vcl1  vcl2   \n",
              "index                                                                        \n",
              "0          6      1      7      7      7      2      6       7     1     1  \\\n",
              "1          6      1      6      5      3      2      7       2     1     1   \n",
              "2          7      1      6      5      4      7      6       2     1     1   \n",
              "3          5      1      5      5      5      5      6       5     1     1   \n",
              "4          6      1      6      3      4      2      6       1     1     0   \n",
              "...      ...    ...    ...    ...    ...    ...    ...     ...   ...   ...   \n",
              "8202       1      7      4      7      6      7      4       1     1     1   \n",
              "8203       6      7      5      5      5      4      1       4     1     1   \n",
              "8204       6      7      2      6      4      2      2       1     1     1   \n",
              "8205       1      7      2      4      2      6      1       5     1     1   \n",
              "8206       4      5      6      3      5      2      5       7     1     1   \n",
              "\n",
              "       vcl3  vcl4  vcl5  vcl6  vcl7  vcl8  vcl9  vcl10  vcl11  vcl12  vcl13   \n",
              "index                                                                         \n",
              "0         0     1     1     0     0     0     0      1      0      0      0  \\\n",
              "1         1     1     1     0     1     1     0      1      1      1      1   \n",
              "2         1     1     1     0     0     0     0      1      0      0      1   \n",
              "3         0     1     1     0     0     0     0      1      0      0      0   \n",
              "4         0     1     1     0     0     0     0      1      0      0      1   \n",
              "...     ...   ...   ...   ...   ...   ...   ...    ...    ...    ...    ...   \n",
              "8202      0     1     1     0     0     0     0      1      0      0      1   \n",
              "8203      1     1     1     0     0     0     0      1      0      0      1   \n",
              "8204      1     1     1     0     0     0     0      1      1      0      1   \n",
              "8205      0     1     1     0     0     0     0      1      0      0      0   \n",
              "8206      0     1     1     0     0     0     0      1      0      0      1   \n",
              "\n",
              "       vcl14  vcl15  vcl16  education  urban  gender  engnat  age  screensize   \n",
              "index                                                                           \n",
              "0          0      1      1          2      3       2       2   20           2  \\\n",
              "1          1      1      1          3      2       1       1   34           2   \n",
              "2          1      1      1          2      1       1       1   19           2   \n",
              "3          1      1      1          2      3       2       1   16           1   \n",
              "4          1      1      1          2      2       1       1   18           2   \n",
              "...      ...    ...    ...        ...    ...     ...     ...  ...         ...   \n",
              "8202       1      1      1          3      3       2       1   22           2   \n",
              "8203       1      1      1          1      3       3       1   18           2   \n",
              "8204       1      1      1          4      2       2       1   46           2   \n",
              "8205       0      1      1          1      3       2       1   14           2   \n",
              "8206       1      1      1          1      2       2       1   16           2   \n",
              "\n",
              "       uniquenetworklocation  hand  religion  orientation  race  voted   \n",
              "index                                                                    \n",
              "0                          1     1         4            1    70      2  \\\n",
              "1                          1     1         1            1    60      1   \n",
              "2                          1     3         2            1    60      2   \n",
              "3                          1     1         1            5    70      2   \n",
              "4                          2     1         7            1    30      2   \n",
              "...                      ...   ...       ...          ...   ...    ...   \n",
              "8202                       1     1         7            1    10      1   \n",
              "8203                       1     1         2            4    70      2   \n",
              "8204                       1     1         4            1    60      1   \n",
              "8205                       1     1         1            5    60      2   \n",
              "8206                       1     1         1            1    70      2   \n",
              "\n",
              "       married  familysize                              major  \n",
              "index                                                          \n",
              "0            1           4                               <NA>  \n",
              "1            3           2  Biology and Philosophy dual major  \n",
              "2            1           1                   Computer Science  \n",
              "3            1           3                               <NA>  \n",
              "4            1           2                               <NA>  \n",
              "...        ...         ...                                ...  \n",
              "8202         1           3                            English  \n",
              "8203         1           2                               <NA>  \n",
              "8204         2           6                          Marketing  \n",
              "8205         1           3                               <NA>  \n",
              "8206         1           3                               <NA>  \n",
              "\n",
              "[8207 rows x 172 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9a13824-f681-44a3-a6a8-9fd4aad10958\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q1a</th>\n",
              "      <th>q1i</th>\n",
              "      <th>q1e</th>\n",
              "      <th>q2a</th>\n",
              "      <th>q2i</th>\n",
              "      <th>q2e</th>\n",
              "      <th>q3a</th>\n",
              "      <th>q3i</th>\n",
              "      <th>q3e</th>\n",
              "      <th>q4a</th>\n",
              "      <th>q4i</th>\n",
              "      <th>q4e</th>\n",
              "      <th>q5a</th>\n",
              "      <th>q5i</th>\n",
              "      <th>q5e</th>\n",
              "      <th>q6a</th>\n",
              "      <th>q6i</th>\n",
              "      <th>q6e</th>\n",
              "      <th>q7a</th>\n",
              "      <th>q7i</th>\n",
              "      <th>q7e</th>\n",
              "      <th>q8a</th>\n",
              "      <th>q8i</th>\n",
              "      <th>q8e</th>\n",
              "      <th>q9a</th>\n",
              "      <th>q9i</th>\n",
              "      <th>q9e</th>\n",
              "      <th>q10a</th>\n",
              "      <th>q10i</th>\n",
              "      <th>q10e</th>\n",
              "      <th>q11a</th>\n",
              "      <th>q11i</th>\n",
              "      <th>q11e</th>\n",
              "      <th>q12a</th>\n",
              "      <th>q12i</th>\n",
              "      <th>q12e</th>\n",
              "      <th>q13a</th>\n",
              "      <th>q13i</th>\n",
              "      <th>q13e</th>\n",
              "      <th>q14a</th>\n",
              "      <th>q14i</th>\n",
              "      <th>q14e</th>\n",
              "      <th>q15a</th>\n",
              "      <th>q15i</th>\n",
              "      <th>q15e</th>\n",
              "      <th>q16a</th>\n",
              "      <th>q16i</th>\n",
              "      <th>q16e</th>\n",
              "      <th>q17a</th>\n",
              "      <th>q17i</th>\n",
              "      <th>q17e</th>\n",
              "      <th>q18a</th>\n",
              "      <th>q18i</th>\n",
              "      <th>q18e</th>\n",
              "      <th>q19a</th>\n",
              "      <th>q19i</th>\n",
              "      <th>q19e</th>\n",
              "      <th>q20a</th>\n",
              "      <th>q20i</th>\n",
              "      <th>q20e</th>\n",
              "      <th>q21a</th>\n",
              "      <th>q21i</th>\n",
              "      <th>q21e</th>\n",
              "      <th>q22a</th>\n",
              "      <th>q22i</th>\n",
              "      <th>q22e</th>\n",
              "      <th>q23a</th>\n",
              "      <th>q23i</th>\n",
              "      <th>q23e</th>\n",
              "      <th>q24a</th>\n",
              "      <th>q24i</th>\n",
              "      <th>q24e</th>\n",
              "      <th>q25a</th>\n",
              "      <th>q25i</th>\n",
              "      <th>q25e</th>\n",
              "      <th>q26a</th>\n",
              "      <th>q26i</th>\n",
              "      <th>q26e</th>\n",
              "      <th>q27a</th>\n",
              "      <th>q27i</th>\n",
              "      <th>q27e</th>\n",
              "      <th>q28a</th>\n",
              "      <th>q28i</th>\n",
              "      <th>q28e</th>\n",
              "      <th>q29a</th>\n",
              "      <th>q29i</th>\n",
              "      <th>q29e</th>\n",
              "      <th>q30a</th>\n",
              "      <th>q30i</th>\n",
              "      <th>q30e</th>\n",
              "      <th>q31a</th>\n",
              "      <th>q31i</th>\n",
              "      <th>q31e</th>\n",
              "      <th>q32a</th>\n",
              "      <th>q32i</th>\n",
              "      <th>q32e</th>\n",
              "      <th>q33a</th>\n",
              "      <th>q33i</th>\n",
              "      <th>q33e</th>\n",
              "      <th>q34a</th>\n",
              "      <th>q34i</th>\n",
              "      <th>q34e</th>\n",
              "      <th>q35a</th>\n",
              "      <th>q35i</th>\n",
              "      <th>q35e</th>\n",
              "      <th>q36a</th>\n",
              "      <th>q36i</th>\n",
              "      <th>q36e</th>\n",
              "      <th>q37a</th>\n",
              "      <th>q37i</th>\n",
              "      <th>q37e</th>\n",
              "      <th>q38a</th>\n",
              "      <th>q38i</th>\n",
              "      <th>q38e</th>\n",
              "      <th>q39a</th>\n",
              "      <th>q39i</th>\n",
              "      <th>q39e</th>\n",
              "      <th>q40a</th>\n",
              "      <th>q40i</th>\n",
              "      <th>q40e</th>\n",
              "      <th>q41a</th>\n",
              "      <th>q41i</th>\n",
              "      <th>q41e</th>\n",
              "      <th>q42a</th>\n",
              "      <th>q42i</th>\n",
              "      <th>q42e</th>\n",
              "      <th>country</th>\n",
              "      <th>source</th>\n",
              "      <th>introelapse</th>\n",
              "      <th>testelapse</th>\n",
              "      <th>surveyelapse</th>\n",
              "      <th>tipi1</th>\n",
              "      <th>tipi2</th>\n",
              "      <th>tipi3</th>\n",
              "      <th>tipi4</th>\n",
              "      <th>tipi5</th>\n",
              "      <th>tipi6</th>\n",
              "      <th>tipi7</th>\n",
              "      <th>tipi8</th>\n",
              "      <th>tipi9</th>\n",
              "      <th>tipi10</th>\n",
              "      <th>vcl1</th>\n",
              "      <th>vcl2</th>\n",
              "      <th>vcl3</th>\n",
              "      <th>vcl4</th>\n",
              "      <th>vcl5</th>\n",
              "      <th>vcl6</th>\n",
              "      <th>vcl7</th>\n",
              "      <th>vcl8</th>\n",
              "      <th>vcl9</th>\n",
              "      <th>vcl10</th>\n",
              "      <th>vcl11</th>\n",
              "      <th>vcl12</th>\n",
              "      <th>vcl13</th>\n",
              "      <th>vcl14</th>\n",
              "      <th>vcl15</th>\n",
              "      <th>vcl16</th>\n",
              "      <th>education</th>\n",
              "      <th>urban</th>\n",
              "      <th>gender</th>\n",
              "      <th>engnat</th>\n",
              "      <th>age</th>\n",
              "      <th>screensize</th>\n",
              "      <th>uniquenetworklocation</th>\n",
              "      <th>hand</th>\n",
              "      <th>religion</th>\n",
              "      <th>orientation</th>\n",
              "      <th>race</th>\n",
              "      <th>voted</th>\n",
              "      <th>married</th>\n",
              "      <th>familysize</th>\n",
              "      <th>major</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>6116</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>3193</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12542</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>6150</td>\n",
              "      <td>3</td>\n",
              "      <td>40</td>\n",
              "      <td>6428</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>17001</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2944</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8626</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>9639</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>6175</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>6008</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>9267</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>5290</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>25694</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>7634</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>8513</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>9078</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>4381</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>6647</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "      <td>6250</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3842</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>7876</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3124</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>6836</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>12063</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>9264</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3957</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2537</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>10880</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>8462</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>5615</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>11412</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5112</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3070</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13377</td>\n",
              "      <td>2</td>\n",
              "      <td>38</td>\n",
              "      <td>4506</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>17227</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>7844</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>20253</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>8528</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4370</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>10310</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>349</td>\n",
              "      <td>213</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2901</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9036</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>3775</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2027</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2714</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3626</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4496</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3453</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>4619</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2540</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>2946</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3508</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1727</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>4508</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2490</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3725</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>4826</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2425</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>3002</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2371</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>4058</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2721</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2511</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2981</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>4554</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>1894</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1541</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3354</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3129</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>5256</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5345</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>3058</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>1690</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>1686</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>4135</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>1852</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>4181</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1898</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1389</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>4266</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1534</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3449</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>143</td>\n",
              "      <td>228</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>Biology and Philosophy dual major</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6729</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4793</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>3328</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>2199</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>2513</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2220</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>2489</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>2570</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5790</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>4348</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5951</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>1973</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>2679</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>3425</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1931</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>4472</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1450</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>2006</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>3613</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1166</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>3214</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>4843</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>1216</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>4086</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>5170</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>1703</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>2482</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>1741</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2681</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2779</td>\n",
              "      <td>3</td>\n",
              "      <td>38</td>\n",
              "      <td>2302</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>3895</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1313</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1426</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>3803</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2430</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>2035</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2754</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2830</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>2357</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>3662</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3372</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>73</td>\n",
              "      <td>128</td>\n",
              "      <td>89</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2236</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1437</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2570</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2188</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>3372</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1975</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1579</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>1736</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>11058</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3389</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>2105</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2218</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1803</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>3058</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>4186</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2184</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3906</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2010</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3128</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>2375</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>1957</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1567</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1334</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2903</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2473</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>1397</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>2119</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4983</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>3193</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2454</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>6983</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3281</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>2984</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>1676</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>5516</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1483</td>\n",
              "      <td>2</td>\n",
              "      <td>33</td>\n",
              "      <td>3556</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>2299</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>2794</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>2537</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>1524</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2944</td>\n",
              "      <td>US</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>121</td>\n",
              "      <td>171</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>3404</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>10935</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3593</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>5086</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>2810</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3677</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>4919</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3288</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>8824</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>5750</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>2475</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>2422</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1638</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>15928</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2770</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>4719</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3639</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>1745</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>8400</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>2408</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>3548</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4149</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>3043</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>3698</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>27429</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>2660</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>2621</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>2219</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2827</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4980</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2623</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>11105</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>2245</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>5499</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>6972</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1761</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8120</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6261</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3951</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2576</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>11750</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>221</td>\n",
              "      <td>183</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>3069</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3561</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>2200</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>3045</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>8671</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2186</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>1527</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2236</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1265</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2232</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>1676</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>1716</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>2553</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1797</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>1585</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>3475</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>2722</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1785</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>4253</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>2077</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1918</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>1533</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>2939</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>2699</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8376</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1452</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>1325</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>3080</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2771</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2336</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>2938</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>3676</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1717</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1301</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>4973</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1492</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>2865</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1791</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1858</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6011</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>1735</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>2849</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>115</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8203</th>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "      <td>3626</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>4161</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "      <td>2640</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7369</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2607</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>2122</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>2473</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>1538</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>4779</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>2741</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2389</td>\n",
              "      <td>3</td>\n",
              "      <td>34</td>\n",
              "      <td>2690</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>3376</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1621</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>2038</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>2406</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>3810</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3910</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>2106</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>3241</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>2975</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>1722</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2456</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3008</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>2506</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>2991</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2173</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2574</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>6601</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>2957</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4044</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2222</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4782</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>6651</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>1604</td>\n",
              "      <td>3</td>\n",
              "      <td>33</td>\n",
              "      <td>3008</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2206</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>2240</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>2424</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>2106</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>4078</td>\n",
              "      <td>US</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>144</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8204</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>2000</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>2055</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>2229</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3527</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1633</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1032</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>3567</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2608</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2936</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1848</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2448</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4936</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1423</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>3063</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>1103</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>2240</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>1832</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>1383</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>3498</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1760</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>1438</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>1422</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>2296</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>1289</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>2384</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2207</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>1632</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>1544</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2351</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>1824</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5042</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1528</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1368</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>1344</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>1889</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4773</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2886</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>1592</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1208</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>2679</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1920</td>\n",
              "      <td>US</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>120</td>\n",
              "      <td>221</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>Marketing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8205</th>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>2350</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>2316</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>2087</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>2614</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>8479</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "      <td>1443</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>1320</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2862</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>1488</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>12252</td>\n",
              "      <td>4</td>\n",
              "      <td>37</td>\n",
              "      <td>1481</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>1291</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2200</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>3329</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>1575</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>1286</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2491</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>3308</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>1336</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>1555</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>1270</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3946</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>1604</td>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>6550</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>1244</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>1256</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>1223</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>1841</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3922</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>2839</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>2615</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>1342</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>1194</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5597</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>1214</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1743</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>1833</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>1371</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3110</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3029</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>4222</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>116</td>\n",
              "      <td>86</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8206</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>28897</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3025</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>3248</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>8048</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3559</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>2219</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4553</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>1890</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>4561</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>4071</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>2909</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>5179</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>2190</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>52923</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>2126</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>3341</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>3949</td>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>4455</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19120</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4429</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>3372</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>3409</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2538</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2541</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>8110</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>2701</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2408</td>\n",
              "      <td>3</td>\n",
              "      <td>37</td>\n",
              "      <td>2909</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>4100</td>\n",
              "      <td>2</td>\n",
              "      <td>33</td>\n",
              "      <td>13153</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>19343</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>5189</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>3088</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>2900</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>23056</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1677</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>15366</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>2718</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>4331</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3769</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>4261</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>4709</td>\n",
              "      <td>US</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>300</td>\n",
              "      <td>234</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8207 rows × 172 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9a13824-f681-44a3-a6a8-9fd4aad10958')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9a13824-f681-44a3-a6a8-9fd4aad10958 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9a13824-f681-44a3-a6a8-9fd4aad10958');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOplAYr41FQ3"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms the Target data into a usable form."
      ],
      "metadata": {
        "id": "P_fLFjz3U-YA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Liw04BKW-o9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e361344-225f-4d9f-a8d3-70235cdb1498"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 59.8 ms (2023-04-22T19:10:14/2023-04-22T19:10:14)</pre>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Finds the Depression, Anxiety, and Stress score on each row and assigns a score.\n",
        "Modeldf = pd.DataFrame()\n",
        "\n",
        "DepressionTarg = ['q3a', 'q5a', 'q10a', 'q13a', 'q16a', 'q17a', 'q21a', 'q24a', 'q26a', 'q31a', 'q34a', 'q37a', 'q38a', 'q42a']\n",
        "AnxietyTarg = ['q2a', 'q4a', 'q7a', 'q9a', 'q15a', 'q19a', 'q20a', 'q23a', 'q25a', 'q28a', 'q30a', 'q36a', 'q40a', 'q41a']\n",
        "StressTarg = ['q1a', 'q6a', 'q8a', 'q11a', 'q12a', 'q14a', 'q18a', 'q22a', 'q27a', 'q29a', 'q32a', 'q33a', 'q35a', 'q39a']\n",
        "\n",
        "features = []\n",
        "targets = []\n",
        "\n",
        "Depression = 0\n",
        "Anxiety = 0\n",
        "Stress = 0\n",
        "\n",
        "#Decrements the saved answers by one due to incorrrect storing in the Dataframe.\n",
        "for i in DepressionTarg:\n",
        "  Depression = sum([Depression, (df[i]-1)])\n",
        "\n",
        "for i in AnxietyTarg:\n",
        "  Anxiety = sum([Anxiety, (df[i]-1)])\n",
        "\n",
        "for i in StressTarg:\n",
        "  Stress = sum([Stress, (df[i]-1)])\n",
        "\n",
        "Modeldf['Depression_Score'] = Depression\n",
        "Modeldf['Anxiety_Score'] = Anxiety\n",
        "Modeldf['Stress_Score'] = Stress\n",
        "\n",
        "targets = targets + ['Depression_Score', 'Anxiety_Score', 'Stress_Score']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the catigorical features and breaks it up into multiple binary features."
      ],
      "metadata": {
        "id": "2HBzsKutVEZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eoT9U6O7ah9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "4929387d-f7f8-43c3-fe11-314c62b09602"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>✔️ 651 ms (2023-04-22T19:10:14/2023-04-22T19:10:15)</pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3652\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3653\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'No_Degree'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-11f120e19b7a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mELoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'education'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mNDLoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModeldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No_Degree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mHSLoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModeldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HighSchool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mULoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModeldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'University'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'No_Degree'"
          ]
        }
      ],
      "source": [
        "#Breaks the eduction column into more binary columns.\n",
        "Modeldf['No_Degree'] = 0\n",
        "Modeldf['HighSchool'] = 0\n",
        "Modeldf['University'] = 0\n",
        "Modeldf['Graduate'] = 0\n",
        "\n",
        "ELoc = df.columns.get_loc('education')\n",
        "\n",
        "NDLoc = Modeldf.columns.get_loc('No_Degree')\n",
        "HSLoc = Modeldf.columns.get_loc('HighSchool')\n",
        "ULoc = Modeldf.columns.get_loc('University')\n",
        "GLoc = Modeldf.columns.get_loc('Graduate')\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,ELoc] == 1:\n",
        "    Modeldf.iat[i,NDLoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 2:\n",
        "    Modeldf.iat[i,HSLoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 3:\n",
        "    Modeldf.iat[i,ULoc] = 1\n",
        "\n",
        "  elif df.iat[i,ELoc] == 4:\n",
        "    Modeldf.iat[i,GLoc] = 1\n",
        "\n",
        "features = features + ['No_Degree', 'HighSchool', 'University', 'Graduate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-00CbDVgxyb"
      },
      "outputs": [],
      "source": [
        "#Breaks the urban column into more binary columns.\n",
        "Modeldf['Rual'] = 0\n",
        "Modeldf['Suburban'] = 0\n",
        "Modeldf['Urban'] = 0\n",
        "\n",
        "Urloc = df.columns.get_loc('urban')\n",
        "\n",
        "RLoc = Modeldf.columns.get_loc('Rual')\n",
        "SULoc = Modeldf.columns.get_loc('Suburban')\n",
        "ULoc = Modeldf.columns.get_loc('Urban')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Urloc] == 1:\n",
        "    Modeldf.iat[i,RLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Urloc] == 2:\n",
        "    Modeldf.iat[i,SULoc] = 1\n",
        "\n",
        "  elif df.iat[i,Urloc] == 3:\n",
        "    Modeldf.iat[i,ULoc] = 1\n",
        "\n",
        "features = features + ['Rual', 'Suburban', 'Urban']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhea8Nl6I7MI"
      },
      "outputs": [],
      "source": [
        "#Breaks the gender column into more binary columns.\n",
        "Modeldf['Male'] = 0\n",
        "Modeldf['Female'] = 0\n",
        "Modeldf['Other_Gender'] = 0\n",
        "\n",
        "gloc = df.columns.get_loc('gender')\n",
        "\n",
        "mLoc = Modeldf.columns.get_loc('Male')\n",
        "fLoc = Modeldf.columns.get_loc('Female')\n",
        "oLoc = Modeldf.columns.get_loc('Other_Gender')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,gloc] == 1:\n",
        "    Modeldf.iat[i,mLoc] = 1\n",
        "\n",
        "  elif df.iat[i,gloc] == 2:\n",
        "    Modeldf.iat[i,fLoc] = 1\n",
        "\n",
        "  elif df.iat[i,gloc] == 3:\n",
        "    Modeldf.iat[i,oLoc] = 1\n",
        "\n",
        "features = features + ['Male', 'Female', 'Other_Gender']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DN6yasKKI1G"
      },
      "outputs": [],
      "source": [
        "#Normalizes the engnat column into more binary columns.\n",
        "Modeldf['Engnat'] = 0\n",
        "\n",
        "eloc = df.columns.get_loc('engnat')\n",
        "\n",
        "newELoc = Modeldf.columns.get_loc('Engnat')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,eloc] == 1:\n",
        "    Modeldf.iat[i,newELoc] = 1\n",
        "\n",
        "features = features + ['Engnat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jerGqHteKw22"
      },
      "outputs": [],
      "source": [
        "#Breaks the hand column into more binary columns.\n",
        "Modeldf['Right_h'] = 0\n",
        "Modeldf['Left_h'] = 0\n",
        "Modeldf['Ambidextrous'] = 0\n",
        "\n",
        "hloc = df.columns.get_loc('hand')\n",
        "\n",
        "rLoc = Modeldf.columns.get_loc('Right_h')\n",
        "lLoc = Modeldf.columns.get_loc('Left_h')\n",
        "aLoc = Modeldf.columns.get_loc('Ambidextrous')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,hloc] == 1:\n",
        "    Modeldf.iat[i,rLoc] = 1\n",
        "\n",
        "  elif df.iat[i,hloc] == 2:\n",
        "    Modeldf.iat[i,lLoc] = 1\n",
        "\n",
        "  elif df.iat[i,hloc] == 3:\n",
        "    Modeldf.iat[i,aLoc] = 1\n",
        "\n",
        "features = features + ['Right_h', 'Left_h', 'Ambidextrous']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyScZFDvLp_8"
      },
      "outputs": [],
      "source": [
        "#Breaks the religion column into more binary columns.\n",
        "Modeldf['Agnostic'] = 0\n",
        "Modeldf['Atheist'] = 0\n",
        "Modeldf['Buddhist'] = 0\n",
        "Modeldf['Catholic'] = 0\n",
        "Modeldf['Mormon'] = 0\n",
        "Modeldf['Protestant'] = 0\n",
        "Modeldf['Other_Christian'] = 0\n",
        "Modeldf['Hindu'] = 0\n",
        "Modeldf['Jewish'] = 0\n",
        "Modeldf['Muslim'] = 0\n",
        "Modeldf['Sikh'] = 0\n",
        "Modeldf['Other_Religion'] = 0\n",
        "\n",
        "Reloc = df.columns.get_loc('religion')\n",
        "\n",
        "agLoc = Modeldf.columns.get_loc('Agnostic')\n",
        "athLoc = Modeldf.columns.get_loc('Atheist')\n",
        "bLoc = Modeldf.columns.get_loc('Buddhist')\n",
        "caLoc = Modeldf.columns.get_loc('Catholic')\n",
        "moLoc = Modeldf.columns.get_loc('Mormon')\n",
        "pLoc = Modeldf.columns.get_loc('Protestant')\n",
        "ocLoc = Modeldf.columns.get_loc('Other_Christian')\n",
        "hLoc = Modeldf.columns.get_loc('Hindu')\n",
        "jLoc = Modeldf.columns.get_loc('Jewish')\n",
        "muLoc = Modeldf.columns.get_loc('Muslim')\n",
        "siLoc = Modeldf.columns.get_loc('Sikh')\n",
        "orLoc = Modeldf.columns.get_loc('Other_Religion')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Reloc] == 1:\n",
        "    Modeldf.iat[i,agLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 2:\n",
        "    Modeldf.iat[i,athLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 3:\n",
        "    Modeldf.iat[i,bLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 4:\n",
        "    Modeldf.iat[i,caLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 5:\n",
        "    Modeldf.iat[i,moLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 6:\n",
        "    Modeldf.iat[i,pLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 7:\n",
        "    Modeldf.iat[i,ocLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 8:\n",
        "    Modeldf.iat[i,hLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 9:\n",
        "    Modeldf.iat[i,jLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 10:\n",
        "    Modeldf.iat[i,muLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 11:\n",
        "    Modeldf.iat[i,siLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Reloc] == 12:\n",
        "    Modeldf.iat[i,orLoc] = 1\n",
        "\n",
        "features = features + ['Agnostic', 'Atheist', 'Buddhist', 'Catholic', 'Mormon', 'Protestant', 'Other_Christian', 'Hindu', 'Jewish', 'Muslim', 'Sikh', 'Other_Religion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPS6mn02Nmgb"
      },
      "outputs": [],
      "source": [
        "#Breaks the orientation column into more binary columns.\n",
        "Modeldf['Heterosexual'] = 0\n",
        "Modeldf['Bisexual'] = 0\n",
        "Modeldf['Homosexual'] = 0\n",
        "Modeldf['Asexual'] = 0\n",
        "Modeldf['Other_Orientation'] = 0\n",
        "\n",
        "Orloc = df.columns.get_loc('orientation')\n",
        "\n",
        "HeLoc = Modeldf.columns.get_loc('Heterosexual')\n",
        "BLoc = Modeldf.columns.get_loc('Bisexual')\n",
        "HoLoc = Modeldf.columns.get_loc('Homosexual')\n",
        "AsLoc = Modeldf.columns.get_loc('Asexual')\n",
        "OtLoc = Modeldf.columns.get_loc('Other_Orientation')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Orloc] == 1:\n",
        "    Modeldf.iat[i,HeLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 2:\n",
        "    Modeldf.iat[i,BLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 3:\n",
        "    Modeldf.iat[i,HoLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 4:\n",
        "    Modeldf.iat[i,AsLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Orloc] == 5:\n",
        "    Modeldf.iat[i,OtLoc] = 1\n",
        "\n",
        "features = features + ['Heterosexual', 'Bisexual', 'Homosexual', 'Asexual', 'Other_Orientation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7q_UpEwOXMj"
      },
      "outputs": [],
      "source": [
        "#Breaks the race column into more binary columns.\n",
        "Modeldf['Asian'] = 0\n",
        "Modeldf['Arab'] = 0\n",
        "Modeldf['Black'] = 0\n",
        "Modeldf['Indigenous_Australian'] = 0\n",
        "Modeldf['Native_American'] = 0\n",
        "Modeldf['White'] = 0\n",
        "Modeldf['Other_Race'] = 0\n",
        "\n",
        "Raloc = df.columns.get_loc('race')\n",
        "\n",
        "AsLoc = Modeldf.columns.get_loc('Asian')\n",
        "ArhLoc = Modeldf.columns.get_loc('Arab')\n",
        "BlLoc = Modeldf.columns.get_loc('Black')\n",
        "IaLoc = Modeldf.columns.get_loc('Indigenous_Australian')\n",
        "NaLoc = Modeldf.columns.get_loc('Native_American')\n",
        "WLoc = Modeldf.columns.get_loc('White')\n",
        "OrLoc = Modeldf.columns.get_loc('Other_Race')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,Raloc] == 10:\n",
        "    Modeldf.iat[i,AsLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 20:\n",
        "    Modeldf.iat[i,ArhLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 30:\n",
        "    Modeldf.iat[i,BlLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 40:\n",
        "    Modeldf.iat[i,IaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 50:\n",
        "    Modeldf.iat[i,NaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 60:\n",
        "    Modeldf.iat[i,WLoc] = 1\n",
        "\n",
        "  elif df.iat[i,Raloc] == 70:\n",
        "    Modeldf.iat[i,OrLoc] = 1\n",
        "\n",
        "features = features + ['Asian', 'Arab', 'Black', 'Indigenous_Australian', 'Native_American', 'White', 'Other_Race']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuU_NNexPsn-"
      },
      "outputs": [],
      "source": [
        "#Normalizes the voted column into more binary columns.\n",
        "Modeldf['Voted'] = 0\n",
        "\n",
        "vloc = df.columns.get_loc('voted')\n",
        "\n",
        "newVLoc = Modeldf.columns.get_loc('Voted')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,vloc] == 1:\n",
        "    Modeldf.iat[i,newVLoc] = 1\n",
        "\n",
        "features = features + ['Voted']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45qg7RY3QGCA"
      },
      "outputs": [],
      "source": [
        "#Breaks the married column into more binary columns.\n",
        "Modeldf['Never_married'] = 0\n",
        "Modeldf['Married'] = 0\n",
        "Modeldf['Previously_married'] = 0\n",
        "\n",
        "mloc = df.columns.get_loc('married')\n",
        "\n",
        "NmLoc = Modeldf.columns.get_loc('Never_married')\n",
        "MaLoc = Modeldf.columns.get_loc('Married')\n",
        "PmLoc = Modeldf.columns.get_loc('Previously_married')\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "  if df.iat[i,mloc] == 1:\n",
        "    Modeldf.iat[i,NmLoc] = 1\n",
        "\n",
        "  elif df.iat[i,mloc] == 2:\n",
        "    Modeldf.iat[i,MaLoc] = 1\n",
        "\n",
        "  elif df.iat[i,mloc] == 3:\n",
        "    Modeldf.iat[i,PmLoc] = 1\n",
        "\n",
        "features = features + ['Never_married', 'Married', 'Previously_married']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takes the data from the TIPI questions and changes it to a usable form."
      ],
      "metadata": {
        "id": "yxFUdCRTVXJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEl8ZDPR1C3m"
      },
      "outputs": [],
      "source": [
        "#Changing the TIPI answers to a usable form.\n",
        "#Only Run this cell ONCE!\n",
        "NomScore = ['tipi1', 'tipi7', 'tipi3', 'tipi9', 'tipi5']\n",
        "RevScore = ['tipi6', 'tipi2', 'tipi8', 'tipi4', 'tipi10']\n",
        "\n",
        "Rev1 = df.columns.get_loc('tipi6')\n",
        "Rev2 = df.columns.get_loc('tipi2')\n",
        "Rev3 = df.columns.get_loc('tipi8')\n",
        "Rev4 = df.columns.get_loc('tipi4')\n",
        "Rev5 = df.columns.get_loc('tipi10')\n",
        "\n",
        "RevLocs = [Rev1, Rev2, Rev3, Rev4, Rev5]\n",
        "\n",
        "for j in RevLocs:\n",
        "  for i in range(len(df)):\n",
        "    if df.iat[i,j] == 1:\n",
        "      df.iat[i,j] = 7\n",
        "\n",
        "    elif df.iat[i,j] == 2:\n",
        "      df.iat[i,j] = 6\n",
        "\n",
        "    elif df.iat[i,j] == 3:\n",
        "      df.iat[i,j] = 5\n",
        "\n",
        "    elif df.iat[i,j] == 4:\n",
        "      df.iat[i,j] = 4\n",
        "\n",
        "    elif df.iat[i,j] == 5:\n",
        "      df.iat[i,j] = 3\n",
        "\n",
        "    elif df.iat[i,j] == 6:\n",
        "      df.iat[i,j] = 2\n",
        "\n",
        "    elif df.iat[i,j] == 7:\n",
        "      df.iat[i,j] = 1\n",
        "\n",
        "df['Extraversion'] = (df[NomScore[0]] + df[RevScore[0]])/2\n",
        "df['Agreeableness'] = (df[NomScore[1]] + df[RevScore[1]])/2\n",
        "df['Conscientiousness'] = (df[NomScore[2]] + df[RevScore[2]])/2\n",
        "df['Emotional_Stability'] = (df[NomScore[3]] + df[RevScore[3]])/2\n",
        "df['Openness'] = (df[NomScore[4]] + df[RevScore[4]])/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1bmmJH41o8v"
      },
      "source": [
        "###Normalize the data so all continuous values is on a 0 to 1 scale to match the other features which are categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drDSk9utUxNp"
      },
      "outputs": [],
      "source": [
        "#Normailizes the non-categorical data.\n",
        "normal = MinMaxScaler()\n",
        "\n",
        "sidedf = pd.DataFrame()\n",
        "sidedf = df[['age', 'familysize']]   \n",
        "sidedf['Extraversion'] = df['Extraversion']\n",
        "sidedf['Agreeableness'] = df['Agreeableness']\n",
        "sidedf['Conscientiousness'] = df['Conscientiousness']\n",
        "sidedf['Emotional_Stability'] = df['Emotional_Stability']\n",
        "sidedf['Openness'] = df['Openness']\n",
        "                      \n",
        "Modeldf['Age'] = 0\n",
        "Modeldf['FamilySize'] = 0\n",
        "\n",
        "sidedf = pd.DataFrame(normal.fit_transform(sidedf), columns=['age','familysize', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness'])\n",
        "\n",
        "Modeldf['Age'] = sidedf['age']\n",
        "Modeldf['FamilySize'] = sidedf['familysize']\n",
        "Modeldf['Extraversion'] = sidedf['Extraversion']\n",
        "Modeldf['Agreeableness'] = sidedf['Agreeableness']\n",
        "Modeldf['Conscientiousness'] = sidedf['Conscientiousness']\n",
        "Modeldf['Emotional_Stability'] = sidedf['Emotional_Stability']\n",
        "Modeldf['Openness'] = sidedf['Openness']\n",
        "ut.pprint(sidedf)\n",
        "features = features + ['Age', 'FamilySize', 'Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the data for classification and Regression Approach"
      ],
      "metadata": {
        "id": "K8kLqWylSwKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the target data can be a catigorical value or a continus value, I decided try some regrssive models and then classifers to see what gives better results."
      ],
      "metadata": {
        "id": "ttDD5kY-UGE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classdf = Modeldf\n",
        "Regressiondf = Modeldf"
      ],
      "metadata": {
        "id": "5IkZS27CJD-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression Approch"
      ],
      "metadata": {
        "id": "sUxogZ5dbXoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Examining the Data before Regression"
      ],
      "metadata": {
        "id": "fS-Q_DA0cnH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Data is checked and verified that all preprocessing steps executed correctly."
      ],
      "metadata": {
        "id": "HsYkYykd2aKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Regressiondf = Modeldf\n",
        "display(Regressiondf.describe(include='all'))\n",
        "ut.pprint(Regressiondf)"
      ],
      "metadata": {
        "id": "P6pZ7-C4cnH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Depression Model"
      ],
      "metadata": {
        "id": "cM2tmWsQVQ-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = Regressiondf[features]\n",
        "\n",
        "#Depression Model.\n",
        "D = Regressiondf[targets[0]]\n",
        "\n",
        "X_Regset, X_HoldRegset, D_Regset, D_HoldRegset = train_test_split(X, D, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "0D2kg4ojVPBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgdfN2fYO-i"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mURufTDvYPl9"
      },
      "outputs": [],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [25,50,100],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "DepRegModel1 = GridSearchCV(KNReg, KNParaReg)\n",
        "DepRegModel1.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc1 = DepRegModel1.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {DModel_Acc1*100}%')\n",
        "print(DepRegModel1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "-dvWxx7RpMyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I spent my focus here since random forrest is both an esemble methoid and will better fit the categorical data."
      ],
      "metadata": {
        "id": "xO5JgQ1iE2GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[20,25,30],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [6,7,8],\n",
        "    'min_samples_split': [12,15,17,20],\n",
        "    'random_state':[randnum]\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "DepRegModel2 = GridSearchCV(RFReg, RFParaReg)\n",
        "DepRegModel2.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc2 = DepRegModel2.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {DModel_Acc2*100}%')\n",
        "print(DepRegModel2.best_params_)"
      ],
      "metadata": {
        "id": "uwQMjDZCPtg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "ubXFQB9R4iUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error', 'poisson', 'quantile'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "DepRegModel3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "DepRegModel3.fit(X_Regset, D_Regset)\n",
        "\n",
        "DModel_Acc3 = DepRegModel3.score(X_HoldRegset, D_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc3*100}%')\n",
        "print(DepRegModel3.best_params_)"
      ],
      "metadata": {
        "id": "aR4-HcGZ4r4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awobaYn3BcJD"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "YvXHN-3XBcJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [DModel_Acc1, DModel_Acc2, DModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestDepRegParams = DepRegModel1.best_params_\n",
        "\n",
        "  DepRegNei = BestDepRegParams['n_neighbors']\n",
        "  DepRegAlg = BestDepRegParams['algorithm']\n",
        "  DepRegWei = BestDepRegParams['weights']\n",
        "\n",
        "  FastDepRegMod = KNeighborsClassifier(n_neighbors=DepRegNei, algorithm=DepRegAlg, weights=DepRegWei, random_state=randnum)\n",
        "\n",
        "  FinalDepRegMod = BaggingRegressor(estimator=FastDepRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepRegMod.fit(X_Regset, D_Regset)\n",
        "\n",
        "  DRegMod_Acc = FinalDepRegMod.score(X_HoldRegset, D_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {DRegMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc3*100}%')"
      ],
      "metadata": {
        "id": "QVNiUcsWBTsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Anxiety Model"
      ],
      "metadata": {
        "id": "fSYPMPxnVY30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Anxiety Model.\n",
        "A = Regressiondf[targets[1]]\n",
        "randnum = 500\n",
        "\n",
        "X_Regset, X_HoldRegset, A_Regset, A_HoldRegset= train_test_split(X, A, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "6EqkYBt8Vd9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz4ZqPeIZU3o"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAyJHtiAZU9n"
      },
      "outputs": [],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [275, 300, 325],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "AnxRegModel1 = GridSearchCV(KNReg, KNParaReg)\n",
        "AnxRegModel1.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc1 = AnxRegModel1.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {AModel_Acc1*100}%')\n",
        "print(AnxRegModel1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "-gZ2J8YzawXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[10,20,30],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [8,9,10],\n",
        "    'min_samples_split': [7,8,9],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "AnxRegModel2 = GridSearchCV(RFReg, RFParaReg)\n",
        "AnxRegModel2.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc2 = AnxRegModel2.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {AModel_Acc2*100}%')\n",
        "print(AnxRegModel2.best_params_)"
      ],
      "metadata": {
        "id": "vFHHfRgMawXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "9UVXrkGd6Gqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error', 'poisson', 'quantile'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "AnxRegModel3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "AnxRegModel3.fit(X_Regset, A_Regset)\n",
        "\n",
        "AModel_Acc3 = AnxRegModel3.score(X_HoldRegset, A_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc3*100}%')\n",
        "print(AnxRegModel3.best_params_)"
      ],
      "metadata": {
        "id": "gxdxWOzl6Gqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NsXxBFawXZ"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "bynBzRhsawXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [AModel_Acc1, AModel_Acc2, AModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestAnxRegParams = AnxRegModel1.best_params_\n",
        "\n",
        "  AnxRegNei = BestAnxRegParams['n_neighbors']\n",
        "  AnxRegAlg = BestAnxRegParams['algorithm']\n",
        "  AnxRegWei = BestAnxRegParams['weights']\n",
        "\n",
        "  FastAnxRegMod = KNeighborsClassifier(n_neighbors=AnxRegNei, algorithm=AnxRegAlg, weights=AnxRegWei, random_state=randnum)\n",
        "\n",
        "  FinalAnxRegMod = BaggingRegressor(estimator=FastAnxRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxRegMod.fit(X_Regset, A_Regset)\n",
        "\n",
        "  ARegMod_Acc = FinalAnxRegMod.score(X_HoldRegset, A_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {ARegMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc3*100}%')"
      ],
      "metadata": {
        "id": "kuwFDHDcawXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stress Model"
      ],
      "metadata": {
        "id": "R9Rfe78eVeZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = Regressiondf[targets[2]]\n",
        "\n",
        "X_Regset, X_HoldRegset, S_Regset, S_HoldRegset = train_test_split(X, S, test_size=Holdout_size)"
      ],
      "metadata": {
        "id": "DQ_GxFbXVhnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klj14ZxLZ_tr"
      },
      "source": [
        "####K Neighbors Regressor Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MJAvIVxaA1t"
      },
      "outputs": [],
      "source": [
        "KNParaReg = {\n",
        "    'n_neighbors': [25, 50, 100],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNReg = KNeighborsRegressor()\n",
        "StsRegMod1 = GridSearchCV(KNReg, KNParaReg)\n",
        "StsRegMod1.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc1 = StsRegMod1.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the KNReg Model is: {SModel_Acc1*100}%')\n",
        "print(StsRegMod1.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random Forest Regression Model"
      ],
      "metadata": {
        "id": "KWboEvpua0yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFParaReg = {\n",
        "    'n_estimators':[10,20,30],\n",
        "    'criterion': ['squared_error', 'absolute_error'],\n",
        "    'max_depth': [8,9,10],\n",
        "    'min_samples_split': [9,10,13,15],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "\n",
        "RFReg = RandomForestRegressor()\n",
        "StsRegMod2 = GridSearchCV(RFReg, RFParaReg)\n",
        "StsRegMod2.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc2 = StsRegMod2.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the RFReg Model is: {SModel_Acc2*100}%')\n",
        "print(StsRegMod2.best_params_)"
      ],
      "metadata": {
        "id": "Qx0X65ZVa0yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Regression Model"
      ],
      "metadata": {
        "id": "PBszTAQs6kie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaReg = {\n",
        "    'loss':['squared_error', 'absolute_error', 'poisson', 'quantile'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBReg = HistGradientBoostingRegressor()\n",
        "StsRegModel3 = GridSearchCV(HGBReg, HGBParaReg)\n",
        "StsRegModel3.fit(X_Regset, S_Regset)\n",
        "\n",
        "SModel_Acc3 = StsRegModel3.score(X_HoldRegset, S_HoldRegset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc3*100}%')\n",
        "print(StsRegModel3.best_params_)"
      ],
      "metadata": {
        "id": "5FwYsPkJ6kie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bizRVBha0yx"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and puts it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "dK9-G7Zoa0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAccuracys = [SModel_Acc1, SModel_Acc2, SModel_Acc3]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == SModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestStsRegParams = StsRegMod1.best_params_\n",
        "\n",
        "  StsRegNei = BestStsRegParams['n_neighbors']\n",
        "  StsRegAlg = BestStsRegParams['algorithm']\n",
        "  StsRegWei = BestStsRegParams['weights']\n",
        "\n",
        "  FastStsRegMod = KNeighborsRegressor(n_neighbors=StsRegNei, algorithm=StsRegAlg, weights=StsRegWei, random_state=randnum)\n",
        "\n",
        "  FinalStsRegMod = BaggingRegressor(estimator=FastStsRegMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsRegMod.fit(X_Regset, S_Regset)\n",
        "\n",
        "  SRegMod_Acc = FinalStsRegMod.score(X_HoldRegset, S_HoldRegset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {SRegMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc3:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc3*100}%')"
      ],
      "metadata": {
        "id": "I1KPVntxa0yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clssification Approach"
      ],
      "metadata": {
        "id": "R9ML79I2ZZ9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classdf = Modeldf\n",
        "#Changes the target scores to match the target classes for Classification.\n",
        "#Grabs the location of the target columns.\n",
        "Dloc = Classdf.columns.get_loc('Depression_Score')\n",
        "Aloc = Classdf.columns.get_loc('Anxiety_Score')\n",
        "Sloc = Classdf.columns.get_loc('Stress_Score')\n",
        "\n",
        "for i in range(len(Classdf)):\n",
        "  #Depression.\n",
        "  if Classdf.iat[i,Dloc] <= 9:\n",
        "    Classdf.iat[i,Dloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 13:\n",
        "    Classdf.iat[i,Dloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 20:\n",
        "    Classdf.iat[i,Dloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] <= 27:\n",
        "    Classdf.iat[i,Dloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Dloc] >= 28:\n",
        "    Classdf.iat[i,Dloc] = 4\n",
        "\n",
        "  #Anxeity.\n",
        "  if Classdf.iat[i,Aloc] <= 7:\n",
        "    Classdf.iat[i,Aloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 9:\n",
        "    Classdf.iat[i,Aloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 14:\n",
        "    Classdf.iat[i,Aloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] <= 19:\n",
        "    Classdf.iat[i,Aloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Aloc] >= 20:\n",
        "    Classdf.iat[i,Aloc] = 4\n",
        "\n",
        "  #Stress.\n",
        "  if Classdf.iat[i,Sloc] <= 14:\n",
        "    Classdf.iat[i,Sloc] = 0\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 18:\n",
        "    Classdf.iat[i,Sloc] = 1\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 25:\n",
        "    Classdf.iat[i,Sloc] = 2\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] <= 33:\n",
        "    Classdf.iat[i,Sloc] = 3\n",
        "\n",
        "  elif Classdf.iat[i,Sloc] >= 34:\n",
        "    Classdf.iat[i,Sloc] = 4"
      ],
      "metadata": {
        "id": "hTvg1NNdfCvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Examining the Data before Classification"
      ],
      "metadata": {
        "id": "9U6PAiVncUZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Data is checked and verified that all preprocessing steps executed correctly."
      ],
      "metadata": {
        "id": "OXxiSh4U2Dgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(Classdf.describe(include='all'))\n",
        "ut.pprint(Classdf)"
      ],
      "metadata": {
        "id": "kltiyKXQcT6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzMSY3rX1KeF"
      },
      "source": [
        "###Depression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ySi2uJX3iZ"
      },
      "source": [
        "####Data Split for Depression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYWPRGeO2Aob"
      },
      "outputs": [],
      "source": [
        "X = Classdf[features]\n",
        "\n",
        "#Depression Model.\n",
        "D = Classdf[targets[0]]\n",
        "\n",
        "X_Claset, X_HoldClaset, D_Calset, D_HoldCalset = train_test_split(X, D, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRwfTEhjYD21"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkXfcitVX9wM"
      },
      "outputs": [],
      "source": [
        "KNParaClass = {\n",
        "    'n_neighbors': [75, 100, 125],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "\n",
        "KNClass = KNeighborsClassifier()\n",
        "DepClaMod1 = GridSearchCV(KNClass, KNParaClass)\n",
        "DepClaMod1.fit(X_Claset, D_Calset)\n",
        "\n",
        "DModel_Acc1 = DepClaMod1.score(X_HoldClaset, D_HoldCalset)\n",
        "print(f'The Accuracy of the KNClass Model is: {DModel_Acc1*100}%')\n",
        "print(DepClaMod1.best_params_)\n",
        "\n",
        "DMod1Pred = DepClaMod1.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod1Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod1Pred, labels=DepClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(DepClaMod1, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_85cpOTvYP5p"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYazB2REYQBt"
      },
      "outputs": [],
      "source": [
        "RandForParaClass = {\n",
        "    'n_estimators': [275, 300, 325], \n",
        "    'criterion': ['gini'], \n",
        "    'max_depth': [10,11,12], \n",
        "    'min_samples_split': [7,8,9], \n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClass = RandomForestClassifier()\n",
        "DepClaMod2 = GridSearchCV(RandForClass, RandForParaClass)\n",
        "DepClaMod2.fit(X_Claset, D_Calset)\n",
        "\n",
        "DModel_Acc2 = DepClaMod2.score(X_HoldClaset, D_HoldCalset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "print(DepClaMod2.best_params_)\n",
        "\n",
        "DMod2Pred = DepClaMod2.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod2Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod2Pred, labels=DepClassNames\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(DepClaMod2, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULKzJo2mYQMF"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv29r2lvYQSm"
      },
      "outputs": [],
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "    \n",
        "DepClaMod3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "DepClaMod3.fit(X_Claset, D_Calset)\n",
        "\n",
        "DModel_Acc3 = DepClaMod3.score(X_HoldClaset, D_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {DModel_Acc3*100}%')\n",
        "print(DepClaMod3.best_params_)\n",
        "\n",
        "DMod3Pred = DepClaMod3.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod3Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod3Pred, labels=DepClassNames\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(DepClaMod3, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NCjepfHMgWVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3,4,5,6],\n",
        "    'C':C,\n",
        "    }\n",
        "    \n",
        "DepClaMod4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "DepClaMod4.fit(X_Claset, D_Calset)\n",
        "\n",
        "DModel_Acc4 = DepClaMod4.score(X_HoldClaset, D_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {DModel_Acc4*100}%')\n",
        "print(DepClaMod4.best_params_)\n",
        "\n",
        "DMod4Pred = DepClaMod4.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod4Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod4Pred, labels=DepClassNames\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(DepClaMod4, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AJK-jh1EkNs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "GXdcfytm7ERR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "DepClassModel5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "DepClassModel5.fit(X_Classet, D_Calset)\n",
        "\n",
        "DModel_Acc5 = DepClassModel5.score(X_HoldClaset, D_HoldCalset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc5*100}%')\n",
        "print(DepClassModel5.best_params_)\n",
        "\n",
        "DMod5Pred = DepClassModel5.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod5Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod5Pred, labels=DepClassNames\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(DepClassModel5, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VWanf8n7ERR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDsCVpWwYf6j"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kz7UEgs8an7"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Claset).astype(np.float32)\n",
        "D_Train_Ten = np.asarray(D_Calset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldClaset).astype(np.float32)\n",
        "D_Test_Ten = np.asarray(D_HoldCalset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNDepClass = Sequential()\n",
        "NNDepClass.add(Dense(32, activation='relu',input_shape=(None,49)))\n",
        "NNDepClass.add(Dropout(0.2))\n",
        "NNDepClass.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNDepClass.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNDepClass.fit(X_Train_Ten, D_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Dscore = NNDepClass.evaluate(X_Test_Ten, D_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Dscore[1]*100}')\n",
        "\n",
        "DMod6Pred = NNDepClass.predict(X_HoldClaset)\n",
        "print(classification_report(D, DMod6Pred, target_names=DepClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(D, DMod6Pred, labels=DepClassNames\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=DepClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(NNDepClass, X_HoldClaset, D_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z872ZOvUYray"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "bSXG9qKk8gbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fJa2BXoxeQM"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [DModel_Acc1, DModel_Acc2, DModel_Acc3, DModel_Acc4, DModel_Acc5, Dscore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == DModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestDepClassParams = DepClaMod1.best_params_\n",
        "\n",
        "  DepClaNei = BestDepClassParams['n_neighbors']\n",
        "  DepClaAlg = BestDepClassParams['algorithm']\n",
        "  DepClaWei = BestDepClassParams['weights']\n",
        "\n",
        "  FastDepClassMod = KNeighborsClassifier(n_neighbors=DepClaNei, algorithm=DepClaAlg, weights=DepClaWei, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Calset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldCalset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {DModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc3:\n",
        "  #The SVC is the best model\n",
        "  BestDepClassParams = DepClaMod3.best_params_\n",
        "\n",
        "  DepClaKer = BestDepClassParams['kernel']\n",
        "  DepClaC = BestDepClassParams['C']\n",
        "\n",
        "  FastDepClassMod = SVC(kernel=DepClaKer, C=DepClaC, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Calset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldCalset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc4:\n",
        "  #The SVC is the best model.\n",
        "  BestDepClassParams = DepClaMod4.best_params_\n",
        "\n",
        "  DepClaKer = BestDepClassParams['kernel']\n",
        "  DepClaDeg = BestDepClassParams['degree']\n",
        "  DepClaC = BestDepClassParams['C']\n",
        "\n",
        "  FastDepClassMod = SVC(kernel=DepClaKer, degree=DepClaDeg, C=DepClaC, random_state=randnum)\n",
        "\n",
        "  FinalDepClaMod = BaggingClassifier(estimator=FastDepClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalDepClaMod.fit(X_Claset, D_Calset)\n",
        "\n",
        "  DClassMod_Acc = FinalDepClaMod.score(X_HoldClaset, D_HoldCalset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {DClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == DModel_Acc5:\n",
        "  #The HisGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {DModel_Acc5*100}%')\n",
        "\n",
        "elif BestMod == Dscore[1]:\n",
        "  #The Nural Network is the best model\n",
        "  print(f'The Accuracy of the Nural Network Model is: {Dscore[1]*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkZipli1SPs"
      },
      "source": [
        "###Anxiety Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FfYWK6TY7Ij"
      },
      "source": [
        "####Data Split for Anxiety"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9772EsXRHfFR"
      },
      "outputs": [],
      "source": [
        "#Anxiety Model.\n",
        "A = Classdf[targets[1]]\n",
        "randnum = 500\n",
        "\n",
        "X_Calset, X_HoldCalset, A_Calset, A_HoldCalset = train_test_split(X, A, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEePMb3ZUrO"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZCDQ9iFZUxw"
      },
      "outputs": [],
      "source": [
        "KNClassPara = {\n",
        "    'n_neighbors': [75, 100, 125], \n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "    \n",
        "KNClass = KNeighborsClassifier()\n",
        "AnxCalModel1 = GridSearchCV(KNClass, KNClassPara)\n",
        "AnxCalModel1.fit(X_Calset, A_Calset)\n",
        "\n",
        "AModel_Acc1 = AnxCalModel1.score(X_HoldCalset, A_HoldCalset)\n",
        "print(f'The Accuracy of the KNClass Model is: {AModel_Acc1*100}%')\n",
        "print(AnxCalModel1.best_params_)\n",
        "\n",
        "AMod1Pred = AnxCalModel1.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod1Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod1Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(AnxCalModel1, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfoOEjx2ZVEE"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUuZIWfbZVJq"
      },
      "outputs": [],
      "source": [
        "RandForClaPar = {\n",
        "    'n_estimators': [175, 200, 225], \n",
        "    'criterion': ['gini'], \n",
        "    'max_depth': [20,23,25], \n",
        "    'min_samples_split': [7,8,9],\n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClassMod = RandomForestClassifier()\n",
        "AnxCalModel2 = GridSearchCV(RandForClassMod, RandForClaPar)\n",
        "AnxCalModel2.fit(X_Calset, A_Calset)\n",
        "\n",
        "AModel_Acc2 = AnxCalModel2.score(X_HoldCalset, A_HoldCalset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "print(AnxCalModel2.best_params_)\n",
        "\n",
        "AMod2Pred = AnxCalModel2.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod2Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod2Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(AnxCalModel2, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4aZ1jS-ZVQC"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ],
      "metadata": {
        "id": "NrVIvOKsm39i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoAVyJ8DZVWT"
      },
      "outputs": [],
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "SVCClass = SVC()\n",
        "AnxCalModel3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "AnxCalModel3.fit(X_Calset, A_Calset)\n",
        "\n",
        "AModel_Acc3 = AnxCalModel3.score(X_HoldCalset, A_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {AModel_Acc3*100}%')\n",
        "print(AnxCalModel3.best_params_)\n",
        "\n",
        "AMod3Pred = AnxCalModel3.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod3Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod3Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(AnxCalModel3, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "SVCClass = SVC()\n",
        "AnxCalModel4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "AnxCalModel4.fit(X_Calset, A_Calset)\n",
        "\n",
        "AModel_Acc4 = AnxCalModel4.score(X_HoldCalset, A_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {AModel_Acc4*100}%')\n",
        "print(AnxCalModel4.best_params_)\n",
        "\n",
        "AMod4Pred = AnxCalModel4.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod4Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod4Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(AnxCalModel4, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94_j1iJ_nHBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "Gh91gq7_8hKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "AnxClassModel5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "AnxClassModel5.fit(X_Classet, A_Calset)\n",
        "\n",
        "AModel_Acc5 = AnxClassModel5.score(X_HoldClaset, A_HoldCalset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc5*100}%')\n",
        "print(AnxClassModel5.best_params_)\n",
        "\n",
        "AMod5Pred = AnxClassModel5.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod5Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod5Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(AnxClassModel5, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_f35mVNf8hKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBKbvEPZvDc"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12Pf1fTu1epw"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Calset).astype(np.float32)\n",
        "A_Train_Ten = np.asarray(A_Calset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldCalset).astype(np.float32)\n",
        "A_Test_Ten = np.asarray(A_HoldCalset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNAnxClassMod = Sequential()\n",
        "NNAnxClassMod.add(Dense(64, activation='relu',input_shape=(None,49)))\n",
        "NNAnxClassMod.add(Dropout(0.2))\n",
        "NNAnxClassMod.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNAnxClassMod.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNAnxClassMod.fit(X_Train_Ten, A_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Ascore = NNAnxClassMod.evaluate(X_Test_Ten, A_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Ascore[1]*100}')\n",
        "\n",
        "AMod6Pred = NNAnxClassMod.predict(X_HoldClaset)\n",
        "print(classification_report(A, AMod6Pred, target_names=AnxClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(A, AMod6Pred, labels=AnxClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=AnxClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(NNAnxClassMod, X_HoldClaset, A_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCeBf2-tZzy1"
      },
      "source": [
        "####Final Ensamble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "rwsoIEk1_D_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cckKeil31eiK"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [AModel_Acc1, AModel_Acc2, AModel_Acc3, AModel_Acc4, AModel_Acc5, Ascore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == AModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestAnxClassParams = AnxCalModel1.best_params_\n",
        "\n",
        "  AnxClaNei = BestAnxClassParams['n_neighbors']\n",
        "  AnxClaAlg = BestAnxClassParams['algorithm']\n",
        "  AnxClaWei = BestAnxClassParams['weights']\n",
        "\n",
        "  FastAnxClassMod = KNeighborsClassifier(n_neighbors=AnxClaNei, algorithm=AnxClaAlg, weights=AnxClaWei, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Calset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldCalset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {AModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc3:\n",
        "  #The SVC is the best model.\n",
        "  BestAnxClassParams = AnxCalModel3.best_params_\n",
        "\n",
        "  AnxClaKer = BestAnxClassParams['kernel']\n",
        "  AnxClaC = BestAnxClassParams['C']\n",
        "\n",
        "  FastAnxClassMod = SVC(kernel=AnxClaKer, C=AnxClaC, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Calset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldCalset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc4:\n",
        "  #The SVC is the best model.\n",
        "  BestAnxClassParams = AnxCalModel4.best_params_\n",
        "\n",
        "  AnxClaKer = BestAnxClassParams['kernel']\n",
        "  AnxClaDeg = BestAnxClassParams['degree']\n",
        "  AnxClaC = BestAnxClassParams['C']\n",
        "\n",
        "  FastAnxClassMod = SVC(kernel=AnxClaKer, degree=AnxClaDeg, C=AnxClaC, random_state=randnum)\n",
        "\n",
        "  FinalAnxClaMod = BaggingClassifier(estimator=FastAnxClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalAnxClaMod.fit(X_Claset, A_Calset)\n",
        "\n",
        "  AClassMod_Acc = FinalAnxClaMod.score(X_HoldClaset, A_HoldCalset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {AClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == AModel_Acc5:\n",
        "  #The HistGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {AModel_Acc5*100}%')\n",
        "\n",
        "elif BestMod == Ascore[1]:\n",
        "  #The Nural Network is the best model.\n",
        "  print(f'The Accuracy of the Nural Network Model is: {Ascore[1]*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emdL0Gl31Vqo"
      },
      "source": [
        "###Stress Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tIhIwb6aFX3"
      },
      "source": [
        "####Data Split for Stress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TavXFjBfHe-B"
      },
      "outputs": [],
      "source": [
        "S = Classdf[targets[2]]\n",
        "\n",
        "X_Calset, X_HoldCalset, S_Calset, S_HoldCalset = train_test_split(X, S, test_size=Holdout_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxnkHoNZ_T7"
      },
      "source": [
        "####K Neighbors Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbXbMnkpZ_l7"
      },
      "outputs": [],
      "source": [
        "KNClassPara = {\n",
        "    'n_neighbors': [15, 20, 100, 300], \n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
        "    'weights': ['uniform', 'distance'],\n",
        "    }\n",
        "    \n",
        "KNClass = KNeighborsClassifier()\n",
        "StrsClassModel1 = GridSearchCV(KNClass, KNClassPara)\n",
        "StrsClassModel1.fit(X_Calset, S_Calset)\n",
        "\n",
        "SModel_Acc1 = StrsClassModel1.score(X_HoldCalset, S_HoldCalset)\n",
        "print(f'The Accuracy of the KNClass Model is: {SModel_Acc1*100}%')\n",
        "print(StrsClassModel1.best_params_)\n",
        "\n",
        "SMod1Pred = StrsClassModel1.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod1Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod1Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(StrsClassModel1, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTB5VansaBHE"
      },
      "source": [
        "####Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKvDZGAMaBwe"
      },
      "outputs": [],
      "source": [
        "RandForClassPara = {\n",
        "    'n_estimators': [100, 150, 200], \n",
        "    'criterion': ['gini', 'entropy', 'log_loss'], \n",
        "    'max_depth': [12,15,20], \n",
        "    'min_samples_split': [5,6,7,8], \n",
        "    'random_state':[randnum],\n",
        "    }\n",
        "    \n",
        "RandForClass = RandomForestClassifier()\n",
        "StrsClassModel2 = GridSearchCV(RandForClass, RandForClassPara)\n",
        "StrsClassModel2.fit(X_Calset, S_Calset)\n",
        "\n",
        "SModel_Acc2 = StrsClassModel2.score(X_HoldCalset, S_HoldCalset)\n",
        "print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "print(StrsClassModel2.best_params_)\n",
        "\n",
        "SMod2Pred = StrsClassModel2.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod2Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod2Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(StrsClassModel2, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_1ji4OaDUQ"
      },
      "source": [
        "####SVC Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYp1NPyDaDaL"
      },
      "outputs": [],
      "source": [
        "SVCClass = SVC(random_state=randnum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Non-Polynomial SVC Model.\n",
        "SVCParaClass1 = {\n",
        "    'kernel': ['linear'],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "StrsClassModel3 = GridSearchCV(SVCClass, SVCParaClass1)\n",
        "StrsClassModel3.fit(X_Calset, S_Calset)\n",
        "\n",
        "SModel_Acc3 = StrsClassModel3.score(X_HoldCalset, S_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {SModel_Acc3*100}%')\n",
        "print(StrsClassModel3.best_params_)\n",
        "\n",
        "SMod3Pred = StrsClassModel3.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod3Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod3Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(StrsClassModel3, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-v8_ZFfsmMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial SVC Model.\n",
        "SVCParaClass2 = {\n",
        "    'kernel': ['poly'],\n",
        "    'degree':[2,3,4,5,6],\n",
        "    'C':C,\n",
        "    }\n",
        "\n",
        "StrsClassModel4 = GridSearchCV(SVCClass, SVCParaClass2)\n",
        "StrsClassModel4.fit(X_Calset, S_Calset)\n",
        "\n",
        "SModel_Acc4 = StrsClassModel4.score(X_HoldCalset, S_HoldCalset)\n",
        "print(f'The Accuracy of the SVC Model is: {SModel_Acc4*100}%')\n",
        "print(StrsClassModel4.best_params_)\n",
        "\n",
        "SMod4Pred = StrsClassModel4.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod4Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod4Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(StrsClassModel4, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OFejHq5CsmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####History Gradient Boosting Classification Model"
      ],
      "metadata": {
        "id": "PObxcmC-9jFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGBParaClass = {\n",
        "    'loss':['log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'],\n",
        "    'learning_rate':[0.1,0.3,0.5,0.7,0.9],\n",
        "    'max_iter':[75,100,150,200],\n",
        "    'max_depth':[2,5,7],\n",
        "    'random_state':[randnum],\n",
        "}\n",
        "\n",
        "HGBClass = HistGradientBoostingClassifier()\n",
        "StsClassModel5 = GridSearchCV(HGBClass, HGBParaClass)\n",
        "StsClassModel5.fit(X_Classet, S_Calset)\n",
        "\n",
        "SModel_Acc5 = StsClassModel5.score(X_HoldClaset, S_HoldCalset)\n",
        "\n",
        "print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc5*100}%')\n",
        "print(StsClassModel5.best_params_)\n",
        "\n",
        "SMod5Pred = StsClassModel5.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod5Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod5Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(StsClassModel5, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MVwiOMhM9jFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVvvKfDbyRq"
      },
      "source": [
        "####Nural Network Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fyrco7d1fJC"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 30\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "X_Train_Ten = np.asarray(X_Calset).astype(np.float32)\n",
        "S_Train_Ten = np.asarray(S_Calset).astype(np.float32)\n",
        "X_Test_Ten = np.asarray(X_HoldCalset).astype(np.float32)\n",
        "S_Test_Ten = np.asarray(S_HoldCalset).astype(np.float32)\n",
        "\n",
        "\n",
        "NNStsClassMod = Sequential()\n",
        "NNStsClassMod.add(Dense(32, activation='relu',input_shape=(None,49)))\n",
        "NNStsClassMod.add(Dropout(0.2))\n",
        "NNStsClassMod.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "NNStsClassMod.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = NNStsClassMod.fit(X_Train_Ten, S_Train_Ten, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\n",
        "Sscore = NNStsClassMod.evaluate(X_Test_Ten, S_Test_Ten, verbose=0)\n",
        "print(f'The Accuracy of the Nural Network: {Sscore[1]*100}')\n",
        "\n",
        "SMod6Pred = NNStsClassMod.predict(X_HoldClaset)\n",
        "print(classification_report(S, SMod6Pred, target_names=StsClassNames))\n",
        "\n",
        "ConMax = confusion_matrix(S, SMod6Pred, labels=StsClassNames)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=ConMax, display_labels=StsClassNames)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(NNStsClassMod, X_HoldClaset, S_HoldCalset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LozrfKKbypZ"
      },
      "source": [
        "####Final Ensemble Methoid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Which model has the greatest accuracy and throws it into an ensemble methoid to increase accuracy."
      ],
      "metadata": {
        "id": "uKfxIRgL_4nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2cYWls11fGq"
      },
      "outputs": [],
      "source": [
        "ModelAccuracys = [SModel_Acc1, SModel_Acc2, SModel_Acc3, SModel_Acc4, SModel_Acc5, Sscore[1]]\n",
        "BestMod = max(ModelAccuracys)\n",
        "\n",
        "if BestMod == SModel_Acc1:\n",
        "  #The KNeighborsClassifier is the best model.\n",
        "  BestStsClassParams = StrsClassModel1.best_params_\n",
        "\n",
        "  StsClaNei = BestStsClassParams['n_neighbors']\n",
        "  StsClaAlg = BestStsClassParams['algorithm']\n",
        "  StsClaWei = BestStsClassParams['weights']\n",
        "\n",
        "  FastStsClassMod = KNeighborsClassifier(n_neighbors=StsClaNei, algorithm=StsClaAlg, weights=StsClaWei, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Calset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldCalset)\n",
        "  print(f'The Accuracy of the KNeighbors Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc2:\n",
        "  #The Random Forest is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the Random Forest Model is: {SModel_Acc2*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc3:\n",
        "  #The SVC is the best model.\n",
        "  BestStsClassParams = StrsClassModel3.best_params_\n",
        "\n",
        "  StsClaKer = BestStsClassParams['kernel']\n",
        "  StsClaC = BestStsClassParams['C']\n",
        "\n",
        "  FastStsClassMod = SVC(kernel=StsClaKer, C=StsClaC, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Calset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldCalset)\n",
        "  print(f'The Accuracy of the Non-Polynomial SVC Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc4:\n",
        "  #The Polynomial SVC is the best model.\n",
        "  BestStsClassParams = StrsClassModel4.best_params_\n",
        "\n",
        "  StsClaKer = BestStsClassParams['kernel']\n",
        "  StsClaDeg = BestStsClassParams['degree']\n",
        "  StsClaC = BestStsClassParams['C']\n",
        "\n",
        "  FastStsClassMod = SVC(kernel=StsClaKer, degree=StsClaDeg, C=StsClaC, random_state=randnum)\n",
        "\n",
        "  FinalStsClaMod = BaggingClassifier(estimator=FastStsClassMod, n_estimators=50, random_state=randnum)\n",
        "  FinalStsClaMod.fit(X_Claset, S_Calset)\n",
        "\n",
        "  SClassMod_Acc = FinalStsClaMod.score(X_HoldClaset, S_HoldCalset)\n",
        "  print(f'The Accuracy of the Polynomial SVC Model is: {SClassMod_Acc*100}%')\n",
        "\n",
        "elif BestMod == SModel_Acc5:\n",
        "  #The HisGradientBoosting is the best model, but it's already an ensemble.\n",
        "  print(f'The Accuracy of the HistGradientBoosting Model is: {SModel_Acc5*100}%')\n",
        "\n",
        "elif BestMod == Sscore[1]:\n",
        "  #The MLP is the best model.\n",
        "  print(f'The Accuracy of the MLP Model is: {Sscore[1]*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "63RHuvMvNaxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yGPZMlPgNkWO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4khyxgT0P3p4",
        "1s8-xWnQULif",
        "9pp71514hoxZ",
        "PB6OhOPkvWI9",
        "UY02y715otK0",
        "_jXaRgKgokQI",
        "K8kLqWylSwKN",
        "63RHuvMvNaxe"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "10hfp4PoZEjrwMoAw5B1e4oKKVU0faOXS",
      "authorship_tag": "ABX9TyPvQpX156lmKIUeeJRJPqD7",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}